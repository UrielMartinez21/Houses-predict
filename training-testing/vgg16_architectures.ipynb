{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay una GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 29\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Dataset/Mosaico/'\n",
    "img_path = 'Dataset/Mosaico/Imagenes/'\n",
    "model_path = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>area</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>mosaic_image</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1184</td>\n",
       "      <td>91901</td>\n",
       "      <td>82.png</td>\n",
       "      <td>397500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1248</td>\n",
       "      <td>93446</td>\n",
       "      <td>471.png</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4190</td>\n",
       "      <td>85255</td>\n",
       "      <td>18.png</td>\n",
       "      <td>1199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1152</td>\n",
       "      <td>92276</td>\n",
       "      <td>352.png</td>\n",
       "      <td>99900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>92276</td>\n",
       "      <td>416.png</td>\n",
       "      <td>67000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  area  zipcode mosaic_image    price\n",
       "0         2        1.0  1184    91901       82.png   397500\n",
       "1         2        2.0  1248    93446      471.png   175000\n",
       "2         5        4.0  4190    85255       18.png  1199000\n",
       "3         2        2.0  1152    92276      352.png    99900\n",
       "4         2        1.0  1000    92276      416.png    67000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(f\"{dataset_path}train.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lab.Comp 1\\Desktop\\Uriel-TT\\houses-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lab.Comp 1\\Desktop\\Uriel-TT\\houses-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo preentrenado VGG16 para extraer características\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Quitamos la última capa fully connected (classifier) para obtener un vector de características\n",
    "vgg16.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-1])\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo final para predecir el precio de una casa a partir de imágenes y datos tabulares\n",
    "class HousePriceModelVGG16(nn.Module):\n",
    "    def __init__(self, num_tabular_features):\n",
    "        super(HousePriceModelVGG16, self).__init__()\n",
    "\n",
    "        # Red preentrenada para imágenes (VGG16)\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "        self.vgg16.classifier = nn.Sequential(*list(self.vgg16.classifier.children())[:-1])  # Quitar capa final fully connected\n",
    "\n",
    "        # Capa completamente conectada para datos tabulares\n",
    "        self.fc_tabular = nn.Sequential(\n",
    "            nn.Linear(num_tabular_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Capa final para combinar características visuales y tabulares\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(4096 + 64, 512),  # 4096 características de VGG16 + 64 de los datos tabulares\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)  # Predicción final de precio\n",
    "        )\n",
    "\n",
    "    def forward(self, images, tabular_data):\n",
    "        # Pasar las imágenes a través de VGG16\n",
    "        visual_features = self.vgg16(images)\n",
    "        \n",
    "        # Pasar los datos tabulares a través de la red fully connected\n",
    "        tabular_features = self.fc_tabular(tabular_data)\n",
    "        \n",
    "        # Concatenar las características visuales y tabulares\n",
    "        combined_features = torch.cat((visual_features, tabular_features), dim=1)\n",
    "        \n",
    "        # Pasar por la capa final para la predicción\n",
    "        output = self.fc_combined(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        return torch.sqrt(self.mse(yhat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado para combinar imágenes y datos tabulares\n",
    "class HousePriceDataset(Dataset):\n",
    "    def __init__(self, dataset, image_folder, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        img_name = row['mosaic_image']\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')  # Asegurarse de que la imagen está en formato RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Aplicamos la transformación para convertir a tensor\n",
    "\n",
    "        tabular_data = row[['bedrooms', 'bathrooms', 'area', 'zipcode']].values.astype(np.float32)\n",
    "        price = row['price']\n",
    "\n",
    "        return image, torch.tensor(tabular_data), torch.tensor(price, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo (modificado para GPU)\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = RMSELoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, tabular_data, prices in train_loader:\n",
    "            # Mover los datos a la GPU\n",
    "            images = images.to(device)\n",
    "            tabular_data = tabular_data.to(device)\n",
    "            prices = prices.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, tabular_data)\n",
    "            loss = criterion(outputs.squeeze(), prices)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    print('Entrenamiento completo')\n",
    "\n",
    "# Evaluación del modelo (modificado para GPU)\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, tabular_data, prices in test_loader:\n",
    "            # Mover los datos a la GPU\n",
    "            images = images.to(device)\n",
    "            tabular_data = tabular_data.to(device)\n",
    "            prices = prices.to(device)\n",
    "\n",
    "            outputs = model(images, tabular_data)\n",
    "\n",
    "            # Asegurarse de que las salidas sean de la forma correcta\n",
    "            outputs = outputs.cpu().numpy()  # Convertir a numpy\n",
    "            prices = prices.cpu().numpy()    # Convertir a numpy\n",
    "\n",
    "            # Si las dimensiones de outputs son 0D (escalar), convertimos a 1D\n",
    "            if outputs.ndim == 0:\n",
    "                outputs = np.expand_dims(outputs, axis=0)\n",
    "            if prices.ndim == 0:\n",
    "                prices = np.expand_dims(prices, axis=0)\n",
    "\n",
    "            predictions.extend(outputs)  # No usar squeeze() aquí\n",
    "            actuals.extend(prices)\n",
    "\n",
    "    # Calcular MAE\n",
    "    mae = np.mean(np.abs(np.array(predictions) - np.array(actuals)))\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las características tabulares\n",
    "scaler = MinMaxScaler()\n",
    "dataset[['bedrooms', 'bathrooms', 'area', 'zipcode']] = scaler.fit_transform(\n",
    "    dataset[['bedrooms', 'bathrooms', 'area', 'zipcode']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 322\n",
      "Validation samples: 81\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = HousePriceDataset(train_data, img_path, transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]))\n",
    "\n",
    "val_dataset = HousePriceDataset(val_data, img_path, transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]))\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 21\n",
      "Validation dataset size: 6\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_loader)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lab.Comp 1\\Desktop\\Uriel-TT\\houses-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lab.Comp 1\\Desktop\\Uriel-TT\\houses-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Instanciar el modelo y entrenarlo\n",
    "num_tabular_features = 4\n",
    "model = HousePriceModelVGG16(num_tabular_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 137840.0228794643\n",
      "Epoch 2/100, Loss: 75986.66496930804\n",
      "Epoch 3/100, Loss: 96250.14453125\n",
      "Epoch 4/100, Loss: 99313.22163318453\n",
      "Epoch 5/100, Loss: 95921.18257068453\n",
      "Epoch 6/100, Loss: 92013.44475446429\n",
      "Epoch 7/100, Loss: 87197.30826822917\n",
      "Epoch 8/100, Loss: 73914.07012648809\n",
      "Epoch 9/100, Loss: 75226.98009672618\n",
      "Epoch 10/100, Loss: 88067.48251488095\n",
      "Epoch 11/100, Loss: 105683.96316964286\n",
      "Epoch 12/100, Loss: 87666.05143229167\n",
      "Epoch 13/100, Loss: 70674.97340029762\n",
      "Epoch 14/100, Loss: 70279.4453125\n",
      "Epoch 15/100, Loss: 84587.79892113095\n",
      "Epoch 16/100, Loss: 68374.86328125\n",
      "Epoch 17/100, Loss: 93073.74683779762\n",
      "Epoch 18/100, Loss: 74653.28497023809\n",
      "Epoch 19/100, Loss: 89014.99293154762\n",
      "Epoch 20/100, Loss: 80169.80115327382\n",
      "Epoch 21/100, Loss: 103015.53292410714\n",
      "Epoch 22/100, Loss: 140868.85751488095\n",
      "Epoch 23/100, Loss: 119680.16592261905\n",
      "Epoch 24/100, Loss: 77204.58742559524\n",
      "Epoch 25/100, Loss: 74936.28218005953\n",
      "Epoch 26/100, Loss: 75021.75093005953\n",
      "Epoch 27/100, Loss: 80565.28869047618\n",
      "Epoch 28/100, Loss: 75900.42261904762\n",
      "Epoch 29/100, Loss: 68338.61030505953\n",
      "Epoch 30/100, Loss: 93406.77883184524\n",
      "Epoch 31/100, Loss: 103900.23344494047\n",
      "Epoch 32/100, Loss: 98877.26525297618\n",
      "Epoch 33/100, Loss: 105617.52752976191\n",
      "Epoch 34/100, Loss: 75908.39589146206\n",
      "Epoch 35/100, Loss: 76057.34895833333\n",
      "Epoch 36/100, Loss: 74717.16220238095\n",
      "Epoch 37/100, Loss: 63402.86309523809\n",
      "Epoch 38/100, Loss: 79256.97116815476\n",
      "Epoch 39/100, Loss: 68052.43312872024\n",
      "Epoch 40/100, Loss: 79445.64787946429\n",
      "Epoch 41/100, Loss: 59705.46661086309\n",
      "Epoch 42/100, Loss: 72964.40922619047\n",
      "Epoch 43/100, Loss: 84939.13234747024\n",
      "Epoch 44/100, Loss: 78700.86830357143\n",
      "Epoch 45/100, Loss: 93074.53952752976\n",
      "Epoch 46/100, Loss: 90290.01460193453\n",
      "Epoch 47/100, Loss: 120848.73028273809\n",
      "Epoch 48/100, Loss: 108183.61197916667\n",
      "Epoch 49/100, Loss: 66728.79268973214\n",
      "Epoch 50/100, Loss: 91444.85379464286\n",
      "Epoch 51/100, Loss: 71992.95182291667\n",
      "Epoch 52/100, Loss: 81730.84486607143\n",
      "Epoch 53/100, Loss: 77547.50130208333\n",
      "Epoch 54/100, Loss: 99754.88318452382\n",
      "Epoch 55/100, Loss: 107808.57087053571\n",
      "Epoch 56/100, Loss: 87964.263671875\n",
      "Epoch 57/100, Loss: 83659.18796502976\n",
      "Epoch 58/100, Loss: 70227.18654668899\n",
      "Epoch 59/100, Loss: 66664.15792410714\n",
      "Epoch 60/100, Loss: 83731.70256696429\n",
      "Epoch 61/100, Loss: 72284.37965029762\n",
      "Epoch 62/100, Loss: 68909.06408110118\n",
      "Epoch 63/100, Loss: 63141.69066220238\n",
      "Epoch 64/100, Loss: 82200.63262648809\n",
      "Epoch 65/100, Loss: 89143.47488839286\n",
      "Epoch 66/100, Loss: 86523.08165922618\n",
      "Epoch 67/100, Loss: 65823.1640625\n",
      "Epoch 68/100, Loss: 61738.12409319197\n",
      "Epoch 69/100, Loss: 66334.73270089286\n",
      "Epoch 70/100, Loss: 80584.92336309524\n",
      "Epoch 71/100, Loss: 70570.09356398809\n",
      "Epoch 72/100, Loss: 86914.60230654762\n",
      "Epoch 73/100, Loss: 61748.30515252976\n",
      "Epoch 74/100, Loss: 63445.69624255953\n",
      "Epoch 75/100, Loss: 80810.79203869047\n",
      "Epoch 76/100, Loss: 66034.42373511905\n",
      "Epoch 77/100, Loss: 79075.26450892857\n",
      "Epoch 78/100, Loss: 74110.20386904762\n",
      "Epoch 79/100, Loss: 77273.20712425595\n",
      "Epoch 80/100, Loss: 67587.01767113095\n",
      "Epoch 81/100, Loss: 73582.95331101191\n",
      "Epoch 82/100, Loss: 72451.52018229167\n",
      "Epoch 83/100, Loss: 73009.29259672618\n",
      "Epoch 84/100, Loss: 73065.17671130953\n",
      "Epoch 85/100, Loss: 65065.01283482143\n",
      "Epoch 86/100, Loss: 71300.24609375\n",
      "Epoch 87/100, Loss: 76451.38969494047\n",
      "Epoch 88/100, Loss: 74673.00818452382\n",
      "Epoch 89/100, Loss: 114324.86272321429\n",
      "Epoch 90/100, Loss: 78653.15401785714\n",
      "Epoch 91/100, Loss: 71802.12007068453\n",
      "Epoch 92/100, Loss: 62238.490327380954\n",
      "Epoch 93/100, Loss: 61325.14741443453\n",
      "Epoch 94/100, Loss: 67341.75158110118\n",
      "Epoch 95/100, Loss: 82406.09672619047\n",
      "Epoch 96/100, Loss: 108588.76060267857\n",
      "Epoch 97/100, Loss: 98414.95600818453\n",
      "Epoch 98/100, Loss: 78409.02920386905\n",
      "Epoch 99/100, Loss: 74302.08965773809\n",
      "Epoch 100/100, Loss: 90912.50130208333\n",
      "Entrenamiento completo\n",
      "Mean Absolute Error (MAE): 281616.53125\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "train_model(model, train_loader, val_loader, num_epochs=100)\n",
    "\n",
    "# Evaluación del modelo\n",
    "evaluate_model(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
