{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWe6zp_gysRy"
      },
      "source": [
        "## Instalar bibliotecas necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu_ManEgywJL",
        "outputId": "49d74aca-d461-419b-a261-00ca5181333f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch                    # Install the PyTorch library for deep learning.\n",
        "!pip install pandas                   # Install the Pandas library for data manipulation.\n",
        "!pip install scikit-learn             # Install scikit-learn for machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbLbSrG-y31h"
      },
      "source": [
        "## Acceder a drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zfyNtyQy6bZ",
        "outputId": "2e55be2b-deb4-4989-d8f7-6a3b66410be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "dataset.csv  Imagenes  test.csv  train.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montar Google Drive en /content/drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Listar archivos en el directorio ra√≠z de Google Drive\n",
        "!ls '/content/drive/MyDrive/Dataset/Dataset/Mosaico'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JnAV0pdyimA"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7dx7V1DAyimG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader,ConcatDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import datetime\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uva53gxUyimI"
      },
      "source": [
        "## Lectura de dataset train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RyLYLTz6yimJ",
        "outputId": "86fac903-1ee2-4a6a-90c7-29bdc821d0b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   bathrooms  bedrooms  area  zipcode mosaic_image    price\n",
              "0        5.0         5  3816    92880      284.png   589900\n",
              "1        2.0         2  1440    92276      347.png   106000\n",
              "2        3.0         4  1625    93510      440.png   639000\n",
              "3        3.0         4  2454    93510      421.png  5858000\n",
              "4        4.5         4  4038    92677      149.png  1795000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee629a71-6393-4846-9b04-058b4ea4bcef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>area</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>mosaic_image</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>3816</td>\n",
              "      <td>92880</td>\n",
              "      <td>284.png</td>\n",
              "      <td>589900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1440</td>\n",
              "      <td>92276</td>\n",
              "      <td>347.png</td>\n",
              "      <td>106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1625</td>\n",
              "      <td>93510</td>\n",
              "      <td>440.png</td>\n",
              "      <td>639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2454</td>\n",
              "      <td>93510</td>\n",
              "      <td>421.png</td>\n",
              "      <td>5858000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "      <td>4038</td>\n",
              "      <td>92677</td>\n",
              "      <td>149.png</td>\n",
              "      <td>1795000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee629a71-6393-4846-9b04-058b4ea4bcef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee629a71-6393-4846-9b04-058b4ea4bcef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee629a71-6393-4846-9b04-058b4ea4bcef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4250bd96-4e51-452b-8992-608b6f537be9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4250bd96-4e51-452b-8992-608b6f537be9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4250bd96-4e51-452b-8992-608b6f537be9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 428,\n  \"fields\": [\n    {\n      \"column\": \"bathrooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.991133865244394,\n        \"min\": 1.0,\n        \"max\": 7.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          5.5,\n          7.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bedrooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"area\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1236,\n        \"min\": 701,\n        \"max\": 9583,\n        \"num_unique_values\": 353,\n        \"samples\": [\n          4464,\n          6000,\n          1472\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"zipcode\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6346,\n        \"min\": 36372,\n        \"max\": 98021,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          81418,\n          95220,\n          93720\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mosaic_image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 428,\n        \"samples\": [\n          \"466.png\",\n          \"299.png\",\n          \"449.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 537505,\n        \"min\": 22000,\n        \"max\": 5858000,\n        \"num_unique_values\": 321,\n        \"samples\": [\n          569000,\n          585000,\n          1399999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# --> Desde local\n",
        "# train = pd.read_csv('../Dataset/Mosaico/train.csv')\n",
        "# test = pd.read_csv('../Dataset/Mosaico/test.csv')\n",
        "\n",
        "# --> Desde google\n",
        "train = pd.read_csv('/content/drive/MyDrive/Dataset/Dataset/Mosaico/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Dataset/Dataset/Mosaico/test.csv')\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl1wVzsTyimK",
        "outputId": "4cfe2c8e-c8c1-441c-eaa5-cac999524e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 428 entries, 0 to 427\n",
            "Data columns (total 6 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   bathrooms     428 non-null    float64\n",
            " 1   bedrooms      428 non-null    int64  \n",
            " 2   area          428 non-null    int64  \n",
            " 3   zipcode       428 non-null    int64  \n",
            " 4   mosaic_image  428 non-null    object \n",
            " 5   price         428 non-null    int64  \n",
            "dtypes: float64(1), int64(4), object(1)\n",
            "memory usage: 20.2+ KB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5cX8xRbyimL"
      },
      "source": [
        "## Preparar Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mgP3MQY0yimL"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df                # DataFrame que contiene los datos\n",
        "        self.transform = transform  # Transformaciones opcionales para las im√°genes\n",
        "\n",
        "        # --> Seleccionar las columnas que se van a estandarizar\n",
        "        self.numeric_features = ['bedrooms', 'bathrooms', 'area', 'zipcode']\n",
        "\n",
        "        # --> Inicializar el estandarizador y ajustarlo a los datos\n",
        "        self.scaler = StandardScaler()\n",
        "        self.df[self.numeric_features] = self.scaler.fit_transform(self.df[self.numeric_features])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # --> M√©todo para obtener una muestra del conjunto de datos dado un √≠ndice\n",
        "    def __getitem__(self, idx):\n",
        "        # --> Obtener las rutas de la imagen\n",
        "        img_path = self.df.loc[idx, 'mosaic_image']\n",
        "\n",
        "        # --> Leer las im√°genes desde las rutas especificadas y convertirlas a tensores float\n",
        "        mosaic_image = read_image(f\"/content/drive/MyDrive/Dataset/Dataset/Mosaico/Imagenes/{img_path}\").float()\n",
        "        # mosaic_image = read_image(f\"../Dataset/Mosaico/Imagenes/{img_path}\").float()\n",
        "\n",
        "        # --> Obtener los valores de las caracter√≠sticas num√©ricas estandarizadas desde el DataFrame\n",
        "        bathrooms = torch.tensor(self.df.loc[idx, 'bathrooms'], dtype=torch.float32)\n",
        "        bedrooms = torch.tensor(self.df.loc[idx, 'bedrooms'], dtype=torch.float32)\n",
        "        area = torch.tensor(self.df.loc[idx, 'area'], dtype=torch.float32)\n",
        "        zipcode = torch.tensor(self.df.loc[idx, 'zipcode'], dtype=torch.float32)\n",
        "        price = torch.tensor(self.df.loc[idx, 'price'], dtype=torch.float32)\n",
        "\n",
        "        # --> Aplicar transformaciones a las im√°genes si se proporcionan\n",
        "        if self.transform:\n",
        "            mosaic_image = self.transform(mosaic_image)\n",
        "\n",
        "        # --> Concatenar las caracter√≠sticas num√©ricas estandarizadas en un solo tensor\n",
        "        numeric_features = torch.hstack((bathrooms, bedrooms, area, zipcode))\n",
        "\n",
        "        # Devolver las im√°genes, las caracter√≠sticas num√©ricas y el precio\n",
        "        return mosaic_image, numeric_features, price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DShaVfOfyimN"
      },
      "source": [
        "## Aumentar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N4pK35EPyimN"
      },
      "outputs": [],
      "source": [
        "data1=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize(256),\n",
        "                                                                transforms.RandomCrop((224,224)),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))\n",
        "data2=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize(256),\n",
        "                                                                transforms.CenterCrop((224,224)),\n",
        "                                                                transforms.ColorJitter(\n",
        "                                                                    brightness=0.6, contrast=1.3, saturation=1.1, hue=.2),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718)),\n",
        "                                                                    ]))\n",
        "data3=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize(256),\n",
        "                                                                transforms.CenterCrop((224,224)),\n",
        "                                                                transforms.ColorJitter(\n",
        "                                                                    brightness=1.3, contrast=.5, saturation=.5),\n",
        "                                                                transforms.RandomHorizontalFlip(p=1),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))\n",
        "data4=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize(256),\n",
        "                                                                transforms.CenterCrop((224,224)),\n",
        "                                                                transforms.ColorJitter(\n",
        "                                                                    brightness=.5, contrast=.5, saturation=.5),\n",
        "                                                                transforms.RandomVerticalFlip(p=1),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))\n",
        "data5=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize((400,400)),\n",
        "                                                                transforms.RandomRotation(degrees = 160),\n",
        "                                                                transforms.CenterCrop((224,224)),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))\n",
        "data6=CustomImageDataset(df=train,transform=transforms.Compose([transforms.Resize(400),\n",
        "                                                                transforms.RandomRotation(degrees = 45),\n",
        "                                                                transforms.CenterCrop((224,224)),\n",
        "                                                                transforms.RandomHorizontalFlip(p=1),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))\n",
        "data7=CustomImageDataset(df=train,transform=transforms.Compose([transforms.RandomVerticalFlip(p=1),\n",
        "                                                                transforms.RandomHorizontalFlip(p=1),\n",
        "                                                                transforms.Resize((224,224)),\n",
        "                                                                transforms.ColorJitter(\n",
        "                                                                    brightness=.5, contrast=.5, saturation=.5),\n",
        "                                                                transforms.Normalize(\n",
        "                                                                    (58.0583, 55.1679, 52.9831),\n",
        "                                                                    (85.9875, 82.3628, 80.8718))\n",
        "                                                                    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NntNlVD6_qyk"
      },
      "source": [
        "## Dividir en train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P8fBoZaOyimO"
      },
      "outputs": [],
      "source": [
        "# Crear un conjunto de datos combinado\n",
        "conjunto_datos = ConcatDataset((data1,data2,data3,data4,data5,data6,data7))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tw4-OX_yimP",
        "outputId": "d533255d-3a87-4202-8417-7abab6dcc3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 2396 Val: 600\n"
          ]
        }
      ],
      "source": [
        "# Determinar el tama√±o de los conjuntos de entrenamiento y validaci√≥n\n",
        "valor_train = int(0.8*len(conjunto_datos))\n",
        "valor_val = len(conjunto_datos)-valor_train\n",
        "\n",
        "print(f\"Train: {valor_train} Val: {valor_val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ULtEKcF2yimP"
      },
      "outputs": [],
      "source": [
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y validaci√≥n\n",
        "\n",
        "train_data,val_data=torch.utils.data.random_split(ConcatDataset((data1,data2,data3,data4,data5,data6,data7)),[valor_train,valor_val])\n",
        "\n",
        "del data1,data2,data3,data4,data5,data6,data7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "M3n4JrQeyimQ"
      },
      "outputs": [],
      "source": [
        "# Crear los dataloaders\n",
        "\n",
        "batch_size=64\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j51Oe8M8yimQ"
      },
      "source": [
        "## Red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mmgQltqQyimQ"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.image_features_ = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Conv2d(16, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            # nn.Conv2d(128, 256, kernel_size=5, padding=2),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            # nn.Dropout(),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.numeric_features_ = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(64, 64*3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(64*3, 64*3*3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.combined_features_ = nn.Sequential(\n",
        "            # nn.Linear(64*3*3*2, 64*3*3*2*2),\n",
        "            nn.Linear(2880, 64*3*3*2*2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Linear(64*3*3*2*2, 64*3*3*2),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(64*3*3*2, 64),\n",
        "            nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x,y):\n",
        "        x = self.image_features_(x)\n",
        "        x=x.view(-1, 64*6*6)\n",
        "        y=self.numeric_features_(y)\n",
        "        z=torch.cat((x,y),1)\n",
        "        z=self.combined_features_(z)\n",
        "        return z.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OYvNK35Fp0WC"
      },
      "outputs": [],
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        return torch.sqrt(self.mse(yhat, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s66heXz0yimR",
        "outputId": "2d9204a1-7e94-48e9-ce81-ef0c5bc56d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Definir el dispositivo\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))\n",
        "\n",
        "# Definir el modelo, la funci√≥n de p√©rdida y el optimizador\n",
        "model = NeuralNetwork().to(device)              # Modelo de red neuronal mandarlo al dispositivo (GPU)\n",
        "optimizer=optim.Adam(model.parameters(),1e-3)   # Adam optimizer para optimizaci√≥n\n",
        "# loss_fn=nn.MSELoss()                          # Mean Squared Error Loss para regresi√≥n\n",
        "loss_fn = RMSELoss()                            # RMSE Loss para regresi√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mB6la_wyimR"
      },
      "source": [
        "## Ciclos de entrenamiento y pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JYOx2Cn4yimR"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "    total_error = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, numeric_features, price) in enumerate(dataloader):\n",
        "            # Mandar datos al dispositivo\n",
        "            imgs = imgs.to(device)\n",
        "            numeric_features = numeric_features.to(device)\n",
        "            price = price.to(device)\n",
        "\n",
        "            # Predecir\n",
        "            pred = model(imgs, numeric_features)\n",
        "\n",
        "            # Calcular p√©rdida\n",
        "            batch_loss = loss_fn(pred, price).item()\n",
        "            test_loss += batch_loss\n",
        "            total_error += torch.abs(pred - price).sum().item()\n",
        "\n",
        "            # Imprimir la p√©rdida de cada batch\n",
        "            print(f\"Batch {batch_idx+1}/{num_batches}, Batch Loss: {batch_loss:.4f}\")\n",
        "\n",
        "    # Calcular p√©rdida y error promedio\n",
        "    test_loss /= num_batches\n",
        "    avg_error = total_error / size\n",
        "\n",
        "    print(f\"Test Error: \\n Avg loss: {test_loss:.6f}, Avg error: {avg_error:.6f} \\n\")\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "E-KgVlbwyimS"
      },
      "outputs": [],
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (imgs, numeric_features, price) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            numeric_features = numeric_features.to(device)\n",
        "            price = price.to(device)\n",
        "\n",
        "            output = model(imgs, numeric_features)\n",
        "\n",
        "            loss = loss_fn(output, price)\n",
        "\n",
        "            # L2 Regularization\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "            # Imprimir la p√©rdida en cada batch\n",
        "            print(f\"Epoch {epoch}, Batch {batch_idx+1}/{len(train_loader)}, Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "        loss_train /= len(train_loader)\n",
        "        train_losses.append(loss_train)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = test_loop(dataloader=val_loader, model=model, loss_fn=loss_fn)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Average Training loss {}'.format(datetime.datetime.now(), epoch, loss_train))\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3KNiSWyimS"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHpVDZT9yimS",
        "outputId": "7b6ad484-e896-45ef-a930-5bbc35c1691c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las √∫ltimas 5000 l√≠neas del flujo de salida.\u001b[0m\n",
            "Epoch 103, Batch 9/38, Batch Loss: 287184.3750\n",
            "Epoch 103, Batch 10/38, Batch Loss: 307996.7812\n",
            "Epoch 103, Batch 11/38, Batch Loss: 257469.4062\n",
            "Epoch 103, Batch 12/38, Batch Loss: 241612.2188\n",
            "Epoch 103, Batch 13/38, Batch Loss: 417648.9688\n",
            "Epoch 103, Batch 14/38, Batch Loss: 258630.9375\n",
            "Epoch 103, Batch 15/38, Batch Loss: 723315.5000\n",
            "Epoch 103, Batch 16/38, Batch Loss: 280710.0938\n",
            "Epoch 103, Batch 17/38, Batch Loss: 321395.5312\n",
            "Epoch 103, Batch 18/38, Batch Loss: 273458.1562\n",
            "Epoch 103, Batch 19/38, Batch Loss: 352652.4062\n",
            "Epoch 103, Batch 20/38, Batch Loss: 238289.8281\n",
            "Epoch 103, Batch 21/38, Batch Loss: 264318.2812\n",
            "Epoch 103, Batch 22/38, Batch Loss: 309211.4375\n",
            "Epoch 103, Batch 23/38, Batch Loss: 539427.8125\n",
            "Epoch 103, Batch 24/38, Batch Loss: 261191.9844\n",
            "Epoch 103, Batch 25/38, Batch Loss: 318582.0938\n",
            "Epoch 103, Batch 26/38, Batch Loss: 688825.3125\n",
            "Epoch 103, Batch 27/38, Batch Loss: 716553.6250\n",
            "Epoch 103, Batch 28/38, Batch Loss: 295162.5000\n",
            "Epoch 103, Batch 29/38, Batch Loss: 711606.8750\n",
            "Epoch 103, Batch 30/38, Batch Loss: 278760.5938\n",
            "Epoch 103, Batch 31/38, Batch Loss: 286336.9062\n",
            "Epoch 103, Batch 32/38, Batch Loss: 222005.2812\n",
            "Epoch 103, Batch 33/38, Batch Loss: 369404.7812\n",
            "Epoch 103, Batch 34/38, Batch Loss: 279282.0312\n",
            "Epoch 103, Batch 35/38, Batch Loss: 267220.2500\n",
            "Epoch 103, Batch 36/38, Batch Loss: 712220.9375\n",
            "Epoch 103, Batch 37/38, Batch Loss: 298690.7812\n",
            "Epoch 103, Batch 38/38, Batch Loss: 398971.1562\n",
            "Batch 1/10, Batch Loss: 242244.6094\n",
            "Batch 2/10, Batch Loss: 297145.9688\n",
            "Batch 3/10, Batch Loss: 388984.4062\n",
            "Batch 4/10, Batch Loss: 368797.5938\n",
            "Batch 5/10, Batch Loss: 388163.5625\n",
            "Batch 6/10, Batch Loss: 387467.5312\n",
            "Batch 7/10, Batch Loss: 340796.4375\n",
            "Batch 8/10, Batch Loss: 263598.8125\n",
            "Batch 9/10, Batch Loss: 334910.2188\n",
            "Batch 10/10, Batch Loss: 267640.5625\n",
            "Test Error: \n",
            " Avg loss: 327974.970313, Avg error: 211209.465000 \n",
            "\n",
            "Epoch 104, Batch 1/38, Batch Loss: 289628.4688\n",
            "Epoch 104, Batch 2/38, Batch Loss: 223115.9844\n",
            "Epoch 104, Batch 3/38, Batch Loss: 706073.1875\n",
            "Epoch 104, Batch 4/38, Batch Loss: 688206.8750\n",
            "Epoch 104, Batch 5/38, Batch Loss: 320173.3750\n",
            "Epoch 104, Batch 6/38, Batch Loss: 707848.3125\n",
            "Epoch 104, Batch 7/38, Batch Loss: 324182.3750\n",
            "Epoch 104, Batch 8/38, Batch Loss: 323073.5938\n",
            "Epoch 104, Batch 9/38, Batch Loss: 332881.9062\n",
            "Epoch 104, Batch 10/38, Batch Loss: 279702.3750\n",
            "Epoch 104, Batch 11/38, Batch Loss: 261378.9844\n",
            "Epoch 104, Batch 12/38, Batch Loss: 285433.6250\n",
            "Epoch 104, Batch 13/38, Batch Loss: 332494.3750\n",
            "Epoch 104, Batch 14/38, Batch Loss: 236751.9844\n",
            "Epoch 104, Batch 15/38, Batch Loss: 311781.3125\n",
            "Epoch 104, Batch 16/38, Batch Loss: 302326.0938\n",
            "Epoch 104, Batch 17/38, Batch Loss: 445990.4375\n",
            "Epoch 104, Batch 18/38, Batch Loss: 299123.7500\n",
            "Epoch 104, Batch 19/38, Batch Loss: 712895.5625\n",
            "Epoch 104, Batch 20/38, Batch Loss: 223440.5938\n",
            "Epoch 104, Batch 21/38, Batch Loss: 312068.3750\n",
            "Epoch 104, Batch 22/38, Batch Loss: 216021.2344\n",
            "Epoch 104, Batch 23/38, Batch Loss: 381404.4688\n",
            "Epoch 104, Batch 24/38, Batch Loss: 254150.6562\n",
            "Epoch 104, Batch 25/38, Batch Loss: 406011.8438\n",
            "Epoch 104, Batch 26/38, Batch Loss: 282629.3750\n",
            "Epoch 104, Batch 27/38, Batch Loss: 279199.4375\n",
            "Epoch 104, Batch 28/38, Batch Loss: 349949.8125\n",
            "Epoch 104, Batch 29/38, Batch Loss: 266694.1250\n",
            "Epoch 104, Batch 30/38, Batch Loss: 704214.8125\n",
            "Epoch 104, Batch 31/38, Batch Loss: 262394.0312\n",
            "Epoch 104, Batch 32/38, Batch Loss: 338477.5625\n",
            "Epoch 104, Batch 33/38, Batch Loss: 241898.9219\n",
            "Epoch 104, Batch 34/38, Batch Loss: 200759.2188\n",
            "Epoch 104, Batch 35/38, Batch Loss: 260872.0156\n",
            "Epoch 104, Batch 36/38, Batch Loss: 294811.3438\n",
            "Epoch 104, Batch 37/38, Batch Loss: 980213.8750\n",
            "Epoch 104, Batch 38/38, Batch Loss: 202594.1094\n",
            "Batch 1/10, Batch Loss: 371573.7812\n",
            "Batch 2/10, Batch Loss: 265420.1562\n",
            "Batch 3/10, Batch Loss: 419349.4062\n",
            "Batch 4/10, Batch Loss: 326097.0000\n",
            "Batch 5/10, Batch Loss: 276556.7500\n",
            "Batch 6/10, Batch Loss: 264769.4688\n",
            "Batch 7/10, Batch Loss: 418879.8125\n",
            "Batch 8/10, Batch Loss: 225567.1562\n",
            "Batch 9/10, Batch Loss: 385637.5000\n",
            "Batch 10/10, Batch Loss: 211234.7969\n",
            "Test Error: \n",
            " Avg loss: 316508.582813, Avg error: 207835.382917 \n",
            "\n",
            "Epoch 105, Batch 1/38, Batch Loss: 708258.0000\n",
            "Epoch 105, Batch 2/38, Batch Loss: 309434.5312\n",
            "Epoch 105, Batch 3/38, Batch Loss: 343965.5938\n",
            "Epoch 105, Batch 4/38, Batch Loss: 270209.1250\n",
            "Epoch 105, Batch 5/38, Batch Loss: 681804.6250\n",
            "Epoch 105, Batch 6/38, Batch Loss: 346119.5312\n",
            "Epoch 105, Batch 7/38, Batch Loss: 267343.5625\n",
            "Epoch 105, Batch 8/38, Batch Loss: 321583.2500\n",
            "Epoch 105, Batch 9/38, Batch Loss: 728656.3125\n",
            "Epoch 105, Batch 10/38, Batch Loss: 670399.6875\n",
            "Epoch 105, Batch 11/38, Batch Loss: 685592.5625\n",
            "Epoch 105, Batch 12/38, Batch Loss: 298595.7188\n",
            "Epoch 105, Batch 13/38, Batch Loss: 274965.1562\n",
            "Epoch 105, Batch 14/38, Batch Loss: 256508.8594\n",
            "Epoch 105, Batch 15/38, Batch Loss: 261321.7344\n",
            "Epoch 105, Batch 16/38, Batch Loss: 398298.7500\n",
            "Epoch 105, Batch 17/38, Batch Loss: 279711.4688\n",
            "Epoch 105, Batch 18/38, Batch Loss: 264858.4062\n",
            "Epoch 105, Batch 19/38, Batch Loss: 330681.3125\n",
            "Epoch 105, Batch 20/38, Batch Loss: 271588.9688\n",
            "Epoch 105, Batch 21/38, Batch Loss: 277354.7812\n",
            "Epoch 105, Batch 22/38, Batch Loss: 738893.6875\n",
            "Epoch 105, Batch 23/38, Batch Loss: 352867.0625\n",
            "Epoch 105, Batch 24/38, Batch Loss: 332484.3750\n",
            "Epoch 105, Batch 25/38, Batch Loss: 283184.6875\n",
            "Epoch 105, Batch 26/38, Batch Loss: 322563.6562\n",
            "Epoch 105, Batch 27/38, Batch Loss: 237961.4219\n",
            "Epoch 105, Batch 28/38, Batch Loss: 264595.1250\n",
            "Epoch 105, Batch 29/38, Batch Loss: 722339.2500\n",
            "Epoch 105, Batch 30/38, Batch Loss: 224426.6719\n",
            "Epoch 105, Batch 31/38, Batch Loss: 358561.3750\n",
            "Epoch 105, Batch 32/38, Batch Loss: 420075.8750\n",
            "Epoch 105, Batch 33/38, Batch Loss: 320635.2500\n",
            "Epoch 105, Batch 34/38, Batch Loss: 322548.5938\n",
            "Epoch 105, Batch 35/38, Batch Loss: 331985.6250\n",
            "Epoch 105, Batch 36/38, Batch Loss: 324927.5625\n",
            "Epoch 105, Batch 37/38, Batch Loss: 249473.4375\n",
            "Epoch 105, Batch 38/38, Batch Loss: 239317.5312\n",
            "Batch 1/10, Batch Loss: 469636.4062\n",
            "Batch 2/10, Batch Loss: 304759.8438\n",
            "Batch 3/10, Batch Loss: 312809.7812\n",
            "Batch 4/10, Batch Loss: 331147.1250\n",
            "Batch 5/10, Batch Loss: 304783.2812\n",
            "Batch 6/10, Batch Loss: 191508.7656\n",
            "Batch 7/10, Batch Loss: 407060.0312\n",
            "Batch 8/10, Batch Loss: 232355.0625\n",
            "Batch 9/10, Batch Loss: 240106.8594\n",
            "Batch 10/10, Batch Loss: 270455.2812\n",
            "Test Error: \n",
            " Avg loss: 306462.243750, Avg error: 205789.198333 \n",
            "\n",
            "Epoch 106, Batch 1/38, Batch Loss: 292647.3125\n",
            "Epoch 106, Batch 2/38, Batch Loss: 250174.6562\n",
            "Epoch 106, Batch 3/38, Batch Loss: 316321.7188\n",
            "Epoch 106, Batch 4/38, Batch Loss: 289030.5312\n",
            "Epoch 106, Batch 5/38, Batch Loss: 347040.9688\n",
            "Epoch 106, Batch 6/38, Batch Loss: 272241.9688\n",
            "Epoch 106, Batch 7/38, Batch Loss: 268057.3438\n",
            "Epoch 106, Batch 8/38, Batch Loss: 687554.1875\n",
            "Epoch 106, Batch 9/38, Batch Loss: 334594.4062\n",
            "Epoch 106, Batch 10/38, Batch Loss: 291195.1875\n",
            "Epoch 106, Batch 11/38, Batch Loss: 281500.0625\n",
            "Epoch 106, Batch 12/38, Batch Loss: 307540.5000\n",
            "Epoch 106, Batch 13/38, Batch Loss: 358722.3750\n",
            "Epoch 106, Batch 14/38, Batch Loss: 727768.6250\n",
            "Epoch 106, Batch 15/38, Batch Loss: 297078.3125\n",
            "Epoch 106, Batch 16/38, Batch Loss: 309451.6562\n",
            "Epoch 106, Batch 17/38, Batch Loss: 252904.9531\n",
            "Epoch 106, Batch 18/38, Batch Loss: 344824.0312\n",
            "Epoch 106, Batch 19/38, Batch Loss: 263777.3438\n",
            "Epoch 106, Batch 20/38, Batch Loss: 286057.9375\n",
            "Epoch 106, Batch 21/38, Batch Loss: 282951.7188\n",
            "Epoch 106, Batch 22/38, Batch Loss: 243992.4375\n",
            "Epoch 106, Batch 23/38, Batch Loss: 276617.1562\n",
            "Epoch 106, Batch 24/38, Batch Loss: 1162430.6250\n",
            "Epoch 106, Batch 25/38, Batch Loss: 452447.6250\n",
            "Epoch 106, Batch 26/38, Batch Loss: 294329.9375\n",
            "Epoch 106, Batch 27/38, Batch Loss: 317691.5312\n",
            "Epoch 106, Batch 28/38, Batch Loss: 305940.0625\n",
            "Epoch 106, Batch 29/38, Batch Loss: 440000.1875\n",
            "Epoch 106, Batch 30/38, Batch Loss: 302460.2500\n",
            "Epoch 106, Batch 31/38, Batch Loss: 283670.3750\n",
            "Epoch 106, Batch 32/38, Batch Loss: 688976.0000\n",
            "Epoch 106, Batch 33/38, Batch Loss: 270468.4688\n",
            "Epoch 106, Batch 34/38, Batch Loss: 757969.7500\n",
            "Epoch 106, Batch 35/38, Batch Loss: 316744.9688\n",
            "Epoch 106, Batch 36/38, Batch Loss: 370821.3750\n",
            "Epoch 106, Batch 37/38, Batch Loss: 308395.5625\n",
            "Epoch 106, Batch 38/38, Batch Loss: 221328.2031\n",
            "Batch 1/10, Batch Loss: 341675.4062\n",
            "Batch 2/10, Batch Loss: 209385.8906\n",
            "Batch 3/10, Batch Loss: 267039.3438\n",
            "Batch 4/10, Batch Loss: 230811.1250\n",
            "Batch 5/10, Batch Loss: 280590.7188\n",
            "Batch 6/10, Batch Loss: 379982.6250\n",
            "Batch 7/10, Batch Loss: 324615.9688\n",
            "Batch 8/10, Batch Loss: 369864.8750\n",
            "Batch 9/10, Batch Loss: 404753.4688\n",
            "Batch 10/10, Batch Loss: 286515.3750\n",
            "Test Error: \n",
            " Avg loss: 309523.479687, Avg error: 206504.043333 \n",
            "\n",
            "Epoch 107, Batch 1/38, Batch Loss: 684561.9375\n",
            "Epoch 107, Batch 2/38, Batch Loss: 298013.5312\n",
            "Epoch 107, Batch 3/38, Batch Loss: 293017.4375\n",
            "Epoch 107, Batch 4/38, Batch Loss: 285497.9375\n",
            "Epoch 107, Batch 5/38, Batch Loss: 340669.5312\n",
            "Epoch 107, Batch 6/38, Batch Loss: 246682.7969\n",
            "Epoch 107, Batch 7/38, Batch Loss: 265654.3750\n",
            "Epoch 107, Batch 8/38, Batch Loss: 317478.0312\n",
            "Epoch 107, Batch 9/38, Batch Loss: 412119.5312\n",
            "Epoch 107, Batch 10/38, Batch Loss: 459805.9062\n",
            "Epoch 107, Batch 11/38, Batch Loss: 258694.1406\n",
            "Epoch 107, Batch 12/38, Batch Loss: 272706.0312\n",
            "Epoch 107, Batch 13/38, Batch Loss: 715234.6250\n",
            "Epoch 107, Batch 14/38, Batch Loss: 286503.4375\n",
            "Epoch 107, Batch 15/38, Batch Loss: 256900.1250\n",
            "Epoch 107, Batch 16/38, Batch Loss: 355776.0625\n",
            "Epoch 107, Batch 17/38, Batch Loss: 308484.4375\n",
            "Epoch 107, Batch 18/38, Batch Loss: 301677.1562\n",
            "Epoch 107, Batch 19/38, Batch Loss: 323068.0312\n",
            "Epoch 107, Batch 20/38, Batch Loss: 342526.2500\n",
            "Epoch 107, Batch 21/38, Batch Loss: 258952.1250\n",
            "Epoch 107, Batch 22/38, Batch Loss: 449639.5312\n",
            "Epoch 107, Batch 23/38, Batch Loss: 232780.7500\n",
            "Epoch 107, Batch 24/38, Batch Loss: 268730.6875\n",
            "Epoch 107, Batch 25/38, Batch Loss: 943078.8125\n",
            "Epoch 107, Batch 26/38, Batch Loss: 349683.5625\n",
            "Epoch 107, Batch 27/38, Batch Loss: 704030.0625\n",
            "Epoch 107, Batch 28/38, Batch Loss: 684934.5000\n",
            "Epoch 107, Batch 29/38, Batch Loss: 321005.3438\n",
            "Epoch 107, Batch 30/38, Batch Loss: 379414.3438\n",
            "Epoch 107, Batch 31/38, Batch Loss: 673235.8750\n",
            "Epoch 107, Batch 32/38, Batch Loss: 410473.1562\n",
            "Epoch 107, Batch 33/38, Batch Loss: 256125.3906\n",
            "Epoch 107, Batch 34/38, Batch Loss: 237422.8281\n",
            "Epoch 107, Batch 35/38, Batch Loss: 240200.3594\n",
            "Epoch 107, Batch 36/38, Batch Loss: 232769.8906\n",
            "Epoch 107, Batch 37/38, Batch Loss: 250540.5469\n",
            "Epoch 107, Batch 38/38, Batch Loss: 239449.6250\n",
            "Batch 1/10, Batch Loss: 458587.8125\n",
            "Batch 2/10, Batch Loss: 306164.5000\n",
            "Batch 3/10, Batch Loss: 280391.1875\n",
            "Batch 4/10, Batch Loss: 312957.8125\n",
            "Batch 5/10, Batch Loss: 292642.7188\n",
            "Batch 6/10, Batch Loss: 363928.4688\n",
            "Batch 7/10, Batch Loss: 425288.8750\n",
            "Batch 8/10, Batch Loss: 310132.9062\n",
            "Batch 9/10, Batch Loss: 197068.0312\n",
            "Batch 10/10, Batch Loss: 186819.4375\n",
            "Test Error: \n",
            " Avg loss: 313398.175000, Avg error: 209254.250833 \n",
            "\n",
            "Epoch 108, Batch 1/38, Batch Loss: 292997.3750\n",
            "Epoch 108, Batch 2/38, Batch Loss: 321899.2188\n",
            "Epoch 108, Batch 3/38, Batch Loss: 336425.3438\n",
            "Epoch 108, Batch 4/38, Batch Loss: 219747.2031\n",
            "Epoch 108, Batch 5/38, Batch Loss: 282401.7500\n",
            "Epoch 108, Batch 6/38, Batch Loss: 289940.7188\n",
            "Epoch 108, Batch 7/38, Batch Loss: 505587.9375\n",
            "Epoch 108, Batch 8/38, Batch Loss: 367331.0312\n",
            "Epoch 108, Batch 9/38, Batch Loss: 934283.2500\n",
            "Epoch 108, Batch 10/38, Batch Loss: 663029.4375\n",
            "Epoch 108, Batch 11/38, Batch Loss: 406802.2812\n",
            "Epoch 108, Batch 12/38, Batch Loss: 396873.0938\n",
            "Epoch 108, Batch 13/38, Batch Loss: 330996.4062\n",
            "Epoch 108, Batch 14/38, Batch Loss: 693165.5625\n",
            "Epoch 108, Batch 15/38, Batch Loss: 250968.1875\n",
            "Epoch 108, Batch 16/38, Batch Loss: 333763.6562\n",
            "Epoch 108, Batch 17/38, Batch Loss: 283594.0625\n",
            "Epoch 108, Batch 18/38, Batch Loss: 293551.0312\n",
            "Epoch 108, Batch 19/38, Batch Loss: 339505.6562\n",
            "Epoch 108, Batch 20/38, Batch Loss: 340446.8750\n",
            "Epoch 108, Batch 21/38, Batch Loss: 303181.1875\n",
            "Epoch 108, Batch 22/38, Batch Loss: 264199.4375\n",
            "Epoch 108, Batch 23/38, Batch Loss: 706621.4375\n",
            "Epoch 108, Batch 24/38, Batch Loss: 292431.1562\n",
            "Epoch 108, Batch 25/38, Batch Loss: 473962.1250\n",
            "Epoch 108, Batch 26/38, Batch Loss: 310444.5000\n",
            "Epoch 108, Batch 27/38, Batch Loss: 204550.6719\n",
            "Epoch 108, Batch 28/38, Batch Loss: 307128.0000\n",
            "Epoch 108, Batch 29/38, Batch Loss: 668791.8750\n",
            "Epoch 108, Batch 30/38, Batch Loss: 213539.9531\n",
            "Epoch 108, Batch 31/38, Batch Loss: 698652.5000\n",
            "Epoch 108, Batch 32/38, Batch Loss: 243168.6562\n",
            "Epoch 108, Batch 33/38, Batch Loss: 312010.6250\n",
            "Epoch 108, Batch 34/38, Batch Loss: 277865.5000\n",
            "Epoch 108, Batch 35/38, Batch Loss: 304548.2188\n",
            "Epoch 108, Batch 36/38, Batch Loss: 271914.4688\n",
            "Epoch 108, Batch 37/38, Batch Loss: 359106.5625\n",
            "Epoch 108, Batch 38/38, Batch Loss: 233540.0625\n",
            "Batch 1/10, Batch Loss: 328837.3750\n",
            "Batch 2/10, Batch Loss: 292129.5938\n",
            "Batch 3/10, Batch Loss: 394721.0312\n",
            "Batch 4/10, Batch Loss: 387914.9375\n",
            "Batch 5/10, Batch Loss: 235054.9531\n",
            "Batch 6/10, Batch Loss: 294354.3125\n",
            "Batch 7/10, Batch Loss: 375271.7812\n",
            "Batch 8/10, Batch Loss: 257981.5938\n",
            "Batch 9/10, Batch Loss: 297207.7812\n",
            "Batch 10/10, Batch Loss: 220709.5625\n",
            "Test Error: \n",
            " Avg loss: 308418.292187, Avg error: 208933.783333 \n",
            "\n",
            "Epoch 109, Batch 1/38, Batch Loss: 368318.7812\n",
            "Epoch 109, Batch 2/38, Batch Loss: 319736.5000\n",
            "Epoch 109, Batch 3/38, Batch Loss: 330250.1562\n",
            "Epoch 109, Batch 4/38, Batch Loss: 334158.9062\n",
            "Epoch 109, Batch 5/38, Batch Loss: 380071.3438\n",
            "Epoch 109, Batch 6/38, Batch Loss: 273790.1875\n",
            "Epoch 109, Batch 7/38, Batch Loss: 301689.2500\n",
            "Epoch 109, Batch 8/38, Batch Loss: 346783.7500\n",
            "Epoch 109, Batch 9/38, Batch Loss: 372311.1562\n",
            "Epoch 109, Batch 10/38, Batch Loss: 239469.2812\n",
            "Epoch 109, Batch 11/38, Batch Loss: 237386.7656\n",
            "Epoch 109, Batch 12/38, Batch Loss: 418940.8438\n",
            "Epoch 109, Batch 13/38, Batch Loss: 261839.0469\n",
            "Epoch 109, Batch 14/38, Batch Loss: 669087.1875\n",
            "Epoch 109, Batch 15/38, Batch Loss: 289086.4375\n",
            "Epoch 109, Batch 16/38, Batch Loss: 321800.4375\n",
            "Epoch 109, Batch 17/38, Batch Loss: 698657.5000\n",
            "Epoch 109, Batch 18/38, Batch Loss: 383095.3125\n",
            "Epoch 109, Batch 19/38, Batch Loss: 302995.3750\n",
            "Epoch 109, Batch 20/38, Batch Loss: 312373.2500\n",
            "Epoch 109, Batch 21/38, Batch Loss: 238766.2656\n",
            "Epoch 109, Batch 22/38, Batch Loss: 276163.4062\n",
            "Epoch 109, Batch 23/38, Batch Loss: 210159.8438\n",
            "Epoch 109, Batch 24/38, Batch Loss: 254213.8281\n",
            "Epoch 109, Batch 25/38, Batch Loss: 265204.5000\n",
            "Epoch 109, Batch 26/38, Batch Loss: 430881.2812\n",
            "Epoch 109, Batch 27/38, Batch Loss: 678585.8125\n",
            "Epoch 109, Batch 28/38, Batch Loss: 228587.9688\n",
            "Epoch 109, Batch 29/38, Batch Loss: 295522.5000\n",
            "Epoch 109, Batch 30/38, Batch Loss: 322170.4375\n",
            "Epoch 109, Batch 31/38, Batch Loss: 359695.3438\n",
            "Epoch 109, Batch 32/38, Batch Loss: 300489.2188\n",
            "Epoch 109, Batch 33/38, Batch Loss: 711092.0625\n",
            "Epoch 109, Batch 34/38, Batch Loss: 673018.7500\n",
            "Epoch 109, Batch 35/38, Batch Loss: 741537.4375\n",
            "Epoch 109, Batch 36/38, Batch Loss: 268349.2500\n",
            "Epoch 109, Batch 37/38, Batch Loss: 284107.8438\n",
            "Epoch 109, Batch 38/38, Batch Loss: 1017714.8750\n",
            "Batch 1/10, Batch Loss: 239273.1406\n",
            "Batch 2/10, Batch Loss: 354249.6875\n",
            "Batch 3/10, Batch Loss: 387231.8750\n",
            "Batch 4/10, Batch Loss: 421866.8438\n",
            "Batch 5/10, Batch Loss: 241991.7969\n",
            "Batch 6/10, Batch Loss: 256387.5938\n",
            "Batch 7/10, Batch Loss: 298930.4688\n",
            "Batch 8/10, Batch Loss: 303565.7188\n",
            "Batch 9/10, Batch Loss: 317304.7188\n",
            "Batch 10/10, Batch Loss: 186901.1562\n",
            "Test Error: \n",
            " Avg loss: 300770.300000, Avg error: 207652.823750 \n",
            "\n",
            "Epoch 110, Batch 1/38, Batch Loss: 698704.6875\n",
            "Epoch 110, Batch 2/38, Batch Loss: 267582.0938\n",
            "Epoch 110, Batch 3/38, Batch Loss: 309285.3750\n",
            "Epoch 110, Batch 4/38, Batch Loss: 410417.5000\n",
            "Epoch 110, Batch 5/38, Batch Loss: 291794.0000\n",
            "Epoch 110, Batch 6/38, Batch Loss: 269258.5000\n",
            "Epoch 110, Batch 7/38, Batch Loss: 273691.3438\n",
            "Epoch 110, Batch 8/38, Batch Loss: 360604.9062\n",
            "Epoch 110, Batch 9/38, Batch Loss: 323239.3125\n",
            "Epoch 110, Batch 10/38, Batch Loss: 375603.5625\n",
            "Epoch 110, Batch 11/38, Batch Loss: 408701.8125\n",
            "Epoch 110, Batch 12/38, Batch Loss: 270183.2500\n",
            "Epoch 110, Batch 13/38, Batch Loss: 955132.1875\n",
            "Epoch 110, Batch 14/38, Batch Loss: 346508.6875\n",
            "Epoch 110, Batch 15/38, Batch Loss: 297052.7500\n",
            "Epoch 110, Batch 16/38, Batch Loss: 716241.1250\n",
            "Epoch 110, Batch 17/38, Batch Loss: 255902.8750\n",
            "Epoch 110, Batch 18/38, Batch Loss: 241937.1250\n",
            "Epoch 110, Batch 19/38, Batch Loss: 270562.6562\n",
            "Epoch 110, Batch 20/38, Batch Loss: 249981.9375\n",
            "Epoch 110, Batch 21/38, Batch Loss: 408431.5625\n",
            "Epoch 110, Batch 22/38, Batch Loss: 276375.7812\n",
            "Epoch 110, Batch 23/38, Batch Loss: 297745.9375\n",
            "Epoch 110, Batch 24/38, Batch Loss: 669120.5625\n",
            "Epoch 110, Batch 25/38, Batch Loss: 250004.0469\n",
            "Epoch 110, Batch 26/38, Batch Loss: 688938.7500\n",
            "Epoch 110, Batch 27/38, Batch Loss: 285986.0625\n",
            "Epoch 110, Batch 28/38, Batch Loss: 228464.0312\n",
            "Epoch 110, Batch 29/38, Batch Loss: 290081.6875\n",
            "Epoch 110, Batch 30/38, Batch Loss: 309974.5000\n",
            "Epoch 110, Batch 31/38, Batch Loss: 269049.7188\n",
            "Epoch 110, Batch 32/38, Batch Loss: 289737.2500\n",
            "Epoch 110, Batch 33/38, Batch Loss: 726250.1875\n",
            "Epoch 110, Batch 34/38, Batch Loss: 283803.5000\n",
            "Epoch 110, Batch 35/38, Batch Loss: 249280.7500\n",
            "Epoch 110, Batch 36/38, Batch Loss: 300500.1562\n",
            "Epoch 110, Batch 37/38, Batch Loss: 363682.0312\n",
            "Epoch 110, Batch 38/38, Batch Loss: 185018.8281\n",
            "Batch 1/10, Batch Loss: 263255.1875\n",
            "Batch 2/10, Batch Loss: 263722.8750\n",
            "Batch 3/10, Batch Loss: 277101.0625\n",
            "Batch 4/10, Batch Loss: 275493.0312\n",
            "Batch 5/10, Batch Loss: 350116.9688\n",
            "Batch 6/10, Batch Loss: 284881.9688\n",
            "Batch 7/10, Batch Loss: 425272.8750\n",
            "Batch 8/10, Batch Loss: 350218.8750\n",
            "Batch 9/10, Batch Loss: 272010.9688\n",
            "Batch 10/10, Batch Loss: 421031.0312\n",
            "Test Error: \n",
            " Avg loss: 318310.484375, Avg error: 207127.100000 \n",
            "\n",
            "2024-09-14 20:08:43.115855 Epoch 110, Average Training loss 367495.55345394736\n",
            "Epoch 111, Batch 1/38, Batch Loss: 392838.5312\n",
            "Epoch 111, Batch 2/38, Batch Loss: 329204.5625\n",
            "Epoch 111, Batch 3/38, Batch Loss: 339449.8438\n",
            "Epoch 111, Batch 4/38, Batch Loss: 253432.7344\n",
            "Epoch 111, Batch 5/38, Batch Loss: 311075.5000\n",
            "Epoch 111, Batch 6/38, Batch Loss: 270607.7500\n",
            "Epoch 111, Batch 7/38, Batch Loss: 741459.6250\n",
            "Epoch 111, Batch 8/38, Batch Loss: 269512.3125\n",
            "Epoch 111, Batch 9/38, Batch Loss: 330874.6875\n",
            "Epoch 111, Batch 10/38, Batch Loss: 233707.6875\n",
            "Epoch 111, Batch 11/38, Batch Loss: 266809.4375\n",
            "Epoch 111, Batch 12/38, Batch Loss: 260676.1094\n",
            "Epoch 111, Batch 13/38, Batch Loss: 298789.6562\n",
            "Epoch 111, Batch 14/38, Batch Loss: 684363.2500\n",
            "Epoch 111, Batch 15/38, Batch Loss: 337053.5938\n",
            "Epoch 111, Batch 16/38, Batch Loss: 678392.8125\n",
            "Epoch 111, Batch 17/38, Batch Loss: 429929.5312\n",
            "Epoch 111, Batch 18/38, Batch Loss: 244634.3750\n",
            "Epoch 111, Batch 19/38, Batch Loss: 244719.7344\n",
            "Epoch 111, Batch 20/38, Batch Loss: 308451.3438\n",
            "Epoch 111, Batch 21/38, Batch Loss: 265718.8750\n",
            "Epoch 111, Batch 22/38, Batch Loss: 787950.6875\n",
            "Epoch 111, Batch 23/38, Batch Loss: 345494.9688\n",
            "Epoch 111, Batch 24/38, Batch Loss: 310566.3125\n",
            "Epoch 111, Batch 25/38, Batch Loss: 277789.3438\n",
            "Epoch 111, Batch 26/38, Batch Loss: 378960.9375\n",
            "Epoch 111, Batch 27/38, Batch Loss: 201713.9219\n",
            "Epoch 111, Batch 28/38, Batch Loss: 265974.8438\n",
            "Epoch 111, Batch 29/38, Batch Loss: 203146.8906\n",
            "Epoch 111, Batch 30/38, Batch Loss: 315968.2812\n",
            "Epoch 111, Batch 31/38, Batch Loss: 277263.6875\n",
            "Epoch 111, Batch 32/38, Batch Loss: 300286.4062\n",
            "Epoch 111, Batch 33/38, Batch Loss: 270538.5000\n",
            "Epoch 111, Batch 34/38, Batch Loss: 267552.4062\n",
            "Epoch 111, Batch 35/38, Batch Loss: 442712.3750\n",
            "Epoch 111, Batch 36/38, Batch Loss: 711163.7500\n",
            "Epoch 111, Batch 37/38, Batch Loss: 959381.9375\n",
            "Epoch 111, Batch 38/38, Batch Loss: 467128.1250\n",
            "Batch 1/10, Batch Loss: 322717.4062\n",
            "Batch 2/10, Batch Loss: 257498.8906\n",
            "Batch 3/10, Batch Loss: 279340.3750\n",
            "Batch 4/10, Batch Loss: 251268.7344\n",
            "Batch 5/10, Batch Loss: 402443.1562\n",
            "Batch 6/10, Batch Loss: 419963.5625\n",
            "Batch 7/10, Batch Loss: 308308.0000\n",
            "Batch 8/10, Batch Loss: 249968.2031\n",
            "Batch 9/10, Batch Loss: 228673.5156\n",
            "Batch 10/10, Batch Loss: 339761.7500\n",
            "Test Error: \n",
            " Avg loss: 305994.359375, Avg error: 214795.695000 \n",
            "\n",
            "Epoch 112, Batch 1/38, Batch Loss: 327446.9375\n",
            "Epoch 112, Batch 2/38, Batch Loss: 323167.2500\n",
            "Epoch 112, Batch 3/38, Batch Loss: 235229.9688\n",
            "Epoch 112, Batch 4/38, Batch Loss: 231832.8438\n",
            "Epoch 112, Batch 5/38, Batch Loss: 321365.0312\n",
            "Epoch 112, Batch 6/38, Batch Loss: 272098.8125\n",
            "Epoch 112, Batch 7/38, Batch Loss: 725442.3125\n",
            "Epoch 112, Batch 8/38, Batch Loss: 302046.6562\n",
            "Epoch 112, Batch 9/38, Batch Loss: 331959.6250\n",
            "Epoch 112, Batch 10/38, Batch Loss: 268030.6562\n",
            "Epoch 112, Batch 11/38, Batch Loss: 212767.4219\n",
            "Epoch 112, Batch 12/38, Batch Loss: 704661.2500\n",
            "Epoch 112, Batch 13/38, Batch Loss: 953792.8750\n",
            "Epoch 112, Batch 14/38, Batch Loss: 367254.9062\n",
            "Epoch 112, Batch 15/38, Batch Loss: 244855.7656\n",
            "Epoch 112, Batch 16/38, Batch Loss: 277558.1562\n",
            "Epoch 112, Batch 17/38, Batch Loss: 288891.7812\n",
            "Epoch 112, Batch 18/38, Batch Loss: 690275.4375\n",
            "Epoch 112, Batch 19/38, Batch Loss: 259425.7812\n",
            "Epoch 112, Batch 20/38, Batch Loss: 246939.8594\n",
            "Epoch 112, Batch 21/38, Batch Loss: 252053.2500\n",
            "Epoch 112, Batch 22/38, Batch Loss: 215568.6250\n",
            "Epoch 112, Batch 23/38, Batch Loss: 464623.6250\n",
            "Epoch 112, Batch 24/38, Batch Loss: 351814.5938\n",
            "Epoch 112, Batch 25/38, Batch Loss: 1019152.1250\n",
            "Epoch 112, Batch 26/38, Batch Loss: 388728.0625\n",
            "Epoch 112, Batch 27/38, Batch Loss: 252622.0312\n",
            "Epoch 112, Batch 28/38, Batch Loss: 226355.3906\n",
            "Epoch 112, Batch 29/38, Batch Loss: 402293.6562\n",
            "Epoch 112, Batch 30/38, Batch Loss: 383445.9375\n",
            "Epoch 112, Batch 31/38, Batch Loss: 258124.2812\n",
            "Epoch 112, Batch 32/38, Batch Loss: 247344.5469\n",
            "Epoch 112, Batch 33/38, Batch Loss: 268545.6562\n",
            "Epoch 112, Batch 34/38, Batch Loss: 390414.5000\n",
            "Epoch 112, Batch 35/38, Batch Loss: 290678.5312\n",
            "Epoch 112, Batch 36/38, Batch Loss: 269348.4688\n",
            "Epoch 112, Batch 37/38, Batch Loss: 260508.6250\n",
            "Epoch 112, Batch 38/38, Batch Loss: 334623.4375\n",
            "Batch 1/10, Batch Loss: 326168.4375\n",
            "Batch 2/10, Batch Loss: 347001.9062\n",
            "Batch 3/10, Batch Loss: 272444.3438\n",
            "Batch 4/10, Batch Loss: 400394.7188\n",
            "Batch 5/10, Batch Loss: 315319.8125\n",
            "Batch 6/10, Batch Loss: 318788.9688\n",
            "Batch 7/10, Batch Loss: 273435.3750\n",
            "Batch 8/10, Batch Loss: 216716.8438\n",
            "Batch 9/10, Batch Loss: 504272.7188\n",
            "Batch 10/10, Batch Loss: 234209.2500\n",
            "Test Error: \n",
            " Avg loss: 320875.237500, Avg error: 211023.618750 \n",
            "\n",
            "Epoch 113, Batch 1/38, Batch Loss: 784977.1875\n",
            "Epoch 113, Batch 2/38, Batch Loss: 274688.5625\n",
            "Epoch 113, Batch 3/38, Batch Loss: 315719.0312\n",
            "Epoch 113, Batch 4/38, Batch Loss: 282549.3125\n",
            "Epoch 113, Batch 5/38, Batch Loss: 271122.9375\n",
            "Epoch 113, Batch 6/38, Batch Loss: 294097.5000\n",
            "Epoch 113, Batch 7/38, Batch Loss: 907397.5000\n",
            "Epoch 113, Batch 8/38, Batch Loss: 322543.7812\n",
            "Epoch 113, Batch 9/38, Batch Loss: 223071.0469\n",
            "Epoch 113, Batch 10/38, Batch Loss: 706663.0625\n",
            "Epoch 113, Batch 11/38, Batch Loss: 284733.5938\n",
            "Epoch 113, Batch 12/38, Batch Loss: 398716.2500\n",
            "Epoch 113, Batch 13/38, Batch Loss: 302521.3438\n",
            "Epoch 113, Batch 14/38, Batch Loss: 219609.9062\n",
            "Epoch 113, Batch 15/38, Batch Loss: 234803.4531\n",
            "Epoch 113, Batch 16/38, Batch Loss: 248127.8281\n",
            "Epoch 113, Batch 17/38, Batch Loss: 228255.6562\n",
            "Epoch 113, Batch 18/38, Batch Loss: 708517.2500\n",
            "Epoch 113, Batch 19/38, Batch Loss: 324479.0312\n",
            "Epoch 113, Batch 20/38, Batch Loss: 218305.8906\n",
            "Epoch 113, Batch 21/38, Batch Loss: 754347.7500\n",
            "Epoch 113, Batch 22/38, Batch Loss: 279002.2812\n",
            "Epoch 113, Batch 23/38, Batch Loss: 213001.9375\n",
            "Epoch 113, Batch 24/38, Batch Loss: 316536.1875\n",
            "Epoch 113, Batch 25/38, Batch Loss: 250874.5156\n",
            "Epoch 113, Batch 26/38, Batch Loss: 287546.3750\n",
            "Epoch 113, Batch 27/38, Batch Loss: 273830.6250\n",
            "Epoch 113, Batch 28/38, Batch Loss: 440589.8125\n",
            "Epoch 113, Batch 29/38, Batch Loss: 382313.5625\n",
            "Epoch 113, Batch 30/38, Batch Loss: 289499.4375\n",
            "Epoch 113, Batch 31/38, Batch Loss: 313366.4062\n",
            "Epoch 113, Batch 32/38, Batch Loss: 253469.8906\n",
            "Epoch 113, Batch 33/38, Batch Loss: 261573.3594\n",
            "Epoch 113, Batch 34/38, Batch Loss: 344096.1562\n",
            "Epoch 113, Batch 35/38, Batch Loss: 751701.4375\n",
            "Epoch 113, Batch 36/38, Batch Loss: 272415.9375\n",
            "Epoch 113, Batch 37/38, Batch Loss: 346845.5938\n",
            "Epoch 113, Batch 38/38, Batch Loss: 179930.3438\n",
            "Batch 1/10, Batch Loss: 370887.5625\n",
            "Batch 2/10, Batch Loss: 340495.1875\n",
            "Batch 3/10, Batch Loss: 371674.2500\n",
            "Batch 4/10, Batch Loss: 394218.4688\n",
            "Batch 5/10, Batch Loss: 272848.5312\n",
            "Batch 6/10, Batch Loss: 235555.2344\n",
            "Batch 7/10, Batch Loss: 276427.7188\n",
            "Batch 8/10, Batch Loss: 325172.6562\n",
            "Batch 9/10, Batch Loss: 271368.2500\n",
            "Batch 10/10, Batch Loss: 351778.4062\n",
            "Test Error: \n",
            " Avg loss: 321042.626563, Avg error: 206500.672500 \n",
            "\n",
            "Epoch 114, Batch 1/38, Batch Loss: 315530.2812\n",
            "Epoch 114, Batch 2/38, Batch Loss: 261244.6250\n",
            "Epoch 114, Batch 3/38, Batch Loss: 328771.6562\n",
            "Epoch 114, Batch 4/38, Batch Loss: 942737.3750\n",
            "Epoch 114, Batch 5/38, Batch Loss: 279186.1875\n",
            "Epoch 114, Batch 6/38, Batch Loss: 261514.8281\n",
            "Epoch 114, Batch 7/38, Batch Loss: 280856.3750\n",
            "Epoch 114, Batch 8/38, Batch Loss: 308074.8125\n",
            "Epoch 114, Batch 9/38, Batch Loss: 710964.7500\n",
            "Epoch 114, Batch 10/38, Batch Loss: 246710.7812\n",
            "Epoch 114, Batch 11/38, Batch Loss: 965415.1875\n",
            "Epoch 114, Batch 12/38, Batch Loss: 249633.9688\n",
            "Epoch 114, Batch 13/38, Batch Loss: 278081.6875\n",
            "Epoch 114, Batch 14/38, Batch Loss: 270321.6562\n",
            "Epoch 114, Batch 15/38, Batch Loss: 356742.2188\n",
            "Epoch 114, Batch 16/38, Batch Loss: 271178.5625\n",
            "Epoch 114, Batch 17/38, Batch Loss: 244106.8906\n",
            "Epoch 114, Batch 18/38, Batch Loss: 312272.0938\n",
            "Epoch 114, Batch 19/38, Batch Loss: 263755.2188\n",
            "Epoch 114, Batch 20/38, Batch Loss: 317033.5938\n",
            "Epoch 114, Batch 21/38, Batch Loss: 287596.9688\n",
            "Epoch 114, Batch 22/38, Batch Loss: 327418.0938\n",
            "Epoch 114, Batch 23/38, Batch Loss: 265788.7500\n",
            "Epoch 114, Batch 24/38, Batch Loss: 439530.0625\n",
            "Epoch 114, Batch 25/38, Batch Loss: 244055.4219\n",
            "Epoch 114, Batch 26/38, Batch Loss: 293430.5938\n",
            "Epoch 114, Batch 27/38, Batch Loss: 312221.1562\n",
            "Epoch 114, Batch 28/38, Batch Loss: 717741.2500\n",
            "Epoch 114, Batch 29/38, Batch Loss: 318258.3125\n",
            "Epoch 114, Batch 30/38, Batch Loss: 287832.8125\n",
            "Epoch 114, Batch 31/38, Batch Loss: 269380.8125\n",
            "Epoch 114, Batch 32/38, Batch Loss: 315668.5312\n",
            "Epoch 114, Batch 33/38, Batch Loss: 299389.6875\n",
            "Epoch 114, Batch 34/38, Batch Loss: 677055.5000\n",
            "Epoch 114, Batch 35/38, Batch Loss: 386177.0938\n",
            "Epoch 114, Batch 36/38, Batch Loss: 275444.1562\n",
            "Epoch 114, Batch 37/38, Batch Loss: 289722.0000\n",
            "Epoch 114, Batch 38/38, Batch Loss: 328881.2188\n",
            "Batch 1/10, Batch Loss: 340817.7188\n",
            "Batch 2/10, Batch Loss: 309916.9062\n",
            "Batch 3/10, Batch Loss: 323107.8750\n",
            "Batch 4/10, Batch Loss: 367525.0625\n",
            "Batch 5/10, Batch Loss: 337382.6250\n",
            "Batch 6/10, Batch Loss: 392065.1562\n",
            "Batch 7/10, Batch Loss: 377707.5000\n",
            "Batch 8/10, Batch Loss: 253843.9375\n",
            "Batch 9/10, Batch Loss: 307306.5000\n",
            "Batch 10/10, Batch Loss: 218587.2812\n",
            "Test Error: \n",
            " Avg loss: 322826.056250, Avg error: 208264.079583 \n",
            "\n",
            "Epoch 115, Batch 1/38, Batch Loss: 228095.1406\n",
            "Epoch 115, Batch 2/38, Batch Loss: 214665.0312\n",
            "Epoch 115, Batch 3/38, Batch Loss: 679136.7500\n",
            "Epoch 115, Batch 4/38, Batch Loss: 410778.0000\n",
            "Epoch 115, Batch 5/38, Batch Loss: 278232.8750\n",
            "Epoch 115, Batch 6/38, Batch Loss: 367734.9062\n",
            "Epoch 115, Batch 7/38, Batch Loss: 272365.9688\n",
            "Epoch 115, Batch 8/38, Batch Loss: 225821.5781\n",
            "Epoch 115, Batch 9/38, Batch Loss: 265399.9375\n",
            "Epoch 115, Batch 10/38, Batch Loss: 248398.2969\n",
            "Epoch 115, Batch 11/38, Batch Loss: 687760.4375\n",
            "Epoch 115, Batch 12/38, Batch Loss: 365851.0000\n",
            "Epoch 115, Batch 13/38, Batch Loss: 471723.9688\n",
            "Epoch 115, Batch 14/38, Batch Loss: 296087.1250\n",
            "Epoch 115, Batch 15/38, Batch Loss: 339059.0625\n",
            "Epoch 115, Batch 16/38, Batch Loss: 240627.7656\n",
            "Epoch 115, Batch 17/38, Batch Loss: 333951.1250\n",
            "Epoch 115, Batch 18/38, Batch Loss: 296837.7500\n",
            "Epoch 115, Batch 19/38, Batch Loss: 406407.7188\n",
            "Epoch 115, Batch 20/38, Batch Loss: 279019.8750\n",
            "Epoch 115, Batch 21/38, Batch Loss: 716012.8750\n",
            "Epoch 115, Batch 22/38, Batch Loss: 377875.6562\n",
            "Epoch 115, Batch 23/38, Batch Loss: 266609.7500\n",
            "Epoch 115, Batch 24/38, Batch Loss: 260064.1250\n",
            "Epoch 115, Batch 25/38, Batch Loss: 347611.9375\n",
            "Epoch 115, Batch 26/38, Batch Loss: 302952.2188\n",
            "Epoch 115, Batch 27/38, Batch Loss: 351390.3125\n",
            "Epoch 115, Batch 28/38, Batch Loss: 214110.7031\n",
            "Epoch 115, Batch 29/38, Batch Loss: 275597.8438\n",
            "Epoch 115, Batch 30/38, Batch Loss: 733264.6250\n",
            "Epoch 115, Batch 31/38, Batch Loss: 240885.0469\n",
            "Epoch 115, Batch 32/38, Batch Loss: 290983.5312\n",
            "Epoch 115, Batch 33/38, Batch Loss: 921632.3750\n",
            "Epoch 115, Batch 34/38, Batch Loss: 268671.8438\n",
            "Epoch 115, Batch 35/38, Batch Loss: 712366.3750\n",
            "Epoch 115, Batch 36/38, Batch Loss: 263487.7500\n",
            "Epoch 115, Batch 37/38, Batch Loss: 291468.5000\n",
            "Epoch 115, Batch 38/38, Batch Loss: 242605.5312\n",
            "Batch 1/10, Batch Loss: 412900.0000\n",
            "Batch 2/10, Batch Loss: 225375.9219\n",
            "Batch 3/10, Batch Loss: 370430.2812\n",
            "Batch 4/10, Batch Loss: 253667.4219\n",
            "Batch 5/10, Batch Loss: 349235.6562\n",
            "Batch 6/10, Batch Loss: 307151.2500\n",
            "Batch 7/10, Batch Loss: 264551.2500\n",
            "Batch 8/10, Batch Loss: 267436.8438\n",
            "Batch 9/10, Batch Loss: 357063.3125\n",
            "Batch 10/10, Batch Loss: 172384.7031\n",
            "Test Error: \n",
            " Avg loss: 298019.664062, Avg error: 207628.630000 \n",
            "\n",
            "Epoch 116, Batch 1/38, Batch Loss: 274523.6250\n",
            "Epoch 116, Batch 2/38, Batch Loss: 240213.1719\n",
            "Epoch 116, Batch 3/38, Batch Loss: 277620.3438\n",
            "Epoch 116, Batch 4/38, Batch Loss: 247188.0625\n",
            "Epoch 116, Batch 5/38, Batch Loss: 688186.8125\n",
            "Epoch 116, Batch 6/38, Batch Loss: 686576.5000\n",
            "Epoch 116, Batch 7/38, Batch Loss: 294079.9062\n",
            "Epoch 116, Batch 8/38, Batch Loss: 266748.6250\n",
            "Epoch 116, Batch 9/38, Batch Loss: 692736.3125\n",
            "Epoch 116, Batch 10/38, Batch Loss: 316242.1250\n",
            "Epoch 116, Batch 11/38, Batch Loss: 277237.2500\n",
            "Epoch 116, Batch 12/38, Batch Loss: 310557.2188\n",
            "Epoch 116, Batch 13/38, Batch Loss: 335845.0000\n",
            "Epoch 116, Batch 14/38, Batch Loss: 287901.3438\n",
            "Epoch 116, Batch 15/38, Batch Loss: 318243.9062\n",
            "Epoch 116, Batch 16/38, Batch Loss: 675981.1250\n",
            "Epoch 116, Batch 17/38, Batch Loss: 233010.1250\n",
            "Epoch 116, Batch 18/38, Batch Loss: 732260.7500\n",
            "Epoch 116, Batch 19/38, Batch Loss: 221566.3125\n",
            "Epoch 116, Batch 20/38, Batch Loss: 380922.5625\n",
            "Epoch 116, Batch 21/38, Batch Loss: 303642.3750\n",
            "Epoch 116, Batch 22/38, Batch Loss: 222192.6250\n",
            "Epoch 116, Batch 23/38, Batch Loss: 773419.3125\n",
            "Epoch 116, Batch 24/38, Batch Loss: 725979.3750\n",
            "Epoch 116, Batch 25/38, Batch Loss: 276809.9062\n",
            "Epoch 116, Batch 26/38, Batch Loss: 336466.1875\n",
            "Epoch 116, Batch 27/38, Batch Loss: 287628.0938\n",
            "Epoch 116, Batch 28/38, Batch Loss: 287325.8438\n",
            "Epoch 116, Batch 29/38, Batch Loss: 308928.6250\n",
            "Epoch 116, Batch 30/38, Batch Loss: 335262.0938\n",
            "Epoch 116, Batch 31/38, Batch Loss: 339443.6875\n",
            "Epoch 116, Batch 32/38, Batch Loss: 364954.4062\n",
            "Epoch 116, Batch 33/38, Batch Loss: 431932.9062\n",
            "Epoch 116, Batch 34/38, Batch Loss: 406050.3750\n",
            "Epoch 116, Batch 35/38, Batch Loss: 349521.6250\n",
            "Epoch 116, Batch 36/38, Batch Loss: 241894.3750\n",
            "Epoch 116, Batch 37/38, Batch Loss: 330420.2188\n",
            "Epoch 116, Batch 38/38, Batch Loss: 269816.2500\n",
            "Batch 1/10, Batch Loss: 329185.8750\n",
            "Batch 2/10, Batch Loss: 330558.4375\n",
            "Batch 3/10, Batch Loss: 375158.1562\n",
            "Batch 4/10, Batch Loss: 287161.5938\n",
            "Batch 5/10, Batch Loss: 400975.8750\n",
            "Batch 6/10, Batch Loss: 264038.2188\n",
            "Batch 7/10, Batch Loss: 283586.7812\n",
            "Batch 8/10, Batch Loss: 315691.4062\n",
            "Batch 9/10, Batch Loss: 293836.1875\n",
            "Batch 10/10, Batch Loss: 240736.4219\n",
            "Test Error: \n",
            " Avg loss: 312092.895313, Avg error: 206882.605833 \n",
            "\n",
            "Epoch 117, Batch 1/38, Batch Loss: 234475.7031\n",
            "Epoch 117, Batch 2/38, Batch Loss: 330434.6250\n",
            "Epoch 117, Batch 3/38, Batch Loss: 385251.5938\n",
            "Epoch 117, Batch 4/38, Batch Loss: 676608.5625\n",
            "Epoch 117, Batch 5/38, Batch Loss: 347855.9688\n",
            "Epoch 117, Batch 6/38, Batch Loss: 295381.4375\n",
            "Epoch 117, Batch 7/38, Batch Loss: 258711.0469\n",
            "Epoch 117, Batch 8/38, Batch Loss: 381417.7500\n",
            "Epoch 117, Batch 9/38, Batch Loss: 305447.7500\n",
            "Epoch 117, Batch 10/38, Batch Loss: 332003.1250\n",
            "Epoch 117, Batch 11/38, Batch Loss: 261033.7656\n",
            "Epoch 117, Batch 12/38, Batch Loss: 388216.4062\n",
            "Epoch 117, Batch 13/38, Batch Loss: 681015.0625\n",
            "Epoch 117, Batch 14/38, Batch Loss: 384104.8125\n",
            "Epoch 117, Batch 15/38, Batch Loss: 347573.3438\n",
            "Epoch 117, Batch 16/38, Batch Loss: 294285.1562\n",
            "Epoch 117, Batch 17/38, Batch Loss: 697231.1250\n",
            "Epoch 117, Batch 18/38, Batch Loss: 331415.4688\n",
            "Epoch 117, Batch 19/38, Batch Loss: 280415.4062\n",
            "Epoch 117, Batch 20/38, Batch Loss: 686137.6250\n",
            "Epoch 117, Batch 21/38, Batch Loss: 323930.5625\n",
            "Epoch 117, Batch 22/38, Batch Loss: 236174.9531\n",
            "Epoch 117, Batch 23/38, Batch Loss: 358774.3750\n",
            "Epoch 117, Batch 24/38, Batch Loss: 193406.0312\n",
            "Epoch 117, Batch 25/38, Batch Loss: 268558.0000\n",
            "Epoch 117, Batch 26/38, Batch Loss: 324422.4688\n",
            "Epoch 117, Batch 27/38, Batch Loss: 734290.8750\n",
            "Epoch 117, Batch 28/38, Batch Loss: 284129.4375\n",
            "Epoch 117, Batch 29/38, Batch Loss: 317169.0938\n",
            "Epoch 117, Batch 30/38, Batch Loss: 224885.5781\n",
            "Epoch 117, Batch 31/38, Batch Loss: 262416.6250\n",
            "Epoch 117, Batch 32/38, Batch Loss: 687788.4375\n",
            "Epoch 117, Batch 33/38, Batch Loss: 277058.1250\n",
            "Epoch 117, Batch 34/38, Batch Loss: 689829.6250\n",
            "Epoch 117, Batch 35/38, Batch Loss: 295253.0938\n",
            "Epoch 117, Batch 36/38, Batch Loss: 284666.5312\n",
            "Epoch 117, Batch 37/38, Batch Loss: 371379.0625\n",
            "Epoch 117, Batch 38/38, Batch Loss: 256918.8125\n",
            "Batch 1/10, Batch Loss: 403651.5000\n",
            "Batch 2/10, Batch Loss: 276235.5312\n",
            "Batch 3/10, Batch Loss: 294381.7500\n",
            "Batch 4/10, Batch Loss: 354416.3438\n",
            "Batch 5/10, Batch Loss: 255528.0156\n",
            "Batch 6/10, Batch Loss: 217519.2812\n",
            "Batch 7/10, Batch Loss: 321558.3750\n",
            "Batch 8/10, Batch Loss: 318442.2500\n",
            "Batch 9/10, Batch Loss: 253361.6094\n",
            "Batch 10/10, Batch Loss: 393391.6250\n",
            "Test Error: \n",
            " Avg loss: 308848.628125, Avg error: 208202.224167 \n",
            "\n",
            "Epoch 118, Batch 1/38, Batch Loss: 705961.2500\n",
            "Epoch 118, Batch 2/38, Batch Loss: 328032.7500\n",
            "Epoch 118, Batch 3/38, Batch Loss: 369482.8750\n",
            "Epoch 118, Batch 4/38, Batch Loss: 333289.4688\n",
            "Epoch 118, Batch 5/38, Batch Loss: 259481.1094\n",
            "Epoch 118, Batch 6/38, Batch Loss: 226919.0156\n",
            "Epoch 118, Batch 7/38, Batch Loss: 313053.9688\n",
            "Epoch 118, Batch 8/38, Batch Loss: 253713.4844\n",
            "Epoch 118, Batch 9/38, Batch Loss: 368981.6875\n",
            "Epoch 118, Batch 10/38, Batch Loss: 288244.0000\n",
            "Epoch 118, Batch 11/38, Batch Loss: 285056.7500\n",
            "Epoch 118, Batch 12/38, Batch Loss: 236969.3438\n",
            "Epoch 118, Batch 13/38, Batch Loss: 334134.1875\n",
            "Epoch 118, Batch 14/38, Batch Loss: 317612.4688\n",
            "Epoch 118, Batch 15/38, Batch Loss: 680772.7500\n",
            "Epoch 118, Batch 16/38, Batch Loss: 293811.0312\n",
            "Epoch 118, Batch 17/38, Batch Loss: 257626.3750\n",
            "Epoch 118, Batch 18/38, Batch Loss: 217807.5938\n",
            "Epoch 118, Batch 19/38, Batch Loss: 385774.1250\n",
            "Epoch 118, Batch 20/38, Batch Loss: 716416.8125\n",
            "Epoch 118, Batch 21/38, Batch Loss: 285979.1562\n",
            "Epoch 118, Batch 22/38, Batch Loss: 300768.1875\n",
            "Epoch 118, Batch 23/38, Batch Loss: 704356.0625\n",
            "Epoch 118, Batch 24/38, Batch Loss: 261998.0000\n",
            "Epoch 118, Batch 25/38, Batch Loss: 326891.3438\n",
            "Epoch 118, Batch 26/38, Batch Loss: 355788.8750\n",
            "Epoch 118, Batch 27/38, Batch Loss: 365133.3750\n",
            "Epoch 118, Batch 28/38, Batch Loss: 288765.8438\n",
            "Epoch 118, Batch 29/38, Batch Loss: 215922.9844\n",
            "Epoch 118, Batch 30/38, Batch Loss: 707197.3125\n",
            "Epoch 118, Batch 31/38, Batch Loss: 312381.3438\n",
            "Epoch 118, Batch 32/38, Batch Loss: 294553.0938\n",
            "Epoch 118, Batch 33/38, Batch Loss: 301025.1875\n",
            "Epoch 118, Batch 34/38, Batch Loss: 276333.9375\n",
            "Epoch 118, Batch 35/38, Batch Loss: 748594.3125\n",
            "Epoch 118, Batch 36/38, Batch Loss: 436114.7812\n",
            "Epoch 118, Batch 37/38, Batch Loss: 274273.4062\n",
            "Epoch 118, Batch 38/38, Batch Loss: 1013760.0625\n",
            "Batch 1/10, Batch Loss: 458767.0625\n",
            "Batch 2/10, Batch Loss: 266243.5938\n",
            "Batch 3/10, Batch Loss: 311475.1250\n",
            "Batch 4/10, Batch Loss: 295188.2812\n",
            "Batch 5/10, Batch Loss: 260104.6250\n",
            "Batch 6/10, Batch Loss: 206994.1094\n",
            "Batch 7/10, Batch Loss: 230842.0312\n",
            "Batch 8/10, Batch Loss: 385073.1875\n",
            "Batch 9/10, Batch Loss: 441713.6875\n",
            "Batch 10/10, Batch Loss: 314536.2812\n",
            "Test Error: \n",
            " Avg loss: 317093.798438, Avg error: 207037.414167 \n",
            "\n",
            "Epoch 119, Batch 1/38, Batch Loss: 241192.7500\n",
            "Epoch 119, Batch 2/38, Batch Loss: 284613.3750\n",
            "Epoch 119, Batch 3/38, Batch Loss: 392736.2188\n",
            "Epoch 119, Batch 4/38, Batch Loss: 319247.2500\n",
            "Epoch 119, Batch 5/38, Batch Loss: 223419.8594\n",
            "Epoch 119, Batch 6/38, Batch Loss: 699686.2500\n",
            "Epoch 119, Batch 7/38, Batch Loss: 326723.8438\n",
            "Epoch 119, Batch 8/38, Batch Loss: 307771.6562\n",
            "Epoch 119, Batch 9/38, Batch Loss: 701916.7500\n",
            "Epoch 119, Batch 10/38, Batch Loss: 235973.9375\n",
            "Epoch 119, Batch 11/38, Batch Loss: 276244.5000\n",
            "Epoch 119, Batch 12/38, Batch Loss: 322840.3750\n",
            "Epoch 119, Batch 13/38, Batch Loss: 271416.9062\n",
            "Epoch 119, Batch 14/38, Batch Loss: 261688.0469\n",
            "Epoch 119, Batch 15/38, Batch Loss: 285530.3438\n",
            "Epoch 119, Batch 16/38, Batch Loss: 701506.6875\n",
            "Epoch 119, Batch 17/38, Batch Loss: 771616.6250\n",
            "Epoch 119, Batch 18/38, Batch Loss: 221537.2969\n",
            "Epoch 119, Batch 19/38, Batch Loss: 256514.0000\n",
            "Epoch 119, Batch 20/38, Batch Loss: 304829.9062\n",
            "Epoch 119, Batch 21/38, Batch Loss: 278411.4688\n",
            "Epoch 119, Batch 22/38, Batch Loss: 361939.0938\n",
            "Epoch 119, Batch 23/38, Batch Loss: 695107.6250\n",
            "Epoch 119, Batch 24/38, Batch Loss: 314988.5000\n",
            "Epoch 119, Batch 25/38, Batch Loss: 672214.5000\n",
            "Epoch 119, Batch 26/38, Batch Loss: 709480.1875\n",
            "Epoch 119, Batch 27/38, Batch Loss: 266055.4062\n",
            "Epoch 119, Batch 28/38, Batch Loss: 326525.8438\n",
            "Epoch 119, Batch 29/38, Batch Loss: 502221.7812\n",
            "Epoch 119, Batch 30/38, Batch Loss: 278391.1250\n",
            "Epoch 119, Batch 31/38, Batch Loss: 325594.3750\n",
            "Epoch 119, Batch 32/38, Batch Loss: 295830.8438\n",
            "Epoch 119, Batch 33/38, Batch Loss: 610462.6250\n",
            "Epoch 119, Batch 34/38, Batch Loss: 313424.5000\n",
            "Epoch 119, Batch 35/38, Batch Loss: 350154.0000\n",
            "Epoch 119, Batch 36/38, Batch Loss: 238945.4062\n",
            "Epoch 119, Batch 37/38, Batch Loss: 265598.0000\n",
            "Epoch 119, Batch 38/38, Batch Loss: 241390.8125\n",
            "Batch 1/10, Batch Loss: 317220.1562\n",
            "Batch 2/10, Batch Loss: 334857.7188\n",
            "Batch 3/10, Batch Loss: 283750.1562\n",
            "Batch 4/10, Batch Loss: 300720.6875\n",
            "Batch 5/10, Batch Loss: 318616.1250\n",
            "Batch 6/10, Batch Loss: 279744.8750\n",
            "Batch 7/10, Batch Loss: 289070.9375\n",
            "Batch 8/10, Batch Loss: 460658.7188\n",
            "Batch 9/10, Batch Loss: 502271.3438\n",
            "Batch 10/10, Batch Loss: 297548.2188\n",
            "Test Error: \n",
            " Avg loss: 338445.893750, Avg error: 217118.505833 \n",
            "\n",
            "Epoch 120, Batch 1/38, Batch Loss: 375310.9375\n",
            "Epoch 120, Batch 2/38, Batch Loss: 395131.0938\n",
            "Epoch 120, Batch 3/38, Batch Loss: 244391.2812\n",
            "Epoch 120, Batch 4/38, Batch Loss: 274432.7500\n",
            "Epoch 120, Batch 5/38, Batch Loss: 243721.8125\n",
            "Epoch 120, Batch 6/38, Batch Loss: 274000.9062\n",
            "Epoch 120, Batch 7/38, Batch Loss: 241107.2344\n",
            "Epoch 120, Batch 8/38, Batch Loss: 709537.0000\n",
            "Epoch 120, Batch 9/38, Batch Loss: 702245.5625\n",
            "Epoch 120, Batch 10/38, Batch Loss: 247350.5469\n",
            "Epoch 120, Batch 11/38, Batch Loss: 352493.9688\n",
            "Epoch 120, Batch 12/38, Batch Loss: 239854.8281\n",
            "Epoch 120, Batch 13/38, Batch Loss: 240666.6250\n",
            "Epoch 120, Batch 14/38, Batch Loss: 217980.8594\n",
            "Epoch 120, Batch 15/38, Batch Loss: 373439.0312\n",
            "Epoch 120, Batch 16/38, Batch Loss: 283583.2812\n",
            "Epoch 120, Batch 17/38, Batch Loss: 367754.6250\n",
            "Epoch 120, Batch 18/38, Batch Loss: 243011.8125\n",
            "Epoch 120, Batch 19/38, Batch Loss: 288150.1875\n",
            "Epoch 120, Batch 20/38, Batch Loss: 735729.9375\n",
            "Epoch 120, Batch 21/38, Batch Loss: 682264.6875\n",
            "Epoch 120, Batch 22/38, Batch Loss: 348338.4375\n",
            "Epoch 120, Batch 23/38, Batch Loss: 292556.3438\n",
            "Epoch 120, Batch 24/38, Batch Loss: 232764.8594\n",
            "Epoch 120, Batch 25/38, Batch Loss: 463861.7812\n",
            "Epoch 120, Batch 26/38, Batch Loss: 296617.5938\n",
            "Epoch 120, Batch 27/38, Batch Loss: 699479.3750\n",
            "Epoch 120, Batch 28/38, Batch Loss: 285958.1562\n",
            "Epoch 120, Batch 29/38, Batch Loss: 351221.0938\n",
            "Epoch 120, Batch 30/38, Batch Loss: 248125.3438\n",
            "Epoch 120, Batch 31/38, Batch Loss: 696724.9375\n",
            "Epoch 120, Batch 32/38, Batch Loss: 343132.3125\n",
            "Epoch 120, Batch 33/38, Batch Loss: 202947.5000\n",
            "Epoch 120, Batch 34/38, Batch Loss: 441637.6250\n",
            "Epoch 120, Batch 35/38, Batch Loss: 243387.3281\n",
            "Epoch 120, Batch 36/38, Batch Loss: 245663.9531\n",
            "Epoch 120, Batch 37/38, Batch Loss: 713932.0625\n",
            "Epoch 120, Batch 38/38, Batch Loss: 221241.6094\n",
            "Batch 1/10, Batch Loss: 356852.4688\n",
            "Batch 2/10, Batch Loss: 270796.4062\n",
            "Batch 3/10, Batch Loss: 319431.0312\n",
            "Batch 4/10, Batch Loss: 237061.5781\n",
            "Batch 5/10, Batch Loss: 290796.1250\n",
            "Batch 6/10, Batch Loss: 377884.5312\n",
            "Batch 7/10, Batch Loss: 260794.5469\n",
            "Batch 8/10, Batch Loss: 367272.8125\n",
            "Batch 9/10, Batch Loss: 515697.4375\n",
            "Batch 10/10, Batch Loss: 259112.0312\n",
            "Test Error: \n",
            " Avg loss: 325569.896875, Avg error: 212374.330833 \n",
            "\n",
            "2024-09-14 20:15:27.265876 Epoch 120, Average Training loss 369993.4021381579\n",
            "Epoch 121, Batch 1/38, Batch Loss: 229853.0938\n",
            "Epoch 121, Batch 2/38, Batch Loss: 484843.5312\n",
            "Epoch 121, Batch 3/38, Batch Loss: 321339.3125\n",
            "Epoch 121, Batch 4/38, Batch Loss: 728219.1250\n",
            "Epoch 121, Batch 5/38, Batch Loss: 353722.5000\n",
            "Epoch 121, Batch 6/38, Batch Loss: 266533.2500\n",
            "Epoch 121, Batch 7/38, Batch Loss: 264102.4375\n",
            "Epoch 121, Batch 8/38, Batch Loss: 933988.5000\n",
            "Epoch 121, Batch 9/38, Batch Loss: 324054.9688\n",
            "Epoch 121, Batch 10/38, Batch Loss: 279685.5312\n",
            "Epoch 121, Batch 11/38, Batch Loss: 366394.9688\n",
            "Epoch 121, Batch 12/38, Batch Loss: 299713.9062\n",
            "Epoch 121, Batch 13/38, Batch Loss: 355882.1562\n",
            "Epoch 121, Batch 14/38, Batch Loss: 272548.5625\n",
            "Epoch 121, Batch 15/38, Batch Loss: 316547.7500\n",
            "Epoch 121, Batch 16/38, Batch Loss: 257051.5156\n",
            "Epoch 121, Batch 17/38, Batch Loss: 266882.5312\n",
            "Epoch 121, Batch 18/38, Batch Loss: 250869.9844\n",
            "Epoch 121, Batch 19/38, Batch Loss: 325656.8125\n",
            "Epoch 121, Batch 20/38, Batch Loss: 255517.9062\n",
            "Epoch 121, Batch 21/38, Batch Loss: 364014.4375\n",
            "Epoch 121, Batch 22/38, Batch Loss: 285770.0312\n",
            "Epoch 121, Batch 23/38, Batch Loss: 715606.0000\n",
            "Epoch 121, Batch 24/38, Batch Loss: 362996.5938\n",
            "Epoch 121, Batch 25/38, Batch Loss: 253228.4219\n",
            "Epoch 121, Batch 26/38, Batch Loss: 245992.5625\n",
            "Epoch 121, Batch 27/38, Batch Loss: 398618.1562\n",
            "Epoch 121, Batch 28/38, Batch Loss: 321601.8438\n",
            "Epoch 121, Batch 29/38, Batch Loss: 277962.2812\n",
            "Epoch 121, Batch 30/38, Batch Loss: 300817.2500\n",
            "Epoch 121, Batch 31/38, Batch Loss: 697151.5625\n",
            "Epoch 121, Batch 32/38, Batch Loss: 691598.8750\n",
            "Epoch 121, Batch 33/38, Batch Loss: 340468.4688\n",
            "Epoch 121, Batch 34/38, Batch Loss: 685795.2500\n",
            "Epoch 121, Batch 35/38, Batch Loss: 258174.6094\n",
            "Epoch 121, Batch 36/38, Batch Loss: 286680.6562\n",
            "Epoch 121, Batch 37/38, Batch Loss: 263517.2500\n",
            "Epoch 121, Batch 38/38, Batch Loss: 318612.5000\n",
            "Batch 1/10, Batch Loss: 331859.7812\n",
            "Batch 2/10, Batch Loss: 295555.8125\n",
            "Batch 3/10, Batch Loss: 401022.5312\n",
            "Batch 4/10, Batch Loss: 452385.4375\n",
            "Batch 5/10, Batch Loss: 243678.3438\n",
            "Batch 6/10, Batch Loss: 237848.9219\n",
            "Batch 7/10, Batch Loss: 346797.4688\n",
            "Batch 8/10, Batch Loss: 355756.3438\n",
            "Batch 9/10, Batch Loss: 296900.9062\n",
            "Batch 10/10, Batch Loss: 259500.2344\n",
            "Test Error: \n",
            " Avg loss: 322130.578125, Avg error: 210682.908333 \n",
            "\n",
            "Epoch 122, Batch 1/38, Batch Loss: 278614.6875\n",
            "Epoch 122, Batch 2/38, Batch Loss: 368794.0938\n",
            "Epoch 122, Batch 3/38, Batch Loss: 684285.5625\n",
            "Epoch 122, Batch 4/38, Batch Loss: 259715.7656\n",
            "Epoch 122, Batch 5/38, Batch Loss: 259724.5469\n",
            "Epoch 122, Batch 6/38, Batch Loss: 284554.3438\n",
            "Epoch 122, Batch 7/38, Batch Loss: 705248.6250\n",
            "Epoch 122, Batch 8/38, Batch Loss: 341106.2188\n",
            "Epoch 122, Batch 9/38, Batch Loss: 242928.5469\n",
            "Epoch 122, Batch 10/38, Batch Loss: 256754.7188\n",
            "Epoch 122, Batch 11/38, Batch Loss: 259845.3750\n",
            "Epoch 122, Batch 12/38, Batch Loss: 280965.2500\n",
            "Epoch 122, Batch 13/38, Batch Loss: 311299.5938\n",
            "Epoch 122, Batch 14/38, Batch Loss: 311515.7500\n",
            "Epoch 122, Batch 15/38, Batch Loss: 384963.9688\n",
            "Epoch 122, Batch 16/38, Batch Loss: 275715.8438\n",
            "Epoch 122, Batch 17/38, Batch Loss: 306083.1875\n",
            "Epoch 122, Batch 18/38, Batch Loss: 695758.1250\n",
            "Epoch 122, Batch 19/38, Batch Loss: 306854.5938\n",
            "Epoch 122, Batch 20/38, Batch Loss: 286705.0938\n",
            "Epoch 122, Batch 21/38, Batch Loss: 287079.3125\n",
            "Epoch 122, Batch 22/38, Batch Loss: 262405.0000\n",
            "Epoch 122, Batch 23/38, Batch Loss: 261293.7969\n",
            "Epoch 122, Batch 24/38, Batch Loss: 322075.0938\n",
            "Epoch 122, Batch 25/38, Batch Loss: 705804.2500\n",
            "Epoch 122, Batch 26/38, Batch Loss: 426506.1875\n",
            "Epoch 122, Batch 27/38, Batch Loss: 353275.2188\n",
            "Epoch 122, Batch 28/38, Batch Loss: 412575.8750\n",
            "Epoch 122, Batch 29/38, Batch Loss: 299995.8125\n",
            "Epoch 122, Batch 30/38, Batch Loss: 356975.3750\n",
            "Epoch 122, Batch 31/38, Batch Loss: 822405.9375\n",
            "Epoch 122, Batch 32/38, Batch Loss: 334450.5625\n",
            "Epoch 122, Batch 33/38, Batch Loss: 304266.7500\n",
            "Epoch 122, Batch 34/38, Batch Loss: 366011.8125\n",
            "Epoch 122, Batch 35/38, Batch Loss: 271475.9062\n",
            "Epoch 122, Batch 36/38, Batch Loss: 284301.8750\n",
            "Epoch 122, Batch 37/38, Batch Loss: 917785.7500\n",
            "Epoch 122, Batch 38/38, Batch Loss: 293049.0938\n",
            "Batch 1/10, Batch Loss: 232187.1250\n",
            "Batch 2/10, Batch Loss: 259048.4531\n",
            "Batch 3/10, Batch Loss: 331558.0938\n",
            "Batch 4/10, Batch Loss: 288572.4688\n",
            "Batch 5/10, Batch Loss: 458457.6562\n",
            "Batch 6/10, Batch Loss: 211114.3750\n",
            "Batch 7/10, Batch Loss: 328686.9375\n",
            "Batch 8/10, Batch Loss: 383794.0000\n",
            "Batch 9/10, Batch Loss: 303893.4062\n",
            "Batch 10/10, Batch Loss: 277326.3125\n",
            "Test Error: \n",
            " Avg loss: 307463.882812, Avg error: 204135.836667 \n",
            "\n",
            "Epoch 123, Batch 1/38, Batch Loss: 729283.9375\n",
            "Epoch 123, Batch 2/38, Batch Loss: 282782.6875\n",
            "Epoch 123, Batch 3/38, Batch Loss: 266362.0938\n",
            "Epoch 123, Batch 4/38, Batch Loss: 697022.1875\n",
            "Epoch 123, Batch 5/38, Batch Loss: 287616.3125\n",
            "Epoch 123, Batch 6/38, Batch Loss: 297542.5312\n",
            "Epoch 123, Batch 7/38, Batch Loss: 306789.2188\n",
            "Epoch 123, Batch 8/38, Batch Loss: 708111.8750\n",
            "Epoch 123, Batch 9/38, Batch Loss: 261392.0938\n",
            "Epoch 123, Batch 10/38, Batch Loss: 765412.2500\n",
            "Epoch 123, Batch 11/38, Batch Loss: 277521.2812\n",
            "Epoch 123, Batch 12/38, Batch Loss: 237192.7344\n",
            "Epoch 123, Batch 13/38, Batch Loss: 378020.3750\n",
            "Epoch 123, Batch 14/38, Batch Loss: 229624.3750\n",
            "Epoch 123, Batch 15/38, Batch Loss: 344875.6250\n",
            "Epoch 123, Batch 16/38, Batch Loss: 687896.0625\n",
            "Epoch 123, Batch 17/38, Batch Loss: 411563.3438\n",
            "Epoch 123, Batch 18/38, Batch Loss: 315563.9375\n",
            "Epoch 123, Batch 19/38, Batch Loss: 680437.6250\n",
            "Epoch 123, Batch 20/38, Batch Loss: 413881.9375\n",
            "Epoch 123, Batch 21/38, Batch Loss: 275003.9062\n",
            "Epoch 123, Batch 22/38, Batch Loss: 281362.7188\n",
            "Epoch 123, Batch 23/38, Batch Loss: 306427.0625\n",
            "Epoch 123, Batch 24/38, Batch Loss: 308673.2500\n",
            "Epoch 123, Batch 25/38, Batch Loss: 310420.7500\n",
            "Epoch 123, Batch 26/38, Batch Loss: 233706.6719\n",
            "Epoch 123, Batch 27/38, Batch Loss: 697532.8750\n",
            "Epoch 123, Batch 28/38, Batch Loss: 270587.7500\n",
            "Epoch 123, Batch 29/38, Batch Loss: 352557.8125\n",
            "Epoch 123, Batch 30/38, Batch Loss: 317036.7188\n",
            "Epoch 123, Batch 31/38, Batch Loss: 280599.1250\n",
            "Epoch 123, Batch 32/38, Batch Loss: 311238.1250\n",
            "Epoch 123, Batch 33/38, Batch Loss: 268551.4375\n",
            "Epoch 123, Batch 34/38, Batch Loss: 222273.6719\n",
            "Epoch 123, Batch 35/38, Batch Loss: 374935.9688\n",
            "Epoch 123, Batch 36/38, Batch Loss: 248611.3750\n",
            "Epoch 123, Batch 37/38, Batch Loss: 271126.4062\n",
            "Epoch 123, Batch 38/38, Batch Loss: 220245.9688\n",
            "Batch 1/10, Batch Loss: 300070.4688\n",
            "Batch 2/10, Batch Loss: 352152.5312\n",
            "Batch 3/10, Batch Loss: 262699.5000\n",
            "Batch 4/10, Batch Loss: 280681.3125\n",
            "Batch 5/10, Batch Loss: 287932.5938\n",
            "Batch 6/10, Batch Loss: 327620.2188\n",
            "Batch 7/10, Batch Loss: 298413.7812\n",
            "Batch 8/10, Batch Loss: 370685.2812\n",
            "Batch 9/10, Batch Loss: 325968.8125\n",
            "Batch 10/10, Batch Loss: 234261.4688\n",
            "Test Error: \n",
            " Avg loss: 304048.596875, Avg error: 211560.680833 \n",
            "\n",
            "Epoch 124, Batch 1/38, Batch Loss: 708059.4375\n",
            "Epoch 124, Batch 2/38, Batch Loss: 284789.0000\n",
            "Epoch 124, Batch 3/38, Batch Loss: 927163.9375\n",
            "Epoch 124, Batch 4/38, Batch Loss: 377180.5312\n",
            "Epoch 124, Batch 5/38, Batch Loss: 327857.4688\n",
            "Epoch 124, Batch 6/38, Batch Loss: 315785.8438\n",
            "Epoch 124, Batch 7/38, Batch Loss: 270458.5000\n",
            "Epoch 124, Batch 8/38, Batch Loss: 308388.9375\n",
            "Epoch 124, Batch 9/38, Batch Loss: 273237.0625\n",
            "Epoch 124, Batch 10/38, Batch Loss: 289982.8125\n",
            "Epoch 124, Batch 11/38, Batch Loss: 390538.8438\n",
            "Epoch 124, Batch 12/38, Batch Loss: 266979.7500\n",
            "Epoch 124, Batch 13/38, Batch Loss: 262457.8750\n",
            "Epoch 124, Batch 14/38, Batch Loss: 674367.5625\n",
            "Epoch 124, Batch 15/38, Batch Loss: 328469.7500\n",
            "Epoch 124, Batch 16/38, Batch Loss: 279672.3438\n",
            "Epoch 124, Batch 17/38, Batch Loss: 329633.1562\n",
            "Epoch 124, Batch 18/38, Batch Loss: 313099.4688\n",
            "Epoch 124, Batch 19/38, Batch Loss: 349097.9375\n",
            "Epoch 124, Batch 20/38, Batch Loss: 280980.1250\n",
            "Epoch 124, Batch 21/38, Batch Loss: 286916.7500\n",
            "Epoch 124, Batch 22/38, Batch Loss: 301669.9375\n",
            "Epoch 124, Batch 23/38, Batch Loss: 279704.1875\n",
            "Epoch 124, Batch 24/38, Batch Loss: 331717.2500\n",
            "Epoch 124, Batch 25/38, Batch Loss: 264922.8750\n",
            "Epoch 124, Batch 26/38, Batch Loss: 211441.9688\n",
            "Epoch 124, Batch 27/38, Batch Loss: 322683.5625\n",
            "Epoch 124, Batch 28/38, Batch Loss: 348852.3125\n",
            "Epoch 124, Batch 29/38, Batch Loss: 675654.6250\n",
            "Epoch 124, Batch 30/38, Batch Loss: 338633.4688\n",
            "Epoch 124, Batch 31/38, Batch Loss: 274405.3125\n",
            "Epoch 124, Batch 32/38, Batch Loss: 345328.1875\n",
            "Epoch 124, Batch 33/38, Batch Loss: 257288.0312\n",
            "Epoch 124, Batch 34/38, Batch Loss: 318605.4375\n",
            "Epoch 124, Batch 35/38, Batch Loss: 692757.1250\n",
            "Epoch 124, Batch 36/38, Batch Loss: 326025.2500\n",
            "Epoch 124, Batch 37/38, Batch Loss: 665331.5000\n",
            "Epoch 124, Batch 38/38, Batch Loss: 360732.9062\n",
            "Batch 1/10, Batch Loss: 238508.5938\n",
            "Batch 2/10, Batch Loss: 266680.9375\n",
            "Batch 3/10, Batch Loss: 308283.8750\n",
            "Batch 4/10, Batch Loss: 238827.2188\n",
            "Batch 5/10, Batch Loss: 265552.5938\n",
            "Batch 6/10, Batch Loss: 447389.7812\n",
            "Batch 7/10, Batch Loss: 367524.7812\n",
            "Batch 8/10, Batch Loss: 284889.9375\n",
            "Batch 9/10, Batch Loss: 252692.9375\n",
            "Batch 10/10, Batch Loss: 444120.5000\n",
            "Test Error: \n",
            " Avg loss: 311447.115625, Avg error: 208131.294167 \n",
            "\n",
            "Epoch 125, Batch 1/38, Batch Loss: 300210.7500\n",
            "Epoch 125, Batch 2/38, Batch Loss: 257997.5312\n",
            "Epoch 125, Batch 3/38, Batch Loss: 280865.2500\n",
            "Epoch 125, Batch 4/38, Batch Loss: 355238.7500\n",
            "Epoch 125, Batch 5/38, Batch Loss: 247473.8594\n",
            "Epoch 125, Batch 6/38, Batch Loss: 286986.7188\n",
            "Epoch 125, Batch 7/38, Batch Loss: 220830.2031\n",
            "Epoch 125, Batch 8/38, Batch Loss: 225030.4688\n",
            "Epoch 125, Batch 9/38, Batch Loss: 254209.9375\n",
            "Epoch 125, Batch 10/38, Batch Loss: 370723.0312\n",
            "Epoch 125, Batch 11/38, Batch Loss: 338076.7812\n",
            "Epoch 125, Batch 12/38, Batch Loss: 692251.6250\n",
            "Epoch 125, Batch 13/38, Batch Loss: 356787.0938\n",
            "Epoch 125, Batch 14/38, Batch Loss: 399873.2500\n",
            "Epoch 125, Batch 15/38, Batch Loss: 675822.3750\n",
            "Epoch 125, Batch 16/38, Batch Loss: 364873.6875\n",
            "Epoch 125, Batch 17/38, Batch Loss: 686745.5000\n",
            "Epoch 125, Batch 18/38, Batch Loss: 340360.0000\n",
            "Epoch 125, Batch 19/38, Batch Loss: 937673.7500\n",
            "Epoch 125, Batch 20/38, Batch Loss: 347057.9062\n",
            "Epoch 125, Batch 21/38, Batch Loss: 283006.6562\n",
            "Epoch 125, Batch 22/38, Batch Loss: 301330.5312\n",
            "Epoch 125, Batch 23/38, Batch Loss: 349076.8750\n",
            "Epoch 125, Batch 24/38, Batch Loss: 278269.8125\n",
            "Epoch 125, Batch 25/38, Batch Loss: 321876.5938\n",
            "Epoch 125, Batch 26/38, Batch Loss: 428058.7812\n",
            "Epoch 125, Batch 27/38, Batch Loss: 280204.7500\n",
            "Epoch 125, Batch 28/38, Batch Loss: 221510.6875\n",
            "Epoch 125, Batch 29/38, Batch Loss: 694080.5625\n",
            "Epoch 125, Batch 30/38, Batch Loss: 281575.5000\n",
            "Epoch 125, Batch 31/38, Batch Loss: 208331.5781\n",
            "Epoch 125, Batch 32/38, Batch Loss: 585652.5625\n",
            "Epoch 125, Batch 33/38, Batch Loss: 364945.1875\n",
            "Epoch 125, Batch 34/38, Batch Loss: 305106.1250\n",
            "Epoch 125, Batch 35/38, Batch Loss: 358492.4062\n",
            "Epoch 125, Batch 36/38, Batch Loss: 260470.2500\n",
            "Epoch 125, Batch 37/38, Batch Loss: 705413.9375\n",
            "Epoch 125, Batch 38/38, Batch Loss: 208923.6094\n",
            "Batch 1/10, Batch Loss: 214781.0469\n",
            "Batch 2/10, Batch Loss: 389343.3125\n",
            "Batch 3/10, Batch Loss: 340642.8750\n",
            "Batch 4/10, Batch Loss: 282605.8750\n",
            "Batch 5/10, Batch Loss: 276950.3125\n",
            "Batch 6/10, Batch Loss: 319868.7500\n",
            "Batch 7/10, Batch Loss: 381489.9688\n",
            "Batch 8/10, Batch Loss: 407240.4688\n",
            "Batch 9/10, Batch Loss: 274075.3438\n",
            "Batch 10/10, Batch Loss: 401496.4062\n",
            "Test Error: \n",
            " Avg loss: 328849.435937, Avg error: 209290.739167 \n",
            "\n",
            "Epoch 126, Batch 1/38, Batch Loss: 234089.6094\n",
            "Epoch 126, Batch 2/38, Batch Loss: 270271.2812\n",
            "Epoch 126, Batch 3/38, Batch Loss: 717245.6875\n",
            "Epoch 126, Batch 4/38, Batch Loss: 697899.3125\n",
            "Epoch 126, Batch 5/38, Batch Loss: 286379.2188\n",
            "Epoch 126, Batch 6/38, Batch Loss: 268509.6875\n",
            "Epoch 126, Batch 7/38, Batch Loss: 304121.5938\n",
            "Epoch 126, Batch 8/38, Batch Loss: 265512.3750\n",
            "Epoch 126, Batch 9/38, Batch Loss: 355208.5625\n",
            "Epoch 126, Batch 10/38, Batch Loss: 252048.8594\n",
            "Epoch 126, Batch 11/38, Batch Loss: 302033.2812\n",
            "Epoch 126, Batch 12/38, Batch Loss: 263256.9375\n",
            "Epoch 126, Batch 13/38, Batch Loss: 281830.0938\n",
            "Epoch 126, Batch 14/38, Batch Loss: 284254.4688\n",
            "Epoch 126, Batch 15/38, Batch Loss: 323656.1250\n",
            "Epoch 126, Batch 16/38, Batch Loss: 687302.7500\n",
            "Epoch 126, Batch 17/38, Batch Loss: 303427.5625\n",
            "Epoch 126, Batch 18/38, Batch Loss: 339430.2188\n",
            "Epoch 126, Batch 19/38, Batch Loss: 463075.3750\n",
            "Epoch 126, Batch 20/38, Batch Loss: 281581.7500\n",
            "Epoch 126, Batch 21/38, Batch Loss: 673619.5000\n",
            "Epoch 126, Batch 22/38, Batch Loss: 449276.2188\n",
            "Epoch 126, Batch 23/38, Batch Loss: 341745.3750\n",
            "Epoch 126, Batch 24/38, Batch Loss: 355776.9375\n",
            "Epoch 126, Batch 25/38, Batch Loss: 277996.8438\n",
            "Epoch 126, Batch 26/38, Batch Loss: 259342.1094\n",
            "Epoch 126, Batch 27/38, Batch Loss: 325030.8438\n",
            "Epoch 126, Batch 28/38, Batch Loss: 300486.4062\n",
            "Epoch 126, Batch 29/38, Batch Loss: 262131.0469\n",
            "Epoch 126, Batch 30/38, Batch Loss: 413104.7188\n",
            "Epoch 126, Batch 31/38, Batch Loss: 346482.9375\n",
            "Epoch 126, Batch 32/38, Batch Loss: 277642.1250\n",
            "Epoch 126, Batch 33/38, Batch Loss: 704753.5625\n",
            "Epoch 126, Batch 34/38, Batch Loss: 712481.5625\n",
            "Epoch 126, Batch 35/38, Batch Loss: 211894.8125\n",
            "Epoch 126, Batch 36/38, Batch Loss: 242182.6250\n",
            "Epoch 126, Batch 37/38, Batch Loss: 339459.2812\n",
            "Epoch 126, Batch 38/38, Batch Loss: 978770.8125\n",
            "Batch 1/10, Batch Loss: 308086.2812\n",
            "Batch 2/10, Batch Loss: 260913.4688\n",
            "Batch 3/10, Batch Loss: 391704.6562\n",
            "Batch 4/10, Batch Loss: 404449.3438\n",
            "Batch 5/10, Batch Loss: 269067.0312\n",
            "Batch 6/10, Batch Loss: 339888.9375\n",
            "Batch 7/10, Batch Loss: 311808.9375\n",
            "Batch 8/10, Batch Loss: 272016.7500\n",
            "Batch 9/10, Batch Loss: 323251.7812\n",
            "Batch 10/10, Batch Loss: 319448.4375\n",
            "Test Error: \n",
            " Avg loss: 320063.562500, Avg error: 206848.652083 \n",
            "\n",
            "Epoch 127, Batch 1/38, Batch Loss: 291360.2188\n",
            "Epoch 127, Batch 2/38, Batch Loss: 246372.1094\n",
            "Epoch 127, Batch 3/38, Batch Loss: 784576.6250\n",
            "Epoch 127, Batch 4/38, Batch Loss: 196965.8125\n",
            "Epoch 127, Batch 5/38, Batch Loss: 245374.3281\n",
            "Epoch 127, Batch 6/38, Batch Loss: 326333.5312\n",
            "Epoch 127, Batch 7/38, Batch Loss: 655909.5625\n",
            "Epoch 127, Batch 8/38, Batch Loss: 335525.8438\n",
            "Epoch 127, Batch 9/38, Batch Loss: 239127.1719\n",
            "Epoch 127, Batch 10/38, Batch Loss: 253495.9219\n",
            "Epoch 127, Batch 11/38, Batch Loss: 265679.8750\n",
            "Epoch 127, Batch 12/38, Batch Loss: 257284.7188\n",
            "Epoch 127, Batch 13/38, Batch Loss: 357993.0312\n",
            "Epoch 127, Batch 14/38, Batch Loss: 285941.2188\n",
            "Epoch 127, Batch 15/38, Batch Loss: 260447.6250\n",
            "Epoch 127, Batch 16/38, Batch Loss: 316398.0312\n",
            "Epoch 127, Batch 17/38, Batch Loss: 373581.6250\n",
            "Epoch 127, Batch 18/38, Batch Loss: 303461.7812\n",
            "Epoch 127, Batch 19/38, Batch Loss: 278472.2500\n",
            "Epoch 127, Batch 20/38, Batch Loss: 314894.0312\n",
            "Epoch 127, Batch 21/38, Batch Loss: 307214.2188\n",
            "Epoch 127, Batch 22/38, Batch Loss: 301631.2188\n",
            "Epoch 127, Batch 23/38, Batch Loss: 294855.3750\n",
            "Epoch 127, Batch 24/38, Batch Loss: 248335.0938\n",
            "Epoch 127, Batch 25/38, Batch Loss: 717325.2500\n",
            "Epoch 127, Batch 26/38, Batch Loss: 666248.5000\n",
            "Epoch 127, Batch 27/38, Batch Loss: 215262.3594\n",
            "Epoch 127, Batch 28/38, Batch Loss: 306746.0312\n",
            "Epoch 127, Batch 29/38, Batch Loss: 660820.3125\n",
            "Epoch 127, Batch 30/38, Batch Loss: 273780.1250\n",
            "Epoch 127, Batch 31/38, Batch Loss: 257440.1875\n",
            "Epoch 127, Batch 32/38, Batch Loss: 346413.3125\n",
            "Epoch 127, Batch 33/38, Batch Loss: 298974.5625\n",
            "Epoch 127, Batch 34/38, Batch Loss: 231764.1406\n",
            "Epoch 127, Batch 35/38, Batch Loss: 753569.1250\n",
            "Epoch 127, Batch 36/38, Batch Loss: 276163.8125\n",
            "Epoch 127, Batch 37/38, Batch Loss: 755509.3750\n",
            "Epoch 127, Batch 38/38, Batch Loss: 414942.4062\n",
            "Batch 1/10, Batch Loss: 206499.6719\n",
            "Batch 2/10, Batch Loss: 359133.4062\n",
            "Batch 3/10, Batch Loss: 250999.0469\n",
            "Batch 4/10, Batch Loss: 367229.0000\n",
            "Batch 5/10, Batch Loss: 413690.9688\n",
            "Batch 6/10, Batch Loss: 333622.0938\n",
            "Batch 7/10, Batch Loss: 309431.6562\n",
            "Batch 8/10, Batch Loss: 263660.1250\n",
            "Batch 9/10, Batch Loss: 267674.8750\n",
            "Batch 10/10, Batch Loss: 221001.7969\n",
            "Test Error: \n",
            " Avg loss: 299294.264062, Avg error: 202993.022500 \n",
            "\n",
            "Epoch 128, Batch 1/38, Batch Loss: 255208.8906\n",
            "Epoch 128, Batch 2/38, Batch Loss: 288047.4375\n",
            "Epoch 128, Batch 3/38, Batch Loss: 254107.7188\n",
            "Epoch 128, Batch 4/38, Batch Loss: 275353.8750\n",
            "Epoch 128, Batch 5/38, Batch Loss: 277859.8125\n",
            "Epoch 128, Batch 6/38, Batch Loss: 318699.4688\n",
            "Epoch 128, Batch 7/38, Batch Loss: 236129.1406\n",
            "Epoch 128, Batch 8/38, Batch Loss: 925553.0625\n",
            "Epoch 128, Batch 9/38, Batch Loss: 712206.2500\n",
            "Epoch 128, Batch 10/38, Batch Loss: 243124.8438\n",
            "Epoch 128, Batch 11/38, Batch Loss: 264224.6875\n",
            "Epoch 128, Batch 12/38, Batch Loss: 261970.7812\n",
            "Epoch 128, Batch 13/38, Batch Loss: 272444.8125\n",
            "Epoch 128, Batch 14/38, Batch Loss: 261748.7812\n",
            "Epoch 128, Batch 15/38, Batch Loss: 301851.2812\n",
            "Epoch 128, Batch 16/38, Batch Loss: 376033.1250\n",
            "Epoch 128, Batch 17/38, Batch Loss: 233950.0312\n",
            "Epoch 128, Batch 18/38, Batch Loss: 329754.0312\n",
            "Epoch 128, Batch 19/38, Batch Loss: 701405.8750\n",
            "Epoch 128, Batch 20/38, Batch Loss: 347562.7812\n",
            "Epoch 128, Batch 21/38, Batch Loss: 274836.2500\n",
            "Epoch 128, Batch 22/38, Batch Loss: 309082.3438\n",
            "Epoch 128, Batch 23/38, Batch Loss: 319669.1250\n",
            "Epoch 128, Batch 24/38, Batch Loss: 326053.5625\n",
            "Epoch 128, Batch 25/38, Batch Loss: 318350.9375\n",
            "Epoch 128, Batch 26/38, Batch Loss: 362216.5000\n",
            "Epoch 128, Batch 27/38, Batch Loss: 361711.1875\n",
            "Epoch 128, Batch 28/38, Batch Loss: 681232.9375\n",
            "Epoch 128, Batch 29/38, Batch Loss: 284976.8438\n",
            "Epoch 128, Batch 30/38, Batch Loss: 274822.5000\n",
            "Epoch 128, Batch 31/38, Batch Loss: 276547.3750\n",
            "Epoch 128, Batch 32/38, Batch Loss: 323368.6250\n",
            "Epoch 128, Batch 33/38, Batch Loss: 290364.2500\n",
            "Epoch 128, Batch 34/38, Batch Loss: 276927.5938\n",
            "Epoch 128, Batch 35/38, Batch Loss: 306736.3125\n",
            "Epoch 128, Batch 36/38, Batch Loss: 692115.9375\n",
            "Epoch 128, Batch 37/38, Batch Loss: 731594.1875\n",
            "Epoch 128, Batch 38/38, Batch Loss: 365143.4688\n",
            "Batch 1/10, Batch Loss: 174874.0938\n",
            "Batch 2/10, Batch Loss: 239623.1562\n",
            "Batch 3/10, Batch Loss: 288492.9375\n",
            "Batch 4/10, Batch Loss: 353153.0312\n",
            "Batch 5/10, Batch Loss: 449241.8125\n",
            "Batch 6/10, Batch Loss: 328082.8125\n",
            "Batch 7/10, Batch Loss: 296747.5312\n",
            "Batch 8/10, Batch Loss: 247721.0312\n",
            "Batch 9/10, Batch Loss: 424702.3750\n",
            "Batch 10/10, Batch Loss: 279178.7500\n",
            "Test Error: \n",
            " Avg loss: 308181.753125, Avg error: 205676.141667 \n",
            "\n",
            "Epoch 129, Batch 1/38, Batch Loss: 223989.1250\n",
            "Epoch 129, Batch 2/38, Batch Loss: 253736.6719\n",
            "Epoch 129, Batch 3/38, Batch Loss: 237459.4531\n",
            "Epoch 129, Batch 4/38, Batch Loss: 249288.9375\n",
            "Epoch 129, Batch 5/38, Batch Loss: 246990.8750\n",
            "Epoch 129, Batch 6/38, Batch Loss: 274375.7812\n",
            "Epoch 129, Batch 7/38, Batch Loss: 296276.3750\n",
            "Epoch 129, Batch 8/38, Batch Loss: 252876.2500\n",
            "Epoch 129, Batch 9/38, Batch Loss: 384993.3438\n",
            "Epoch 129, Batch 10/38, Batch Loss: 244792.7812\n",
            "Epoch 129, Batch 11/38, Batch Loss: 315884.5312\n",
            "Epoch 129, Batch 12/38, Batch Loss: 310378.8750\n",
            "Epoch 129, Batch 13/38, Batch Loss: 399151.3750\n",
            "Epoch 129, Batch 14/38, Batch Loss: 335515.1875\n",
            "Epoch 129, Batch 15/38, Batch Loss: 337923.5625\n",
            "Epoch 129, Batch 16/38, Batch Loss: 304086.4062\n",
            "Epoch 129, Batch 17/38, Batch Loss: 272261.0000\n",
            "Epoch 129, Batch 18/38, Batch Loss: 660679.7500\n",
            "Epoch 129, Batch 19/38, Batch Loss: 267688.2500\n",
            "Epoch 129, Batch 20/38, Batch Loss: 301818.3750\n",
            "Epoch 129, Batch 21/38, Batch Loss: 374011.2812\n",
            "Epoch 129, Batch 22/38, Batch Loss: 259230.1094\n",
            "Epoch 129, Batch 23/38, Batch Loss: 204313.6719\n",
            "Epoch 129, Batch 24/38, Batch Loss: 235243.4375\n",
            "Epoch 129, Batch 25/38, Batch Loss: 1135645.1250\n",
            "Epoch 129, Batch 26/38, Batch Loss: 362463.0312\n",
            "Epoch 129, Batch 27/38, Batch Loss: 232294.4531\n",
            "Epoch 129, Batch 28/38, Batch Loss: 347709.4375\n",
            "Epoch 129, Batch 29/38, Batch Loss: 662836.6875\n",
            "Epoch 129, Batch 30/38, Batch Loss: 726687.0000\n",
            "Epoch 129, Batch 31/38, Batch Loss: 272810.1875\n",
            "Epoch 129, Batch 32/38, Batch Loss: 341743.7500\n",
            "Epoch 129, Batch 33/38, Batch Loss: 239747.1719\n",
            "Epoch 129, Batch 34/38, Batch Loss: 402454.1875\n",
            "Epoch 129, Batch 35/38, Batch Loss: 710112.3750\n",
            "Epoch 129, Batch 36/38, Batch Loss: 254706.6406\n",
            "Epoch 129, Batch 37/38, Batch Loss: 322128.7500\n",
            "Epoch 129, Batch 38/38, Batch Loss: 524561.0625\n",
            "Batch 1/10, Batch Loss: 346332.5625\n",
            "Batch 2/10, Batch Loss: 416288.4062\n",
            "Batch 3/10, Batch Loss: 266444.6250\n",
            "Batch 4/10, Batch Loss: 246200.7188\n",
            "Batch 5/10, Batch Loss: 325647.0000\n",
            "Batch 6/10, Batch Loss: 321253.4062\n",
            "Batch 7/10, Batch Loss: 283551.8438\n",
            "Batch 8/10, Batch Loss: 298725.2500\n",
            "Batch 9/10, Batch Loss: 257086.6406\n",
            "Batch 10/10, Batch Loss: 233297.0469\n",
            "Test Error: \n",
            " Avg loss: 299482.750000, Avg error: 207144.165833 \n",
            "\n",
            "Epoch 130, Batch 1/38, Batch Loss: 307298.0000\n",
            "Epoch 130, Batch 2/38, Batch Loss: 360842.8750\n",
            "Epoch 130, Batch 3/38, Batch Loss: 260512.9844\n",
            "Epoch 130, Batch 4/38, Batch Loss: 244237.9062\n",
            "Epoch 130, Batch 5/38, Batch Loss: 694997.3125\n",
            "Epoch 130, Batch 6/38, Batch Loss: 244749.7344\n",
            "Epoch 130, Batch 7/38, Batch Loss: 466818.7500\n",
            "Epoch 130, Batch 8/38, Batch Loss: 250709.5469\n",
            "Epoch 130, Batch 9/38, Batch Loss: 251726.9219\n",
            "Epoch 130, Batch 10/38, Batch Loss: 935697.5625\n",
            "Epoch 130, Batch 11/38, Batch Loss: 258693.4375\n",
            "Epoch 130, Batch 12/38, Batch Loss: 469824.2500\n",
            "Epoch 130, Batch 13/38, Batch Loss: 341328.0000\n",
            "Epoch 130, Batch 14/38, Batch Loss: 269214.0312\n",
            "Epoch 130, Batch 15/38, Batch Loss: 296074.6875\n",
            "Epoch 130, Batch 16/38, Batch Loss: 304523.8438\n",
            "Epoch 130, Batch 17/38, Batch Loss: 245930.5000\n",
            "Epoch 130, Batch 18/38, Batch Loss: 252142.8594\n",
            "Epoch 130, Batch 19/38, Batch Loss: 682775.5000\n",
            "Epoch 130, Batch 20/38, Batch Loss: 228376.8906\n",
            "Epoch 130, Batch 21/38, Batch Loss: 353393.5625\n",
            "Epoch 130, Batch 22/38, Batch Loss: 853846.9375\n",
            "Epoch 130, Batch 23/38, Batch Loss: 316215.4688\n",
            "Epoch 130, Batch 24/38, Batch Loss: 274981.0625\n",
            "Epoch 130, Batch 25/38, Batch Loss: 255368.0938\n",
            "Epoch 130, Batch 26/38, Batch Loss: 294720.6250\n",
            "Epoch 130, Batch 27/38, Batch Loss: 286884.5938\n",
            "Epoch 130, Batch 28/38, Batch Loss: 757288.0625\n",
            "Epoch 130, Batch 29/38, Batch Loss: 292278.7500\n",
            "Epoch 130, Batch 30/38, Batch Loss: 336724.1875\n",
            "Epoch 130, Batch 31/38, Batch Loss: 249914.0156\n",
            "Epoch 130, Batch 32/38, Batch Loss: 296739.6250\n",
            "Epoch 130, Batch 33/38, Batch Loss: 263838.0625\n",
            "Epoch 130, Batch 34/38, Batch Loss: 379259.7812\n",
            "Epoch 130, Batch 35/38, Batch Loss: 709448.6875\n",
            "Epoch 130, Batch 36/38, Batch Loss: 685703.7500\n",
            "Epoch 130, Batch 37/38, Batch Loss: 283026.2188\n",
            "Epoch 130, Batch 38/38, Batch Loss: 285183.3750\n",
            "Batch 1/10, Batch Loss: 231986.3125\n",
            "Batch 2/10, Batch Loss: 261925.9531\n",
            "Batch 3/10, Batch Loss: 320816.9688\n",
            "Batch 4/10, Batch Loss: 314486.3125\n",
            "Batch 5/10, Batch Loss: 329889.2500\n",
            "Batch 6/10, Batch Loss: 326929.7812\n",
            "Batch 7/10, Batch Loss: 363570.8438\n",
            "Batch 8/10, Batch Loss: 348281.5312\n",
            "Batch 9/10, Batch Loss: 269889.9688\n",
            "Batch 10/10, Batch Loss: 322979.6562\n",
            "Test Error: \n",
            " Avg loss: 309075.657813, Avg error: 202770.328333 \n",
            "\n",
            "2024-09-14 20:22:09.573256 Epoch 130, Average Training loss 382665.5382401316\n",
            "Epoch 131, Batch 1/38, Batch Loss: 232674.8438\n",
            "Epoch 131, Batch 2/38, Batch Loss: 216013.3438\n",
            "Epoch 131, Batch 3/38, Batch Loss: 408211.0938\n",
            "Epoch 131, Batch 4/38, Batch Loss: 276153.6562\n",
            "Epoch 131, Batch 5/38, Batch Loss: 252533.3281\n",
            "Epoch 131, Batch 6/38, Batch Loss: 671334.7500\n",
            "Epoch 131, Batch 7/38, Batch Loss: 408745.4375\n",
            "Epoch 131, Batch 8/38, Batch Loss: 243227.6094\n",
            "Epoch 131, Batch 9/38, Batch Loss: 751933.0000\n",
            "Epoch 131, Batch 10/38, Batch Loss: 253317.0469\n",
            "Epoch 131, Batch 11/38, Batch Loss: 248803.3750\n",
            "Epoch 131, Batch 12/38, Batch Loss: 323239.4062\n",
            "Epoch 131, Batch 13/38, Batch Loss: 415622.1250\n",
            "Epoch 131, Batch 14/38, Batch Loss: 935149.8125\n",
            "Epoch 131, Batch 15/38, Batch Loss: 681404.7500\n",
            "Epoch 131, Batch 16/38, Batch Loss: 291948.2188\n",
            "Epoch 131, Batch 17/38, Batch Loss: 354236.9688\n",
            "Epoch 131, Batch 18/38, Batch Loss: 267516.4062\n",
            "Epoch 131, Batch 19/38, Batch Loss: 678529.3750\n",
            "Epoch 131, Batch 20/38, Batch Loss: 251680.8438\n",
            "Epoch 131, Batch 21/38, Batch Loss: 320307.8438\n",
            "Epoch 131, Batch 22/38, Batch Loss: 355281.2188\n",
            "Epoch 131, Batch 23/38, Batch Loss: 359299.8750\n",
            "Epoch 131, Batch 24/38, Batch Loss: 261165.1719\n",
            "Epoch 131, Batch 25/38, Batch Loss: 345556.9375\n",
            "Epoch 131, Batch 26/38, Batch Loss: 268468.8750\n",
            "Epoch 131, Batch 27/38, Batch Loss: 280188.2812\n",
            "Epoch 131, Batch 28/38, Batch Loss: 217416.8281\n",
            "Epoch 131, Batch 29/38, Batch Loss: 411059.8750\n",
            "Epoch 131, Batch 30/38, Batch Loss: 251991.9531\n",
            "Epoch 131, Batch 31/38, Batch Loss: 263494.0000\n",
            "Epoch 131, Batch 32/38, Batch Loss: 282933.0625\n",
            "Epoch 131, Batch 33/38, Batch Loss: 224397.2188\n",
            "Epoch 131, Batch 34/38, Batch Loss: 358111.5000\n",
            "Epoch 131, Batch 35/38, Batch Loss: 669649.8750\n",
            "Epoch 131, Batch 36/38, Batch Loss: 372293.6562\n",
            "Epoch 131, Batch 37/38, Batch Loss: 289931.6250\n",
            "Epoch 131, Batch 38/38, Batch Loss: 518123.5625\n",
            "Batch 1/10, Batch Loss: 306600.4375\n",
            "Batch 2/10, Batch Loss: 362333.6875\n",
            "Batch 3/10, Batch Loss: 401029.7500\n",
            "Batch 4/10, Batch Loss: 224236.8906\n",
            "Batch 5/10, Batch Loss: 289023.4375\n",
            "Batch 6/10, Batch Loss: 409453.4688\n",
            "Batch 7/10, Batch Loss: 263541.3750\n",
            "Batch 8/10, Batch Loss: 362995.2812\n",
            "Batch 9/10, Batch Loss: 208120.5469\n",
            "Batch 10/10, Batch Loss: 230566.5469\n",
            "Test Error: \n",
            " Avg loss: 305790.142188, Avg error: 203120.520000 \n",
            "\n",
            "Epoch 132, Batch 1/38, Batch Loss: 225136.2656\n",
            "Epoch 132, Batch 2/38, Batch Loss: 683991.8750\n",
            "Epoch 132, Batch 3/38, Batch Loss: 724928.4375\n",
            "Epoch 132, Batch 4/38, Batch Loss: 218524.6562\n",
            "Epoch 132, Batch 5/38, Batch Loss: 270518.2188\n",
            "Epoch 132, Batch 6/38, Batch Loss: 349862.1562\n",
            "Epoch 132, Batch 7/38, Batch Loss: 254343.2500\n",
            "Epoch 132, Batch 8/38, Batch Loss: 219124.0312\n",
            "Epoch 132, Batch 9/38, Batch Loss: 224623.8125\n",
            "Epoch 132, Batch 10/38, Batch Loss: 325630.9062\n",
            "Epoch 132, Batch 11/38, Batch Loss: 230339.9219\n",
            "Epoch 132, Batch 12/38, Batch Loss: 649721.1875\n",
            "Epoch 132, Batch 13/38, Batch Loss: 705714.4375\n",
            "Epoch 132, Batch 14/38, Batch Loss: 288031.1875\n",
            "Epoch 132, Batch 15/38, Batch Loss: 266253.8125\n",
            "Epoch 132, Batch 16/38, Batch Loss: 314566.8438\n",
            "Epoch 132, Batch 17/38, Batch Loss: 275712.5312\n",
            "Epoch 132, Batch 18/38, Batch Loss: 377227.7188\n",
            "Epoch 132, Batch 19/38, Batch Loss: 298363.5625\n",
            "Epoch 132, Batch 20/38, Batch Loss: 241500.9062\n",
            "Epoch 132, Batch 21/38, Batch Loss: 678406.8125\n",
            "Epoch 132, Batch 22/38, Batch Loss: 345004.9375\n",
            "Epoch 132, Batch 23/38, Batch Loss: 284893.5312\n",
            "Epoch 132, Batch 24/38, Batch Loss: 682751.3125\n",
            "Epoch 132, Batch 25/38, Batch Loss: 290030.5625\n",
            "Epoch 132, Batch 26/38, Batch Loss: 234113.6562\n",
            "Epoch 132, Batch 27/38, Batch Loss: 270281.4062\n",
            "Epoch 132, Batch 28/38, Batch Loss: 507461.7188\n",
            "Epoch 132, Batch 29/38, Batch Loss: 327402.2500\n",
            "Epoch 132, Batch 30/38, Batch Loss: 263577.0625\n",
            "Epoch 132, Batch 31/38, Batch Loss: 367099.6875\n",
            "Epoch 132, Batch 32/38, Batch Loss: 247394.7969\n",
            "Epoch 132, Batch 33/38, Batch Loss: 247094.7969\n",
            "Epoch 132, Batch 34/38, Batch Loss: 254789.0469\n",
            "Epoch 132, Batch 35/38, Batch Loss: 329407.7500\n",
            "Epoch 132, Batch 36/38, Batch Loss: 285094.7188\n",
            "Epoch 132, Batch 37/38, Batch Loss: 774106.8125\n",
            "Epoch 132, Batch 38/38, Batch Loss: 223186.2969\n",
            "Batch 1/10, Batch Loss: 225359.7500\n",
            "Batch 2/10, Batch Loss: 342204.9688\n",
            "Batch 3/10, Batch Loss: 387442.9688\n",
            "Batch 4/10, Batch Loss: 319566.3438\n",
            "Batch 5/10, Batch Loss: 290478.9062\n",
            "Batch 6/10, Batch Loss: 430356.6875\n",
            "Batch 7/10, Batch Loss: 336382.3438\n",
            "Batch 8/10, Batch Loss: 262405.0625\n",
            "Batch 9/10, Batch Loss: 280439.1562\n",
            "Batch 10/10, Batch Loss: 390927.5625\n",
            "Test Error: \n",
            " Avg loss: 326556.375000, Avg error: 205388.217500 \n",
            "\n",
            "Epoch 133, Batch 1/38, Batch Loss: 315054.8750\n",
            "Epoch 133, Batch 2/38, Batch Loss: 449413.8438\n",
            "Epoch 133, Batch 3/38, Batch Loss: 346218.9062\n",
            "Epoch 133, Batch 4/38, Batch Loss: 318262.9688\n",
            "Epoch 133, Batch 5/38, Batch Loss: 302025.1250\n",
            "Epoch 133, Batch 6/38, Batch Loss: 310242.3438\n",
            "Epoch 133, Batch 7/38, Batch Loss: 760799.9375\n",
            "Epoch 133, Batch 8/38, Batch Loss: 412610.8750\n",
            "Epoch 133, Batch 9/38, Batch Loss: 366072.1875\n",
            "Epoch 133, Batch 10/38, Batch Loss: 424831.5625\n",
            "Epoch 133, Batch 11/38, Batch Loss: 231165.8438\n",
            "Epoch 133, Batch 12/38, Batch Loss: 250443.0000\n",
            "Epoch 133, Batch 13/38, Batch Loss: 329648.1250\n",
            "Epoch 133, Batch 14/38, Batch Loss: 358036.9688\n",
            "Epoch 133, Batch 15/38, Batch Loss: 311188.4375\n",
            "Epoch 133, Batch 16/38, Batch Loss: 681734.1875\n",
            "Epoch 133, Batch 17/38, Batch Loss: 311023.4375\n",
            "Epoch 133, Batch 18/38, Batch Loss: 268000.8750\n",
            "Epoch 133, Batch 19/38, Batch Loss: 245227.8281\n",
            "Epoch 133, Batch 20/38, Batch Loss: 286663.5312\n",
            "Epoch 133, Batch 21/38, Batch Loss: 258627.1406\n",
            "Epoch 133, Batch 22/38, Batch Loss: 315787.5938\n",
            "Epoch 133, Batch 23/38, Batch Loss: 732680.9375\n",
            "Epoch 133, Batch 24/38, Batch Loss: 260011.3750\n",
            "Epoch 133, Batch 25/38, Batch Loss: 726496.0000\n",
            "Epoch 133, Batch 26/38, Batch Loss: 227108.8594\n",
            "Epoch 133, Batch 27/38, Batch Loss: 304654.1562\n",
            "Epoch 133, Batch 28/38, Batch Loss: 297994.3438\n",
            "Epoch 133, Batch 29/38, Batch Loss: 271947.7188\n",
            "Epoch 133, Batch 30/38, Batch Loss: 673468.1875\n",
            "Epoch 133, Batch 31/38, Batch Loss: 284933.5938\n",
            "Epoch 133, Batch 32/38, Batch Loss: 258091.5312\n",
            "Epoch 133, Batch 33/38, Batch Loss: 697338.9375\n",
            "Epoch 133, Batch 34/38, Batch Loss: 265836.5625\n",
            "Epoch 133, Batch 35/38, Batch Loss: 266563.9688\n",
            "Epoch 133, Batch 36/38, Batch Loss: 397622.8125\n",
            "Epoch 133, Batch 37/38, Batch Loss: 280869.3750\n",
            "Epoch 133, Batch 38/38, Batch Loss: 955384.6875\n",
            "Batch 1/10, Batch Loss: 434079.2812\n",
            "Batch 2/10, Batch Loss: 320229.3750\n",
            "Batch 3/10, Batch Loss: 308412.9688\n",
            "Batch 4/10, Batch Loss: 234851.1250\n",
            "Batch 5/10, Batch Loss: 379867.0625\n",
            "Batch 6/10, Batch Loss: 293495.6875\n",
            "Batch 7/10, Batch Loss: 307174.2500\n",
            "Batch 8/10, Batch Loss: 353648.1562\n",
            "Batch 9/10, Batch Loss: 295601.0625\n",
            "Batch 10/10, Batch Loss: 503304.3750\n",
            "Test Error: \n",
            " Avg loss: 343066.334375, Avg error: 210638.270000 \n",
            "\n",
            "Epoch 134, Batch 1/38, Batch Loss: 198635.2031\n",
            "Epoch 134, Batch 2/38, Batch Loss: 325825.5938\n",
            "Epoch 134, Batch 3/38, Batch Loss: 256119.3906\n",
            "Epoch 134, Batch 4/38, Batch Loss: 308218.1562\n",
            "Epoch 134, Batch 5/38, Batch Loss: 275229.7812\n",
            "Epoch 134, Batch 6/38, Batch Loss: 353022.7812\n",
            "Epoch 134, Batch 7/38, Batch Loss: 271346.1250\n",
            "Epoch 134, Batch 8/38, Batch Loss: 696991.2500\n",
            "Epoch 134, Batch 9/38, Batch Loss: 311044.0625\n",
            "Epoch 134, Batch 10/38, Batch Loss: 260072.5156\n",
            "Epoch 134, Batch 11/38, Batch Loss: 290146.6875\n",
            "Epoch 134, Batch 12/38, Batch Loss: 351479.9062\n",
            "Epoch 134, Batch 13/38, Batch Loss: 334168.4688\n",
            "Epoch 134, Batch 14/38, Batch Loss: 277257.5938\n",
            "Epoch 134, Batch 15/38, Batch Loss: 708491.1250\n",
            "Epoch 134, Batch 16/38, Batch Loss: 298138.5938\n",
            "Epoch 134, Batch 17/38, Batch Loss: 690922.8125\n",
            "Epoch 134, Batch 18/38, Batch Loss: 708108.6250\n",
            "Epoch 134, Batch 19/38, Batch Loss: 385430.4062\n",
            "Epoch 134, Batch 20/38, Batch Loss: 291918.5625\n",
            "Epoch 134, Batch 21/38, Batch Loss: 315630.1250\n",
            "Epoch 134, Batch 22/38, Batch Loss: 376294.5312\n",
            "Epoch 134, Batch 23/38, Batch Loss: 255428.1875\n",
            "Epoch 134, Batch 24/38, Batch Loss: 224331.8750\n",
            "Epoch 134, Batch 25/38, Batch Loss: 337489.2188\n",
            "Epoch 134, Batch 26/38, Batch Loss: 242077.5469\n",
            "Epoch 134, Batch 27/38, Batch Loss: 362861.9688\n",
            "Epoch 134, Batch 28/38, Batch Loss: 332730.5625\n",
            "Epoch 134, Batch 29/38, Batch Loss: 383764.6875\n",
            "Epoch 134, Batch 30/38, Batch Loss: 681212.3125\n",
            "Epoch 134, Batch 31/38, Batch Loss: 245067.5781\n",
            "Epoch 134, Batch 32/38, Batch Loss: 697946.1250\n",
            "Epoch 134, Batch 33/38, Batch Loss: 273669.4062\n",
            "Epoch 134, Batch 34/38, Batch Loss: 330524.1562\n",
            "Epoch 134, Batch 35/38, Batch Loss: 272284.8125\n",
            "Epoch 134, Batch 36/38, Batch Loss: 331215.2812\n",
            "Epoch 134, Batch 37/38, Batch Loss: 270374.7188\n",
            "Epoch 134, Batch 38/38, Batch Loss: 983341.4375\n",
            "Batch 1/10, Batch Loss: 262741.2500\n",
            "Batch 2/10, Batch Loss: 247360.9375\n",
            "Batch 3/10, Batch Loss: 298525.8125\n",
            "Batch 4/10, Batch Loss: 344063.0938\n",
            "Batch 5/10, Batch Loss: 354637.5312\n",
            "Batch 6/10, Batch Loss: 414185.2812\n",
            "Batch 7/10, Batch Loss: 255555.5469\n",
            "Batch 8/10, Batch Loss: 418434.3750\n",
            "Batch 9/10, Batch Loss: 228619.2500\n",
            "Batch 10/10, Batch Loss: 404515.0625\n",
            "Test Error: \n",
            " Avg loss: 322863.814063, Avg error: 203250.663333 \n",
            "\n",
            "Epoch 135, Batch 1/38, Batch Loss: 287022.3125\n",
            "Epoch 135, Batch 2/38, Batch Loss: 276787.1875\n",
            "Epoch 135, Batch 3/38, Batch Loss: 204445.0938\n",
            "Epoch 135, Batch 4/38, Batch Loss: 402103.5938\n",
            "Epoch 135, Batch 5/38, Batch Loss: 678688.5000\n",
            "Epoch 135, Batch 6/38, Batch Loss: 315482.7500\n",
            "Epoch 135, Batch 7/38, Batch Loss: 314486.3750\n",
            "Epoch 135, Batch 8/38, Batch Loss: 320176.9062\n",
            "Epoch 135, Batch 9/38, Batch Loss: 300553.4688\n",
            "Epoch 135, Batch 10/38, Batch Loss: 257084.5938\n",
            "Epoch 135, Batch 11/38, Batch Loss: 226237.6719\n",
            "Epoch 135, Batch 12/38, Batch Loss: 288674.8125\n",
            "Epoch 135, Batch 13/38, Batch Loss: 416422.9062\n",
            "Epoch 135, Batch 14/38, Batch Loss: 290169.6250\n",
            "Epoch 135, Batch 15/38, Batch Loss: 304867.3125\n",
            "Epoch 135, Batch 16/38, Batch Loss: 281172.9375\n",
            "Epoch 135, Batch 17/38, Batch Loss: 267242.5000\n",
            "Epoch 135, Batch 18/38, Batch Loss: 689419.3125\n",
            "Epoch 135, Batch 19/38, Batch Loss: 496056.0000\n",
            "Epoch 135, Batch 20/38, Batch Loss: 670444.5625\n",
            "Epoch 135, Batch 21/38, Batch Loss: 374287.7812\n",
            "Epoch 135, Batch 22/38, Batch Loss: 217760.5000\n",
            "Epoch 135, Batch 23/38, Batch Loss: 343684.7188\n",
            "Epoch 135, Batch 24/38, Batch Loss: 322485.5938\n",
            "Epoch 135, Batch 25/38, Batch Loss: 720000.0625\n",
            "Epoch 135, Batch 26/38, Batch Loss: 274702.3125\n",
            "Epoch 135, Batch 27/38, Batch Loss: 719799.9375\n",
            "Epoch 135, Batch 28/38, Batch Loss: 296263.1562\n",
            "Epoch 135, Batch 29/38, Batch Loss: 679170.3750\n",
            "Epoch 135, Batch 30/38, Batch Loss: 646435.6250\n",
            "Epoch 135, Batch 31/38, Batch Loss: 306391.9062\n",
            "Epoch 135, Batch 32/38, Batch Loss: 299659.1875\n",
            "Epoch 135, Batch 33/38, Batch Loss: 383073.1562\n",
            "Epoch 135, Batch 34/38, Batch Loss: 310289.5000\n",
            "Epoch 135, Batch 35/38, Batch Loss: 271656.5000\n",
            "Epoch 135, Batch 36/38, Batch Loss: 289180.9688\n",
            "Epoch 135, Batch 37/38, Batch Loss: 309543.8125\n",
            "Epoch 135, Batch 38/38, Batch Loss: 328463.9375\n",
            "Batch 1/10, Batch Loss: 238382.0469\n",
            "Batch 2/10, Batch Loss: 260329.1094\n",
            "Batch 3/10, Batch Loss: 271520.5625\n",
            "Batch 4/10, Batch Loss: 326870.2500\n",
            "Batch 5/10, Batch Loss: 386965.0000\n",
            "Batch 6/10, Batch Loss: 333069.7812\n",
            "Batch 7/10, Batch Loss: 297259.9062\n",
            "Batch 8/10, Batch Loss: 386580.2500\n",
            "Batch 9/10, Batch Loss: 245585.9688\n",
            "Batch 10/10, Batch Loss: 432199.9062\n",
            "Test Error: \n",
            " Avg loss: 317876.278125, Avg error: 202110.718333 \n",
            "\n",
            "Epoch 136, Batch 1/38, Batch Loss: 405255.2500\n",
            "Epoch 136, Batch 2/38, Batch Loss: 231050.8438\n",
            "Epoch 136, Batch 3/38, Batch Loss: 226307.4219\n",
            "Epoch 136, Batch 4/38, Batch Loss: 280075.0938\n",
            "Epoch 136, Batch 5/38, Batch Loss: 382867.9688\n",
            "Epoch 136, Batch 6/38, Batch Loss: 300226.6562\n",
            "Epoch 136, Batch 7/38, Batch Loss: 202890.6719\n",
            "Epoch 136, Batch 8/38, Batch Loss: 687995.7500\n",
            "Epoch 136, Batch 9/38, Batch Loss: 352276.2188\n",
            "Epoch 136, Batch 10/38, Batch Loss: 228086.9688\n",
            "Epoch 136, Batch 11/38, Batch Loss: 296302.6562\n",
            "Epoch 136, Batch 12/38, Batch Loss: 307033.7188\n",
            "Epoch 136, Batch 13/38, Batch Loss: 231897.0000\n",
            "Epoch 136, Batch 14/38, Batch Loss: 332707.6250\n",
            "Epoch 136, Batch 15/38, Batch Loss: 679489.8125\n",
            "Epoch 136, Batch 16/38, Batch Loss: 698491.6250\n",
            "Epoch 136, Batch 17/38, Batch Loss: 285269.8125\n",
            "Epoch 136, Batch 18/38, Batch Loss: 258514.7031\n",
            "Epoch 136, Batch 19/38, Batch Loss: 379446.7188\n",
            "Epoch 136, Batch 20/38, Batch Loss: 728486.4375\n",
            "Epoch 136, Batch 21/38, Batch Loss: 656448.0000\n",
            "Epoch 136, Batch 22/38, Batch Loss: 336491.4062\n",
            "Epoch 136, Batch 23/38, Batch Loss: 277386.9688\n",
            "Epoch 136, Batch 24/38, Batch Loss: 216035.8906\n",
            "Epoch 136, Batch 25/38, Batch Loss: 262421.6250\n",
            "Epoch 136, Batch 26/38, Batch Loss: 316850.6562\n",
            "Epoch 136, Batch 27/38, Batch Loss: 398238.8750\n",
            "Epoch 136, Batch 28/38, Batch Loss: 324291.5000\n",
            "Epoch 136, Batch 29/38, Batch Loss: 335136.5625\n",
            "Epoch 136, Batch 30/38, Batch Loss: 687685.1875\n",
            "Epoch 136, Batch 31/38, Batch Loss: 216311.5000\n",
            "Epoch 136, Batch 32/38, Batch Loss: 272503.2500\n",
            "Epoch 136, Batch 33/38, Batch Loss: 325297.0000\n",
            "Epoch 136, Batch 34/38, Batch Loss: 711559.6250\n",
            "Epoch 136, Batch 35/38, Batch Loss: 249004.0000\n",
            "Epoch 136, Batch 36/38, Batch Loss: 320411.6875\n",
            "Epoch 136, Batch 37/38, Batch Loss: 278616.7500\n",
            "Epoch 136, Batch 38/38, Batch Loss: 302367.9375\n",
            "Batch 1/10, Batch Loss: 212755.7188\n",
            "Batch 2/10, Batch Loss: 247283.8906\n",
            "Batch 3/10, Batch Loss: 345704.0312\n",
            "Batch 4/10, Batch Loss: 250860.6875\n",
            "Batch 5/10, Batch Loss: 320642.4375\n",
            "Batch 6/10, Batch Loss: 512477.0000\n",
            "Batch 7/10, Batch Loss: 329951.5000\n",
            "Batch 8/10, Batch Loss: 292097.8125\n",
            "Batch 9/10, Batch Loss: 238929.4375\n",
            "Batch 10/10, Batch Loss: 256299.3750\n",
            "Test Error: \n",
            " Avg loss: 300700.189063, Avg error: 203717.033333 \n",
            "\n",
            "Epoch 137, Batch 1/38, Batch Loss: 223927.8281\n",
            "Epoch 137, Batch 2/38, Batch Loss: 699695.0000\n",
            "Epoch 137, Batch 3/38, Batch Loss: 250889.2500\n",
            "Epoch 137, Batch 4/38, Batch Loss: 223809.1875\n",
            "Epoch 137, Batch 5/38, Batch Loss: 366654.9375\n",
            "Epoch 137, Batch 6/38, Batch Loss: 273593.3438\n",
            "Epoch 137, Batch 7/38, Batch Loss: 255452.1562\n",
            "Epoch 137, Batch 8/38, Batch Loss: 393771.0625\n",
            "Epoch 137, Batch 9/38, Batch Loss: 348207.9062\n",
            "Epoch 137, Batch 10/38, Batch Loss: 253327.6094\n",
            "Epoch 137, Batch 11/38, Batch Loss: 286067.7500\n",
            "Epoch 137, Batch 12/38, Batch Loss: 688494.5000\n",
            "Epoch 137, Batch 13/38, Batch Loss: 320606.1875\n",
            "Epoch 137, Batch 14/38, Batch Loss: 309903.1250\n",
            "Epoch 137, Batch 15/38, Batch Loss: 280726.8125\n",
            "Epoch 137, Batch 16/38, Batch Loss: 279538.6250\n",
            "Epoch 137, Batch 17/38, Batch Loss: 692922.1875\n",
            "Epoch 137, Batch 18/38, Batch Loss: 216721.5938\n",
            "Epoch 137, Batch 19/38, Batch Loss: 314473.0625\n",
            "Epoch 137, Batch 20/38, Batch Loss: 376553.9688\n",
            "Epoch 137, Batch 21/38, Batch Loss: 276910.4375\n",
            "Epoch 137, Batch 22/38, Batch Loss: 295367.5625\n",
            "Epoch 137, Batch 23/38, Batch Loss: 305351.9375\n",
            "Epoch 137, Batch 24/38, Batch Loss: 253417.9375\n",
            "Epoch 137, Batch 25/38, Batch Loss: 673433.0625\n",
            "Epoch 137, Batch 26/38, Batch Loss: 292881.1875\n",
            "Epoch 137, Batch 27/38, Batch Loss: 287473.9688\n",
            "Epoch 137, Batch 28/38, Batch Loss: 326366.8750\n",
            "Epoch 137, Batch 29/38, Batch Loss: 277530.5938\n",
            "Epoch 137, Batch 30/38, Batch Loss: 378917.7500\n",
            "Epoch 137, Batch 31/38, Batch Loss: 255384.2656\n",
            "Epoch 137, Batch 32/38, Batch Loss: 294824.2500\n",
            "Epoch 137, Batch 33/38, Batch Loss: 307820.6250\n",
            "Epoch 137, Batch 34/38, Batch Loss: 278195.5938\n",
            "Epoch 137, Batch 35/38, Batch Loss: 668391.2500\n",
            "Epoch 137, Batch 36/38, Batch Loss: 961099.0000\n",
            "Epoch 137, Batch 37/38, Batch Loss: 364465.1562\n",
            "Epoch 137, Batch 38/38, Batch Loss: 229993.3281\n",
            "Batch 1/10, Batch Loss: 348398.2812\n",
            "Batch 2/10, Batch Loss: 357819.9062\n",
            "Batch 3/10, Batch Loss: 280672.4688\n",
            "Batch 4/10, Batch Loss: 362806.2812\n",
            "Batch 5/10, Batch Loss: 274768.4375\n",
            "Batch 6/10, Batch Loss: 348727.9062\n",
            "Batch 7/10, Batch Loss: 297764.6250\n",
            "Batch 8/10, Batch Loss: 266689.5938\n",
            "Batch 9/10, Batch Loss: 220466.9531\n",
            "Batch 10/10, Batch Loss: 182902.7656\n",
            "Test Error: \n",
            " Avg loss: 294101.721875, Avg error: 201219.282500 \n",
            "\n",
            "Epoch 138, Batch 1/38, Batch Loss: 671052.1875\n",
            "Epoch 138, Batch 2/38, Batch Loss: 278216.2812\n",
            "Epoch 138, Batch 3/38, Batch Loss: 307172.4688\n",
            "Epoch 138, Batch 4/38, Batch Loss: 357321.5000\n",
            "Epoch 138, Batch 5/38, Batch Loss: 261068.5938\n",
            "Epoch 138, Batch 6/38, Batch Loss: 699592.2500\n",
            "Epoch 138, Batch 7/38, Batch Loss: 355777.2188\n",
            "Epoch 138, Batch 8/38, Batch Loss: 306325.0625\n",
            "Epoch 138, Batch 9/38, Batch Loss: 289071.7500\n",
            "Epoch 138, Batch 10/38, Batch Loss: 262637.1250\n",
            "Epoch 138, Batch 11/38, Batch Loss: 378460.4688\n",
            "Epoch 138, Batch 12/38, Batch Loss: 378586.5000\n",
            "Epoch 138, Batch 13/38, Batch Loss: 233821.6250\n",
            "Epoch 138, Batch 14/38, Batch Loss: 420171.1875\n",
            "Epoch 138, Batch 15/38, Batch Loss: 712625.7500\n",
            "Epoch 138, Batch 16/38, Batch Loss: 282713.6250\n",
            "Epoch 138, Batch 17/38, Batch Loss: 231766.3438\n",
            "Epoch 138, Batch 18/38, Batch Loss: 300935.6875\n",
            "Epoch 138, Batch 19/38, Batch Loss: 662246.9375\n",
            "Epoch 138, Batch 20/38, Batch Loss: 671103.4375\n",
            "Epoch 138, Batch 21/38, Batch Loss: 400177.0625\n",
            "Epoch 138, Batch 22/38, Batch Loss: 372857.3438\n",
            "Epoch 138, Batch 23/38, Batch Loss: 270663.2188\n",
            "Epoch 138, Batch 24/38, Batch Loss: 235163.7656\n",
            "Epoch 138, Batch 25/38, Batch Loss: 298943.1250\n",
            "Epoch 138, Batch 26/38, Batch Loss: 235858.0625\n",
            "Epoch 138, Batch 27/38, Batch Loss: 280761.0938\n",
            "Epoch 138, Batch 28/38, Batch Loss: 283528.3438\n",
            "Epoch 138, Batch 29/38, Batch Loss: 254922.5469\n",
            "Epoch 138, Batch 30/38, Batch Loss: 315218.9062\n",
            "Epoch 138, Batch 31/38, Batch Loss: 687568.8125\n",
            "Epoch 138, Batch 32/38, Batch Loss: 262660.7188\n",
            "Epoch 138, Batch 33/38, Batch Loss: 252185.2969\n",
            "Epoch 138, Batch 34/38, Batch Loss: 685431.4375\n",
            "Epoch 138, Batch 35/38, Batch Loss: 373604.2188\n",
            "Epoch 138, Batch 36/38, Batch Loss: 312831.7188\n",
            "Epoch 138, Batch 37/38, Batch Loss: 443754.8125\n",
            "Epoch 138, Batch 38/38, Batch Loss: 245704.4062\n",
            "Batch 1/10, Batch Loss: 249309.1875\n",
            "Batch 2/10, Batch Loss: 342821.9688\n",
            "Batch 3/10, Batch Loss: 310866.5312\n",
            "Batch 4/10, Batch Loss: 369011.1875\n",
            "Batch 5/10, Batch Loss: 281106.5000\n",
            "Batch 6/10, Batch Loss: 306699.3750\n",
            "Batch 7/10, Batch Loss: 253466.0000\n",
            "Batch 8/10, Batch Loss: 264423.5312\n",
            "Batch 9/10, Batch Loss: 380363.9375\n",
            "Batch 10/10, Batch Loss: 241392.5938\n",
            "Test Error: \n",
            " Avg loss: 299946.081250, Avg error: 211438.471667 \n",
            "\n",
            "Epoch 139, Batch 1/38, Batch Loss: 275663.7500\n",
            "Epoch 139, Batch 2/38, Batch Loss: 276028.6250\n",
            "Epoch 139, Batch 3/38, Batch Loss: 312906.2812\n",
            "Epoch 139, Batch 4/38, Batch Loss: 278333.8750\n",
            "Epoch 139, Batch 5/38, Batch Loss: 678554.3750\n",
            "Epoch 139, Batch 6/38, Batch Loss: 214391.7500\n",
            "Epoch 139, Batch 7/38, Batch Loss: 245763.2656\n",
            "Epoch 139, Batch 8/38, Batch Loss: 360818.4688\n",
            "Epoch 139, Batch 9/38, Batch Loss: 238919.0781\n",
            "Epoch 139, Batch 10/38, Batch Loss: 375523.7188\n",
            "Epoch 139, Batch 11/38, Batch Loss: 273677.8750\n",
            "Epoch 139, Batch 12/38, Batch Loss: 298305.2812\n",
            "Epoch 139, Batch 13/38, Batch Loss: 277281.6250\n",
            "Epoch 139, Batch 14/38, Batch Loss: 319005.4688\n",
            "Epoch 139, Batch 15/38, Batch Loss: 194186.2656\n",
            "Epoch 139, Batch 16/38, Batch Loss: 359436.9688\n",
            "Epoch 139, Batch 17/38, Batch Loss: 309143.7812\n",
            "Epoch 139, Batch 18/38, Batch Loss: 275222.1875\n",
            "Epoch 139, Batch 19/38, Batch Loss: 662851.0625\n",
            "Epoch 139, Batch 20/38, Batch Loss: 318493.4375\n",
            "Epoch 139, Batch 21/38, Batch Loss: 676381.5625\n",
            "Epoch 139, Batch 22/38, Batch Loss: 373847.4375\n",
            "Epoch 139, Batch 23/38, Batch Loss: 370948.8438\n",
            "Epoch 139, Batch 24/38, Batch Loss: 276903.1562\n",
            "Epoch 139, Batch 25/38, Batch Loss: 275278.4688\n",
            "Epoch 139, Batch 26/38, Batch Loss: 273369.6562\n",
            "Epoch 139, Batch 27/38, Batch Loss: 258436.4375\n",
            "Epoch 139, Batch 28/38, Batch Loss: 700530.5000\n",
            "Epoch 139, Batch 29/38, Batch Loss: 712414.1875\n",
            "Epoch 139, Batch 30/38, Batch Loss: 457327.8750\n",
            "Epoch 139, Batch 31/38, Batch Loss: 198911.9688\n",
            "Epoch 139, Batch 32/38, Batch Loss: 720885.9375\n",
            "Epoch 139, Batch 33/38, Batch Loss: 716982.8125\n",
            "Epoch 139, Batch 34/38, Batch Loss: 456416.3750\n",
            "Epoch 139, Batch 35/38, Batch Loss: 286110.0938\n",
            "Epoch 139, Batch 36/38, Batch Loss: 510372.1250\n",
            "Epoch 139, Batch 37/38, Batch Loss: 277716.7812\n",
            "Epoch 139, Batch 38/38, Batch Loss: 509812.3438\n",
            "Batch 1/10, Batch Loss: 257062.5156\n",
            "Batch 2/10, Batch Loss: 298559.7812\n",
            "Batch 3/10, Batch Loss: 271409.9062\n",
            "Batch 4/10, Batch Loss: 266052.1562\n",
            "Batch 5/10, Batch Loss: 318532.0000\n",
            "Batch 6/10, Batch Loss: 358436.2812\n",
            "Batch 7/10, Batch Loss: 225599.9531\n",
            "Batch 8/10, Batch Loss: 410091.6562\n",
            "Batch 9/10, Batch Loss: 263070.9375\n",
            "Batch 10/10, Batch Loss: 472468.2812\n",
            "Test Error: \n",
            " Avg loss: 314128.346875, Avg error: 203931.030833 \n",
            "\n",
            "Epoch 140, Batch 1/38, Batch Loss: 357191.1562\n",
            "Epoch 140, Batch 2/38, Batch Loss: 893742.1875\n",
            "Epoch 140, Batch 3/38, Batch Loss: 250411.4688\n",
            "Epoch 140, Batch 4/38, Batch Loss: 332592.9688\n",
            "Epoch 140, Batch 5/38, Batch Loss: 692646.3125\n",
            "Epoch 140, Batch 6/38, Batch Loss: 273807.4062\n",
            "Epoch 140, Batch 7/38, Batch Loss: 666964.6250\n",
            "Epoch 140, Batch 8/38, Batch Loss: 395552.5312\n",
            "Epoch 140, Batch 9/38, Batch Loss: 255838.0469\n",
            "Epoch 140, Batch 10/38, Batch Loss: 256036.3125\n",
            "Epoch 140, Batch 11/38, Batch Loss: 364924.2188\n",
            "Epoch 140, Batch 12/38, Batch Loss: 308362.7188\n",
            "Epoch 140, Batch 13/38, Batch Loss: 677924.8750\n",
            "Epoch 140, Batch 14/38, Batch Loss: 238380.7344\n",
            "Epoch 140, Batch 15/38, Batch Loss: 220193.3125\n",
            "Epoch 140, Batch 16/38, Batch Loss: 314636.9375\n",
            "Epoch 140, Batch 17/38, Batch Loss: 332455.9688\n",
            "Epoch 140, Batch 18/38, Batch Loss: 255151.9844\n",
            "Epoch 140, Batch 19/38, Batch Loss: 572438.1250\n",
            "Epoch 140, Batch 20/38, Batch Loss: 305525.1562\n",
            "Epoch 140, Batch 21/38, Batch Loss: 248978.3594\n",
            "Epoch 140, Batch 22/38, Batch Loss: 336956.7500\n",
            "Epoch 140, Batch 23/38, Batch Loss: 693363.8125\n",
            "Epoch 140, Batch 24/38, Batch Loss: 319004.0625\n",
            "Epoch 140, Batch 25/38, Batch Loss: 327915.0938\n",
            "Epoch 140, Batch 26/38, Batch Loss: 376440.8125\n",
            "Epoch 140, Batch 27/38, Batch Loss: 338617.1250\n",
            "Epoch 140, Batch 28/38, Batch Loss: 289297.6250\n",
            "Epoch 140, Batch 29/38, Batch Loss: 721368.9375\n",
            "Epoch 140, Batch 30/38, Batch Loss: 459843.4062\n",
            "Epoch 140, Batch 31/38, Batch Loss: 284655.8750\n",
            "Epoch 140, Batch 32/38, Batch Loss: 286937.8750\n",
            "Epoch 140, Batch 33/38, Batch Loss: 217810.7969\n",
            "Epoch 140, Batch 34/38, Batch Loss: 353247.3750\n",
            "Epoch 140, Batch 35/38, Batch Loss: 325448.5312\n",
            "Epoch 140, Batch 36/38, Batch Loss: 288836.9688\n",
            "Epoch 140, Batch 37/38, Batch Loss: 319415.4375\n",
            "Epoch 140, Batch 38/38, Batch Loss: 153939.9062\n",
            "Batch 1/10, Batch Loss: 314071.5625\n",
            "Batch 2/10, Batch Loss: 224799.1094\n",
            "Batch 3/10, Batch Loss: 339327.4062\n",
            "Batch 4/10, Batch Loss: 317437.5625\n",
            "Batch 5/10, Batch Loss: 262052.7188\n",
            "Batch 6/10, Batch Loss: 505584.0938\n",
            "Batch 7/10, Batch Loss: 381411.4062\n",
            "Batch 8/10, Batch Loss: 270613.4062\n",
            "Batch 9/10, Batch Loss: 348222.4375\n",
            "Batch 10/10, Batch Loss: 329688.1250\n",
            "Test Error: \n",
            " Avg loss: 329320.782813, Avg error: 212610.845000 \n",
            "\n",
            "2024-09-14 20:28:53.885864 Epoch 140, Average Training loss 376496.20518092107\n",
            "Epoch 141, Batch 1/38, Batch Loss: 303615.0000\n",
            "Epoch 141, Batch 2/38, Batch Loss: 334045.6562\n",
            "Epoch 141, Batch 3/38, Batch Loss: 311796.0000\n",
            "Epoch 141, Batch 4/38, Batch Loss: 267461.1250\n",
            "Epoch 141, Batch 5/38, Batch Loss: 299014.3750\n",
            "Epoch 141, Batch 6/38, Batch Loss: 691502.3750\n",
            "Epoch 141, Batch 7/38, Batch Loss: 253783.4688\n",
            "Epoch 141, Batch 8/38, Batch Loss: 696402.6875\n",
            "Epoch 141, Batch 9/38, Batch Loss: 472986.0938\n",
            "Epoch 141, Batch 10/38, Batch Loss: 291171.2188\n",
            "Epoch 141, Batch 11/38, Batch Loss: 295232.6875\n",
            "Epoch 141, Batch 12/38, Batch Loss: 675876.4375\n",
            "Epoch 141, Batch 13/38, Batch Loss: 262833.8750\n",
            "Epoch 141, Batch 14/38, Batch Loss: 656931.5000\n",
            "Epoch 141, Batch 15/38, Batch Loss: 328332.8438\n",
            "Epoch 141, Batch 16/38, Batch Loss: 276795.8438\n",
            "Epoch 141, Batch 17/38, Batch Loss: 311104.1250\n",
            "Epoch 141, Batch 18/38, Batch Loss: 276600.8438\n",
            "Epoch 141, Batch 19/38, Batch Loss: 283969.7500\n",
            "Epoch 141, Batch 20/38, Batch Loss: 332395.8750\n",
            "Epoch 141, Batch 21/38, Batch Loss: 282678.7188\n",
            "Epoch 141, Batch 22/38, Batch Loss: 243764.2344\n",
            "Epoch 141, Batch 23/38, Batch Loss: 321757.7812\n",
            "Epoch 141, Batch 24/38, Batch Loss: 680712.2500\n",
            "Epoch 141, Batch 25/38, Batch Loss: 301854.9688\n",
            "Epoch 141, Batch 26/38, Batch Loss: 318163.2188\n",
            "Epoch 141, Batch 27/38, Batch Loss: 769535.1875\n",
            "Epoch 141, Batch 28/38, Batch Loss: 313741.8125\n",
            "Epoch 141, Batch 29/38, Batch Loss: 354573.3125\n",
            "Epoch 141, Batch 30/38, Batch Loss: 279572.4062\n",
            "Epoch 141, Batch 31/38, Batch Loss: 292595.4688\n",
            "Epoch 141, Batch 32/38, Batch Loss: 430353.1250\n",
            "Epoch 141, Batch 33/38, Batch Loss: 236766.0000\n",
            "Epoch 141, Batch 34/38, Batch Loss: 289051.4375\n",
            "Epoch 141, Batch 35/38, Batch Loss: 270540.1562\n",
            "Epoch 141, Batch 36/38, Batch Loss: 665399.8125\n",
            "Epoch 141, Batch 37/38, Batch Loss: 275474.2188\n",
            "Epoch 141, Batch 38/38, Batch Loss: 269589.2812\n",
            "Batch 1/10, Batch Loss: 256171.9688\n",
            "Batch 2/10, Batch Loss: 250824.7812\n",
            "Batch 3/10, Batch Loss: 322907.6562\n",
            "Batch 4/10, Batch Loss: 339420.3125\n",
            "Batch 5/10, Batch Loss: 404570.9375\n",
            "Batch 6/10, Batch Loss: 301349.5312\n",
            "Batch 7/10, Batch Loss: 270654.1562\n",
            "Batch 8/10, Batch Loss: 381464.0625\n",
            "Batch 9/10, Batch Loss: 353094.3125\n",
            "Batch 10/10, Batch Loss: 230762.4375\n",
            "Test Error: \n",
            " Avg loss: 311122.015625, Avg error: 203152.705000 \n",
            "\n",
            "Epoch 142, Batch 1/38, Batch Loss: 332787.3750\n",
            "Epoch 142, Batch 2/38, Batch Loss: 249247.4531\n",
            "Epoch 142, Batch 3/38, Batch Loss: 361503.7188\n",
            "Epoch 142, Batch 4/38, Batch Loss: 269909.2188\n",
            "Epoch 142, Batch 5/38, Batch Loss: 296620.1562\n",
            "Epoch 142, Batch 6/38, Batch Loss: 291884.7188\n",
            "Epoch 142, Batch 7/38, Batch Loss: 218970.2500\n",
            "Epoch 142, Batch 8/38, Batch Loss: 271890.7188\n",
            "Epoch 142, Batch 9/38, Batch Loss: 335385.4062\n",
            "Epoch 142, Batch 10/38, Batch Loss: 695749.9375\n",
            "Epoch 142, Batch 11/38, Batch Loss: 230164.7344\n",
            "Epoch 142, Batch 12/38, Batch Loss: 270141.4062\n",
            "Epoch 142, Batch 13/38, Batch Loss: 221190.1875\n",
            "Epoch 142, Batch 14/38, Batch Loss: 944298.6250\n",
            "Epoch 142, Batch 15/38, Batch Loss: 298873.8125\n",
            "Epoch 142, Batch 16/38, Batch Loss: 252730.8438\n",
            "Epoch 142, Batch 17/38, Batch Loss: 231448.3125\n",
            "Epoch 142, Batch 18/38, Batch Loss: 252329.8438\n",
            "Epoch 142, Batch 19/38, Batch Loss: 633694.1250\n",
            "Epoch 142, Batch 20/38, Batch Loss: 316918.2812\n",
            "Epoch 142, Batch 21/38, Batch Loss: 323262.8750\n",
            "Epoch 142, Batch 22/38, Batch Loss: 268578.9375\n",
            "Epoch 142, Batch 23/38, Batch Loss: 308189.6875\n",
            "Epoch 142, Batch 24/38, Batch Loss: 333877.0000\n",
            "Epoch 142, Batch 25/38, Batch Loss: 689384.1875\n",
            "Epoch 142, Batch 26/38, Batch Loss: 249527.1250\n",
            "Epoch 142, Batch 27/38, Batch Loss: 316297.5312\n",
            "Epoch 142, Batch 28/38, Batch Loss: 661795.1250\n",
            "Epoch 142, Batch 29/38, Batch Loss: 226609.9062\n",
            "Epoch 142, Batch 30/38, Batch Loss: 369488.3125\n",
            "Epoch 142, Batch 31/38, Batch Loss: 408036.1875\n",
            "Epoch 142, Batch 32/38, Batch Loss: 391081.9062\n",
            "Epoch 142, Batch 33/38, Batch Loss: 671945.3125\n",
            "Epoch 142, Batch 34/38, Batch Loss: 328455.9375\n",
            "Epoch 142, Batch 35/38, Batch Loss: 386259.9375\n",
            "Epoch 142, Batch 36/38, Batch Loss: 306501.8750\n",
            "Epoch 142, Batch 37/38, Batch Loss: 342905.8125\n",
            "Epoch 142, Batch 38/38, Batch Loss: 326749.7188\n",
            "Batch 1/10, Batch Loss: 320921.1562\n",
            "Batch 2/10, Batch Loss: 322786.2188\n",
            "Batch 3/10, Batch Loss: 345222.8438\n",
            "Batch 4/10, Batch Loss: 308775.5312\n",
            "Batch 5/10, Batch Loss: 244265.2500\n",
            "Batch 6/10, Batch Loss: 231779.8125\n",
            "Batch 7/10, Batch Loss: 322245.0000\n",
            "Batch 8/10, Batch Loss: 274023.2188\n",
            "Batch 9/10, Batch Loss: 260811.1094\n",
            "Batch 10/10, Batch Loss: 448780.0625\n",
            "Test Error: \n",
            " Avg loss: 307961.020313, Avg error: 205832.325000 \n",
            "\n",
            "Epoch 143, Batch 1/38, Batch Loss: 272966.3750\n",
            "Epoch 143, Batch 2/38, Batch Loss: 936978.1875\n",
            "Epoch 143, Batch 3/38, Batch Loss: 246962.1562\n",
            "Epoch 143, Batch 4/38, Batch Loss: 196514.5781\n",
            "Epoch 143, Batch 5/38, Batch Loss: 372566.8438\n",
            "Epoch 143, Batch 6/38, Batch Loss: 306884.6875\n",
            "Epoch 143, Batch 7/38, Batch Loss: 314624.6250\n",
            "Epoch 143, Batch 8/38, Batch Loss: 290448.6250\n",
            "Epoch 143, Batch 9/38, Batch Loss: 251487.0000\n",
            "Epoch 143, Batch 10/38, Batch Loss: 282457.4375\n",
            "Epoch 143, Batch 11/38, Batch Loss: 328599.2500\n",
            "Epoch 143, Batch 12/38, Batch Loss: 332237.3438\n",
            "Epoch 143, Batch 13/38, Batch Loss: 301888.6562\n",
            "Epoch 143, Batch 14/38, Batch Loss: 916381.8125\n",
            "Epoch 143, Batch 15/38, Batch Loss: 272098.5312\n",
            "Epoch 143, Batch 16/38, Batch Loss: 307740.0312\n",
            "Epoch 143, Batch 17/38, Batch Loss: 339491.1250\n",
            "Epoch 143, Batch 18/38, Batch Loss: 312895.7188\n",
            "Epoch 143, Batch 19/38, Batch Loss: 319403.1250\n",
            "Epoch 143, Batch 20/38, Batch Loss: 337238.8438\n",
            "Epoch 143, Batch 21/38, Batch Loss: 403487.5625\n",
            "Epoch 143, Batch 22/38, Batch Loss: 354706.4688\n",
            "Epoch 143, Batch 23/38, Batch Loss: 237840.4219\n",
            "Epoch 143, Batch 24/38, Batch Loss: 714513.5625\n",
            "Epoch 143, Batch 25/38, Batch Loss: 393721.6875\n",
            "Epoch 143, Batch 26/38, Batch Loss: 691173.8750\n",
            "Epoch 143, Batch 27/38, Batch Loss: 289329.9062\n",
            "Epoch 143, Batch 28/38, Batch Loss: 707450.0625\n",
            "Epoch 143, Batch 29/38, Batch Loss: 302575.1875\n",
            "Epoch 143, Batch 30/38, Batch Loss: 314542.9062\n",
            "Epoch 143, Batch 31/38, Batch Loss: 239033.7344\n",
            "Epoch 143, Batch 32/38, Batch Loss: 217873.1250\n",
            "Epoch 143, Batch 33/38, Batch Loss: 303641.9375\n",
            "Epoch 143, Batch 34/38, Batch Loss: 284075.7188\n",
            "Epoch 143, Batch 35/38, Batch Loss: 338518.5625\n",
            "Epoch 143, Batch 36/38, Batch Loss: 333918.4062\n",
            "Epoch 143, Batch 37/38, Batch Loss: 403889.6250\n",
            "Epoch 143, Batch 38/38, Batch Loss: 525256.9375\n",
            "Batch 1/10, Batch Loss: 251894.4531\n",
            "Batch 2/10, Batch Loss: 370145.6875\n",
            "Batch 3/10, Batch Loss: 228308.6562\n",
            "Batch 4/10, Batch Loss: 293140.7500\n",
            "Batch 5/10, Batch Loss: 264827.7500\n",
            "Batch 6/10, Batch Loss: 297441.4062\n",
            "Batch 7/10, Batch Loss: 399978.7500\n",
            "Batch 8/10, Batch Loss: 337470.9688\n",
            "Batch 9/10, Batch Loss: 269527.0625\n",
            "Batch 10/10, Batch Loss: 280312.1250\n",
            "Test Error: \n",
            " Avg loss: 299304.760937, Avg error: 202693.448333 \n",
            "\n",
            "Epoch 144, Batch 1/38, Batch Loss: 912570.0625\n",
            "Epoch 144, Batch 2/38, Batch Loss: 326486.1875\n",
            "Epoch 144, Batch 3/38, Batch Loss: 337785.2812\n",
            "Epoch 144, Batch 4/38, Batch Loss: 398069.0000\n",
            "Epoch 144, Batch 5/38, Batch Loss: 266970.9062\n",
            "Epoch 144, Batch 6/38, Batch Loss: 685768.1875\n",
            "Epoch 144, Batch 7/38, Batch Loss: 688282.7500\n",
            "Epoch 144, Batch 8/38, Batch Loss: 394984.7500\n",
            "Epoch 144, Batch 9/38, Batch Loss: 258192.4688\n",
            "Epoch 144, Batch 10/38, Batch Loss: 411461.9375\n",
            "Epoch 144, Batch 11/38, Batch Loss: 291543.3750\n",
            "Epoch 144, Batch 12/38, Batch Loss: 288122.3125\n",
            "Epoch 144, Batch 13/38, Batch Loss: 231198.0781\n",
            "Epoch 144, Batch 14/38, Batch Loss: 387664.2188\n",
            "Epoch 144, Batch 15/38, Batch Loss: 274409.7500\n",
            "Epoch 144, Batch 16/38, Batch Loss: 267472.8438\n",
            "Epoch 144, Batch 17/38, Batch Loss: 402815.2500\n",
            "Epoch 144, Batch 18/38, Batch Loss: 263048.9688\n",
            "Epoch 144, Batch 19/38, Batch Loss: 297414.4062\n",
            "Epoch 144, Batch 20/38, Batch Loss: 294141.7188\n",
            "Epoch 144, Batch 21/38, Batch Loss: 216949.2812\n",
            "Epoch 144, Batch 22/38, Batch Loss: 305616.4062\n",
            "Epoch 144, Batch 23/38, Batch Loss: 258654.6406\n",
            "Epoch 144, Batch 24/38, Batch Loss: 301212.5312\n",
            "Epoch 144, Batch 25/38, Batch Loss: 275083.2188\n",
            "Epoch 144, Batch 26/38, Batch Loss: 410086.7812\n",
            "Epoch 144, Batch 27/38, Batch Loss: 681884.1250\n",
            "Epoch 144, Batch 28/38, Batch Loss: 321978.6875\n",
            "Epoch 144, Batch 29/38, Batch Loss: 271074.8125\n",
            "Epoch 144, Batch 30/38, Batch Loss: 246722.0000\n",
            "Epoch 144, Batch 31/38, Batch Loss: 711320.8750\n",
            "Epoch 144, Batch 32/38, Batch Loss: 298331.7500\n",
            "Epoch 144, Batch 33/38, Batch Loss: 248523.9062\n",
            "Epoch 144, Batch 34/38, Batch Loss: 359545.6250\n",
            "Epoch 144, Batch 35/38, Batch Loss: 692556.8750\n",
            "Epoch 144, Batch 36/38, Batch Loss: 343353.5312\n",
            "Epoch 144, Batch 37/38, Batch Loss: 270086.4688\n",
            "Epoch 144, Batch 38/38, Batch Loss: 399879.4688\n",
            "Batch 1/10, Batch Loss: 362668.8438\n",
            "Batch 2/10, Batch Loss: 342745.0000\n",
            "Batch 3/10, Batch Loss: 231811.8125\n",
            "Batch 4/10, Batch Loss: 299617.0625\n",
            "Batch 5/10, Batch Loss: 383457.9688\n",
            "Batch 6/10, Batch Loss: 373411.1562\n",
            "Batch 7/10, Batch Loss: 264545.5000\n",
            "Batch 8/10, Batch Loss: 279335.9688\n",
            "Batch 9/10, Batch Loss: 259777.0000\n",
            "Batch 10/10, Batch Loss: 177039.1094\n",
            "Test Error: \n",
            " Avg loss: 297440.942188, Avg error: 202231.325833 \n",
            "\n",
            "Epoch 145, Batch 1/38, Batch Loss: 734322.5625\n",
            "Epoch 145, Batch 2/38, Batch Loss: 287301.3750\n",
            "Epoch 145, Batch 3/38, Batch Loss: 686121.3125\n",
            "Epoch 145, Batch 4/38, Batch Loss: 326625.8438\n",
            "Epoch 145, Batch 5/38, Batch Loss: 273827.2500\n",
            "Epoch 145, Batch 6/38, Batch Loss: 249224.4375\n",
            "Epoch 145, Batch 7/38, Batch Loss: 267277.1875\n",
            "Epoch 145, Batch 8/38, Batch Loss: 277680.2188\n",
            "Epoch 145, Batch 9/38, Batch Loss: 710031.5625\n",
            "Epoch 145, Batch 10/38, Batch Loss: 356019.7812\n",
            "Epoch 145, Batch 11/38, Batch Loss: 338406.7188\n",
            "Epoch 145, Batch 12/38, Batch Loss: 672619.9375\n",
            "Epoch 145, Batch 13/38, Batch Loss: 335777.7188\n",
            "Epoch 145, Batch 14/38, Batch Loss: 255653.2500\n",
            "Epoch 145, Batch 15/38, Batch Loss: 246005.8125\n",
            "Epoch 145, Batch 16/38, Batch Loss: 225635.8438\n",
            "Epoch 145, Batch 17/38, Batch Loss: 288006.5625\n",
            "Epoch 145, Batch 18/38, Batch Loss: 239280.0625\n",
            "Epoch 145, Batch 19/38, Batch Loss: 255296.2969\n",
            "Epoch 145, Batch 20/38, Batch Loss: 246925.1875\n",
            "Epoch 145, Batch 21/38, Batch Loss: 678692.0000\n",
            "Epoch 145, Batch 22/38, Batch Loss: 218742.0156\n",
            "Epoch 145, Batch 23/38, Batch Loss: 398788.7500\n",
            "Epoch 145, Batch 24/38, Batch Loss: 303192.5938\n",
            "Epoch 145, Batch 25/38, Batch Loss: 386642.1562\n",
            "Epoch 145, Batch 26/38, Batch Loss: 271248.0625\n",
            "Epoch 145, Batch 27/38, Batch Loss: 303468.1875\n",
            "Epoch 145, Batch 28/38, Batch Loss: 281224.7812\n",
            "Epoch 145, Batch 29/38, Batch Loss: 295353.4375\n",
            "Epoch 145, Batch 30/38, Batch Loss: 755051.0000\n",
            "Epoch 145, Batch 31/38, Batch Loss: 311455.4688\n",
            "Epoch 145, Batch 32/38, Batch Loss: 393514.7188\n",
            "Epoch 145, Batch 33/38, Batch Loss: 692644.3750\n",
            "Epoch 145, Batch 34/38, Batch Loss: 311377.9688\n",
            "Epoch 145, Batch 35/38, Batch Loss: 294855.3750\n",
            "Epoch 145, Batch 36/38, Batch Loss: 249787.1719\n",
            "Epoch 145, Batch 37/38, Batch Loss: 397245.1875\n",
            "Epoch 145, Batch 38/38, Batch Loss: 243032.4375\n",
            "Batch 1/10, Batch Loss: 239604.3750\n",
            "Batch 2/10, Batch Loss: 272820.8438\n",
            "Batch 3/10, Batch Loss: 289092.0625\n",
            "Batch 4/10, Batch Loss: 517425.5938\n",
            "Batch 5/10, Batch Loss: 291514.4688\n",
            "Batch 6/10, Batch Loss: 260377.1719\n",
            "Batch 7/10, Batch Loss: 323438.5938\n",
            "Batch 8/10, Batch Loss: 268203.3125\n",
            "Batch 9/10, Batch Loss: 366478.4375\n",
            "Batch 10/10, Batch Loss: 258364.1719\n",
            "Test Error: \n",
            " Avg loss: 308731.903125, Avg error: 206699.344167 \n",
            "\n",
            "Epoch 146, Batch 1/38, Batch Loss: 353172.3438\n",
            "Epoch 146, Batch 2/38, Batch Loss: 957433.6875\n",
            "Epoch 146, Batch 3/38, Batch Loss: 342670.8125\n",
            "Epoch 146, Batch 4/38, Batch Loss: 685622.8125\n",
            "Epoch 146, Batch 5/38, Batch Loss: 346073.2812\n",
            "Epoch 146, Batch 6/38, Batch Loss: 330604.2812\n",
            "Epoch 146, Batch 7/38, Batch Loss: 355475.9375\n",
            "Epoch 146, Batch 8/38, Batch Loss: 267792.8438\n",
            "Epoch 146, Batch 9/38, Batch Loss: 241174.2969\n",
            "Epoch 146, Batch 10/38, Batch Loss: 354982.4688\n",
            "Epoch 146, Batch 11/38, Batch Loss: 293176.6875\n",
            "Epoch 146, Batch 12/38, Batch Loss: 235517.4375\n",
            "Epoch 146, Batch 13/38, Batch Loss: 254804.6406\n",
            "Epoch 146, Batch 14/38, Batch Loss: 288973.3750\n",
            "Epoch 146, Batch 15/38, Batch Loss: 248599.5781\n",
            "Epoch 146, Batch 16/38, Batch Loss: 287962.7188\n",
            "Epoch 146, Batch 17/38, Batch Loss: 700362.9375\n",
            "Epoch 146, Batch 18/38, Batch Loss: 265306.7812\n",
            "Epoch 146, Batch 19/38, Batch Loss: 267129.1250\n",
            "Epoch 146, Batch 20/38, Batch Loss: 306338.9062\n",
            "Epoch 146, Batch 21/38, Batch Loss: 257869.4219\n",
            "Epoch 146, Batch 22/38, Batch Loss: 301232.2500\n",
            "Epoch 146, Batch 23/38, Batch Loss: 258196.5156\n",
            "Epoch 146, Batch 24/38, Batch Loss: 232536.5625\n",
            "Epoch 146, Batch 25/38, Batch Loss: 296794.4375\n",
            "Epoch 146, Batch 26/38, Batch Loss: 917877.8750\n",
            "Epoch 146, Batch 27/38, Batch Loss: 299196.1875\n",
            "Epoch 146, Batch 28/38, Batch Loss: 264527.9062\n",
            "Epoch 146, Batch 29/38, Batch Loss: 313656.9062\n",
            "Epoch 146, Batch 30/38, Batch Loss: 370984.7500\n",
            "Epoch 146, Batch 31/38, Batch Loss: 382012.5000\n",
            "Epoch 146, Batch 32/38, Batch Loss: 287646.4688\n",
            "Epoch 146, Batch 33/38, Batch Loss: 285916.4062\n",
            "Epoch 146, Batch 34/38, Batch Loss: 356440.2812\n",
            "Epoch 146, Batch 35/38, Batch Loss: 656862.1875\n",
            "Epoch 146, Batch 36/38, Batch Loss: 284085.3125\n",
            "Epoch 146, Batch 37/38, Batch Loss: 332065.9375\n",
            "Epoch 146, Batch 38/38, Batch Loss: 257291.5000\n",
            "Batch 1/10, Batch Loss: 332334.3125\n",
            "Batch 2/10, Batch Loss: 406393.7188\n",
            "Batch 3/10, Batch Loss: 196653.0625\n",
            "Batch 4/10, Batch Loss: 255946.2812\n",
            "Batch 5/10, Batch Loss: 339982.4375\n",
            "Batch 6/10, Batch Loss: 238746.4375\n",
            "Batch 7/10, Batch Loss: 310840.8438\n",
            "Batch 8/10, Batch Loss: 257683.3281\n",
            "Batch 9/10, Batch Loss: 265209.5938\n",
            "Batch 10/10, Batch Loss: 440275.8750\n",
            "Test Error: \n",
            " Avg loss: 304406.589062, Avg error: 200258.649167 \n",
            "\n",
            "Epoch 147, Batch 1/38, Batch Loss: 254965.7656\n",
            "Epoch 147, Batch 2/38, Batch Loss: 667385.0625\n",
            "Epoch 147, Batch 3/38, Batch Loss: 257121.1250\n",
            "Epoch 147, Batch 4/38, Batch Loss: 679174.3750\n",
            "Epoch 147, Batch 5/38, Batch Loss: 285922.5000\n",
            "Epoch 147, Batch 6/38, Batch Loss: 709070.1250\n",
            "Epoch 147, Batch 7/38, Batch Loss: 321960.3125\n",
            "Epoch 147, Batch 8/38, Batch Loss: 408343.6250\n",
            "Epoch 147, Batch 9/38, Batch Loss: 743804.1250\n",
            "Epoch 147, Batch 10/38, Batch Loss: 317891.0312\n",
            "Epoch 147, Batch 11/38, Batch Loss: 227069.4062\n",
            "Epoch 147, Batch 12/38, Batch Loss: 278336.5000\n",
            "Epoch 147, Batch 13/38, Batch Loss: 366758.2500\n",
            "Epoch 147, Batch 14/38, Batch Loss: 366461.1875\n",
            "Epoch 147, Batch 15/38, Batch Loss: 263505.3125\n",
            "Epoch 147, Batch 16/38, Batch Loss: 277460.4375\n",
            "Epoch 147, Batch 17/38, Batch Loss: 286365.4375\n",
            "Epoch 147, Batch 18/38, Batch Loss: 259213.5938\n",
            "Epoch 147, Batch 19/38, Batch Loss: 274985.9375\n",
            "Epoch 147, Batch 20/38, Batch Loss: 260304.6719\n",
            "Epoch 147, Batch 21/38, Batch Loss: 222849.1406\n",
            "Epoch 147, Batch 22/38, Batch Loss: 371257.0938\n",
            "Epoch 147, Batch 23/38, Batch Loss: 227439.2969\n",
            "Epoch 147, Batch 24/38, Batch Loss: 380814.5312\n",
            "Epoch 147, Batch 25/38, Batch Loss: 282090.0938\n",
            "Epoch 147, Batch 26/38, Batch Loss: 242767.6250\n",
            "Epoch 147, Batch 27/38, Batch Loss: 309509.1875\n",
            "Epoch 147, Batch 28/38, Batch Loss: 302726.2500\n",
            "Epoch 147, Batch 29/38, Batch Loss: 243455.2031\n",
            "Epoch 147, Batch 30/38, Batch Loss: 244373.1250\n",
            "Epoch 147, Batch 31/38, Batch Loss: 922404.7500\n",
            "Epoch 147, Batch 32/38, Batch Loss: 390675.0938\n",
            "Epoch 147, Batch 33/38, Batch Loss: 279369.1875\n",
            "Epoch 147, Batch 34/38, Batch Loss: 265223.0312\n",
            "Epoch 147, Batch 35/38, Batch Loss: 300013.4688\n",
            "Epoch 147, Batch 36/38, Batch Loss: 741212.9375\n",
            "Epoch 147, Batch 37/38, Batch Loss: 277697.7812\n",
            "Epoch 147, Batch 38/38, Batch Loss: 259811.0469\n",
            "Batch 1/10, Batch Loss: 289058.3125\n",
            "Batch 2/10, Batch Loss: 308592.3125\n",
            "Batch 3/10, Batch Loss: 261115.3438\n",
            "Batch 4/10, Batch Loss: 334803.4688\n",
            "Batch 5/10, Batch Loss: 236242.6719\n",
            "Batch 6/10, Batch Loss: 222468.1406\n",
            "Batch 7/10, Batch Loss: 385398.8750\n",
            "Batch 8/10, Batch Loss: 272012.4062\n",
            "Batch 9/10, Batch Loss: 435532.7812\n",
            "Batch 10/10, Batch Loss: 263923.8125\n",
            "Test Error: \n",
            " Avg loss: 300914.812500, Avg error: 202888.888333 \n",
            "\n",
            "Epoch 148, Batch 1/38, Batch Loss: 261849.8125\n",
            "Epoch 148, Batch 2/38, Batch Loss: 298203.3438\n",
            "Epoch 148, Batch 3/38, Batch Loss: 239740.0781\n",
            "Epoch 148, Batch 4/38, Batch Loss: 289766.3125\n",
            "Epoch 148, Batch 5/38, Batch Loss: 269990.6875\n",
            "Epoch 148, Batch 6/38, Batch Loss: 698289.6875\n",
            "Epoch 148, Batch 7/38, Batch Loss: 681139.6875\n",
            "Epoch 148, Batch 8/38, Batch Loss: 323085.6250\n",
            "Epoch 148, Batch 9/38, Batch Loss: 697873.1250\n",
            "Epoch 148, Batch 10/38, Batch Loss: 240775.1562\n",
            "Epoch 148, Batch 11/38, Batch Loss: 259249.0312\n",
            "Epoch 148, Batch 12/38, Batch Loss: 277207.1875\n",
            "Epoch 148, Batch 13/38, Batch Loss: 366375.8125\n",
            "Epoch 148, Batch 14/38, Batch Loss: 233098.5625\n",
            "Epoch 148, Batch 15/38, Batch Loss: 915408.7500\n",
            "Epoch 148, Batch 16/38, Batch Loss: 285845.0312\n",
            "Epoch 148, Batch 17/38, Batch Loss: 723856.1250\n",
            "Epoch 148, Batch 18/38, Batch Loss: 256311.6875\n",
            "Epoch 148, Batch 19/38, Batch Loss: 666294.8750\n",
            "Epoch 148, Batch 20/38, Batch Loss: 351454.6875\n",
            "Epoch 148, Batch 21/38, Batch Loss: 317556.3750\n",
            "Epoch 148, Batch 22/38, Batch Loss: 298308.2188\n",
            "Epoch 148, Batch 23/38, Batch Loss: 317624.0938\n",
            "Epoch 148, Batch 24/38, Batch Loss: 242479.5938\n",
            "Epoch 148, Batch 25/38, Batch Loss: 284078.5625\n",
            "Epoch 148, Batch 26/38, Batch Loss: 293773.5312\n",
            "Epoch 148, Batch 27/38, Batch Loss: 245568.4219\n",
            "Epoch 148, Batch 28/38, Batch Loss: 281942.9375\n",
            "Epoch 148, Batch 29/38, Batch Loss: 309347.3125\n",
            "Epoch 148, Batch 30/38, Batch Loss: 281925.3125\n",
            "Epoch 148, Batch 31/38, Batch Loss: 370558.1875\n",
            "Epoch 148, Batch 32/38, Batch Loss: 258001.9844\n",
            "Epoch 148, Batch 33/38, Batch Loss: 339276.2500\n",
            "Epoch 148, Batch 34/38, Batch Loss: 350284.5000\n",
            "Epoch 148, Batch 35/38, Batch Loss: 376908.3438\n",
            "Epoch 148, Batch 36/38, Batch Loss: 315734.1562\n",
            "Epoch 148, Batch 37/38, Batch Loss: 334809.9062\n",
            "Epoch 148, Batch 38/38, Batch Loss: 433588.7812\n",
            "Batch 1/10, Batch Loss: 246913.3438\n",
            "Batch 2/10, Batch Loss: 253743.3281\n",
            "Batch 3/10, Batch Loss: 321930.1250\n",
            "Batch 4/10, Batch Loss: 270195.0625\n",
            "Batch 5/10, Batch Loss: 270247.0625\n",
            "Batch 6/10, Batch Loss: 369950.9062\n",
            "Batch 7/10, Batch Loss: 304750.8438\n",
            "Batch 8/10, Batch Loss: 258264.6875\n",
            "Batch 9/10, Batch Loss: 331026.2188\n",
            "Batch 10/10, Batch Loss: 476387.2812\n",
            "Test Error: \n",
            " Avg loss: 310340.885937, Avg error: 202966.023333 \n",
            "\n",
            "Epoch 149, Batch 1/38, Batch Loss: 724053.6250\n",
            "Epoch 149, Batch 2/38, Batch Loss: 370487.5312\n",
            "Epoch 149, Batch 3/38, Batch Loss: 296375.6250\n",
            "Epoch 149, Batch 4/38, Batch Loss: 919425.3125\n",
            "Epoch 149, Batch 5/38, Batch Loss: 248773.2500\n",
            "Epoch 149, Batch 6/38, Batch Loss: 434868.2500\n",
            "Epoch 149, Batch 7/38, Batch Loss: 260091.1562\n",
            "Epoch 149, Batch 8/38, Batch Loss: 206436.5312\n",
            "Epoch 149, Batch 9/38, Batch Loss: 225841.5938\n",
            "Epoch 149, Batch 10/38, Batch Loss: 684549.3125\n",
            "Epoch 149, Batch 11/38, Batch Loss: 260334.9844\n",
            "Epoch 149, Batch 12/38, Batch Loss: 329506.6562\n",
            "Epoch 149, Batch 13/38, Batch Loss: 339419.5938\n",
            "Epoch 149, Batch 14/38, Batch Loss: 296887.8125\n",
            "Epoch 149, Batch 15/38, Batch Loss: 276447.7500\n",
            "Epoch 149, Batch 16/38, Batch Loss: 330543.0312\n",
            "Epoch 149, Batch 17/38, Batch Loss: 282177.8125\n",
            "Epoch 149, Batch 18/38, Batch Loss: 302691.7188\n",
            "Epoch 149, Batch 19/38, Batch Loss: 447540.5312\n",
            "Epoch 149, Batch 20/38, Batch Loss: 277356.4375\n",
            "Epoch 149, Batch 21/38, Batch Loss: 262351.6250\n",
            "Epoch 149, Batch 22/38, Batch Loss: 443601.3438\n",
            "Epoch 149, Batch 23/38, Batch Loss: 251015.3438\n",
            "Epoch 149, Batch 24/38, Batch Loss: 249745.7500\n",
            "Epoch 149, Batch 25/38, Batch Loss: 220666.8438\n",
            "Epoch 149, Batch 26/38, Batch Loss: 253501.3594\n",
            "Epoch 149, Batch 27/38, Batch Loss: 329529.5938\n",
            "Epoch 149, Batch 28/38, Batch Loss: 256685.5156\n",
            "Epoch 149, Batch 29/38, Batch Loss: 352233.2188\n",
            "Epoch 149, Batch 30/38, Batch Loss: 924090.7500\n",
            "Epoch 149, Batch 31/38, Batch Loss: 286347.1562\n",
            "Epoch 149, Batch 32/38, Batch Loss: 257541.2969\n",
            "Epoch 149, Batch 33/38, Batch Loss: 353395.8750\n",
            "Epoch 149, Batch 34/38, Batch Loss: 362728.1250\n",
            "Epoch 149, Batch 35/38, Batch Loss: 321752.9062\n",
            "Epoch 149, Batch 36/38, Batch Loss: 308699.0938\n",
            "Epoch 149, Batch 37/38, Batch Loss: 656221.1875\n",
            "Epoch 149, Batch 38/38, Batch Loss: 324116.4688\n",
            "Batch 1/10, Batch Loss: 349527.2812\n",
            "Batch 2/10, Batch Loss: 388677.6562\n",
            "Batch 3/10, Batch Loss: 278439.1875\n",
            "Batch 4/10, Batch Loss: 341469.0938\n",
            "Batch 5/10, Batch Loss: 381434.7500\n",
            "Batch 6/10, Batch Loss: 247688.9219\n",
            "Batch 7/10, Batch Loss: 184900.4062\n",
            "Batch 8/10, Batch Loss: 329513.9375\n",
            "Batch 9/10, Batch Loss: 284028.5000\n",
            "Batch 10/10, Batch Loss: 213658.8281\n",
            "Test Error: \n",
            " Avg loss: 299933.856250, Avg error: 201062.705417 \n",
            "\n",
            "Epoch 150, Batch 1/38, Batch Loss: 299409.5625\n",
            "Epoch 150, Batch 2/38, Batch Loss: 313743.1562\n",
            "Epoch 150, Batch 3/38, Batch Loss: 674293.2500\n",
            "Epoch 150, Batch 4/38, Batch Loss: 235202.2500\n",
            "Epoch 150, Batch 5/38, Batch Loss: 213688.4375\n",
            "Epoch 150, Batch 6/38, Batch Loss: 326831.5000\n",
            "Epoch 150, Batch 7/38, Batch Loss: 298093.0312\n",
            "Epoch 150, Batch 8/38, Batch Loss: 338961.1562\n",
            "Epoch 150, Batch 9/38, Batch Loss: 642542.0000\n",
            "Epoch 150, Batch 10/38, Batch Loss: 710572.1250\n",
            "Epoch 150, Batch 11/38, Batch Loss: 670763.4375\n",
            "Epoch 150, Batch 12/38, Batch Loss: 328268.5312\n",
            "Epoch 150, Batch 13/38, Batch Loss: 269230.3438\n",
            "Epoch 150, Batch 14/38, Batch Loss: 333958.5312\n",
            "Epoch 150, Batch 15/38, Batch Loss: 249558.8438\n",
            "Epoch 150, Batch 16/38, Batch Loss: 330422.4688\n",
            "Epoch 150, Batch 17/38, Batch Loss: 433563.7812\n",
            "Epoch 150, Batch 18/38, Batch Loss: 286114.2500\n",
            "Epoch 150, Batch 19/38, Batch Loss: 282940.1562\n",
            "Epoch 150, Batch 20/38, Batch Loss: 293258.6875\n",
            "Epoch 150, Batch 21/38, Batch Loss: 690645.2500\n",
            "Epoch 150, Batch 22/38, Batch Loss: 280332.9375\n",
            "Epoch 150, Batch 23/38, Batch Loss: 225647.8281\n",
            "Epoch 150, Batch 24/38, Batch Loss: 315648.1250\n",
            "Epoch 150, Batch 25/38, Batch Loss: 749845.9375\n",
            "Epoch 150, Batch 26/38, Batch Loss: 259065.3906\n",
            "Epoch 150, Batch 27/38, Batch Loss: 284085.8125\n",
            "Epoch 150, Batch 28/38, Batch Loss: 702000.1250\n",
            "Epoch 150, Batch 29/38, Batch Loss: 388280.5938\n",
            "Epoch 150, Batch 30/38, Batch Loss: 312969.7812\n",
            "Epoch 150, Batch 31/38, Batch Loss: 482177.0000\n",
            "Epoch 150, Batch 32/38, Batch Loss: 219350.8281\n",
            "Epoch 150, Batch 33/38, Batch Loss: 276057.1562\n",
            "Epoch 150, Batch 34/38, Batch Loss: 269632.5938\n",
            "Epoch 150, Batch 35/38, Batch Loss: 284003.4062\n",
            "Epoch 150, Batch 36/38, Batch Loss: 243647.1250\n",
            "Epoch 150, Batch 37/38, Batch Loss: 344334.4688\n",
            "Epoch 150, Batch 38/38, Batch Loss: 200916.3594\n",
            "Batch 1/10, Batch Loss: 242023.5625\n",
            "Batch 2/10, Batch Loss: 323172.8750\n",
            "Batch 3/10, Batch Loss: 239424.8438\n",
            "Batch 4/10, Batch Loss: 307891.5938\n",
            "Batch 5/10, Batch Loss: 442938.0938\n",
            "Batch 6/10, Batch Loss: 267049.9688\n",
            "Batch 7/10, Batch Loss: 323534.4375\n",
            "Batch 8/10, Batch Loss: 409662.7812\n",
            "Batch 9/10, Batch Loss: 406967.1875\n",
            "Batch 10/10, Batch Loss: 204248.5625\n",
            "Test Error: \n",
            " Avg loss: 316691.390625, Avg error: 208375.980000 \n",
            "\n",
            "2024-09-14 20:35:37.978750 Epoch 150, Average Training loss 370001.4794407895\n",
            "Epoch 151, Batch 1/38, Batch Loss: 232746.8438\n",
            "Epoch 151, Batch 2/38, Batch Loss: 369474.7188\n",
            "Epoch 151, Batch 3/38, Batch Loss: 268786.6875\n",
            "Epoch 151, Batch 4/38, Batch Loss: 272104.6875\n",
            "Epoch 151, Batch 5/38, Batch Loss: 675528.3750\n",
            "Epoch 151, Batch 6/38, Batch Loss: 272813.9062\n",
            "Epoch 151, Batch 7/38, Batch Loss: 264890.3438\n",
            "Epoch 151, Batch 8/38, Batch Loss: 398494.9375\n",
            "Epoch 151, Batch 9/38, Batch Loss: 240191.0469\n",
            "Epoch 151, Batch 10/38, Batch Loss: 341858.2500\n",
            "Epoch 151, Batch 11/38, Batch Loss: 283288.5000\n",
            "Epoch 151, Batch 12/38, Batch Loss: 331164.4375\n",
            "Epoch 151, Batch 13/38, Batch Loss: 268499.6250\n",
            "Epoch 151, Batch 14/38, Batch Loss: 690110.0625\n",
            "Epoch 151, Batch 15/38, Batch Loss: 283485.9375\n",
            "Epoch 151, Batch 16/38, Batch Loss: 311313.9062\n",
            "Epoch 151, Batch 17/38, Batch Loss: 723600.4375\n",
            "Epoch 151, Batch 18/38, Batch Loss: 287522.9062\n",
            "Epoch 151, Batch 19/38, Batch Loss: 260918.0156\n",
            "Epoch 151, Batch 20/38, Batch Loss: 710297.0625\n",
            "Epoch 151, Batch 21/38, Batch Loss: 317434.4062\n",
            "Epoch 151, Batch 22/38, Batch Loss: 300148.2188\n",
            "Epoch 151, Batch 23/38, Batch Loss: 678643.6250\n",
            "Epoch 151, Batch 24/38, Batch Loss: 264858.5312\n",
            "Epoch 151, Batch 25/38, Batch Loss: 239410.2188\n",
            "Epoch 151, Batch 26/38, Batch Loss: 206915.1250\n",
            "Epoch 151, Batch 27/38, Batch Loss: 369555.8125\n",
            "Epoch 151, Batch 28/38, Batch Loss: 753772.8125\n",
            "Epoch 151, Batch 29/38, Batch Loss: 653450.6250\n",
            "Epoch 151, Batch 30/38, Batch Loss: 321258.5625\n",
            "Epoch 151, Batch 31/38, Batch Loss: 208056.6719\n",
            "Epoch 151, Batch 32/38, Batch Loss: 279172.9375\n",
            "Epoch 151, Batch 33/38, Batch Loss: 382495.6250\n",
            "Epoch 151, Batch 34/38, Batch Loss: 242207.6250\n",
            "Epoch 151, Batch 35/38, Batch Loss: 369306.5312\n",
            "Epoch 151, Batch 36/38, Batch Loss: 271650.4688\n",
            "Epoch 151, Batch 37/38, Batch Loss: 311658.9062\n",
            "Epoch 151, Batch 38/38, Batch Loss: 304805.1562\n",
            "Batch 1/10, Batch Loss: 233472.8906\n",
            "Batch 2/10, Batch Loss: 417573.0938\n",
            "Batch 3/10, Batch Loss: 343194.1875\n",
            "Batch 4/10, Batch Loss: 281185.2500\n",
            "Batch 5/10, Batch Loss: 269797.2188\n",
            "Batch 6/10, Batch Loss: 340058.0312\n",
            "Batch 7/10, Batch Loss: 255389.2812\n",
            "Batch 8/10, Batch Loss: 241822.0781\n",
            "Batch 9/10, Batch Loss: 261943.0469\n",
            "Batch 10/10, Batch Loss: 574597.7500\n",
            "Test Error: \n",
            " Avg loss: 321903.282813, Avg error: 201974.651667 \n",
            "\n",
            "Epoch 152, Batch 1/38, Batch Loss: 244416.7969\n",
            "Epoch 152, Batch 2/38, Batch Loss: 665883.8750\n",
            "Epoch 152, Batch 3/38, Batch Loss: 708286.5625\n",
            "Epoch 152, Batch 4/38, Batch Loss: 281604.0312\n",
            "Epoch 152, Batch 5/38, Batch Loss: 251497.6094\n",
            "Epoch 152, Batch 6/38, Batch Loss: 260797.1875\n",
            "Epoch 152, Batch 7/38, Batch Loss: 336510.8125\n",
            "Epoch 152, Batch 8/38, Batch Loss: 727157.1875\n",
            "Epoch 152, Batch 9/38, Batch Loss: 313390.8125\n",
            "Epoch 152, Batch 10/38, Batch Loss: 306270.7500\n",
            "Epoch 152, Batch 11/38, Batch Loss: 235876.5469\n",
            "Epoch 152, Batch 12/38, Batch Loss: 281832.4688\n",
            "Epoch 152, Batch 13/38, Batch Loss: 690098.4375\n",
            "Epoch 152, Batch 14/38, Batch Loss: 676173.5625\n",
            "Epoch 152, Batch 15/38, Batch Loss: 302689.1250\n",
            "Epoch 152, Batch 16/38, Batch Loss: 313107.0312\n",
            "Epoch 152, Batch 17/38, Batch Loss: 199580.4062\n",
            "Epoch 152, Batch 18/38, Batch Loss: 250350.3750\n",
            "Epoch 152, Batch 19/38, Batch Loss: 309441.8750\n",
            "Epoch 152, Batch 20/38, Batch Loss: 234463.8125\n",
            "Epoch 152, Batch 21/38, Batch Loss: 207105.5781\n",
            "Epoch 152, Batch 22/38, Batch Loss: 246568.7500\n",
            "Epoch 152, Batch 23/38, Batch Loss: 321261.1562\n",
            "Epoch 152, Batch 24/38, Batch Loss: 456634.0938\n",
            "Epoch 152, Batch 25/38, Batch Loss: 713791.2500\n",
            "Epoch 152, Batch 26/38, Batch Loss: 289621.6562\n",
            "Epoch 152, Batch 27/38, Batch Loss: 229049.7969\n",
            "Epoch 152, Batch 28/38, Batch Loss: 296814.0938\n",
            "Epoch 152, Batch 29/38, Batch Loss: 430710.8125\n",
            "Epoch 152, Batch 30/38, Batch Loss: 254581.2188\n",
            "Epoch 152, Batch 31/38, Batch Loss: 235809.7031\n",
            "Epoch 152, Batch 32/38, Batch Loss: 310761.9688\n",
            "Epoch 152, Batch 33/38, Batch Loss: 415036.4375\n",
            "Epoch 152, Batch 34/38, Batch Loss: 221631.2969\n",
            "Epoch 152, Batch 35/38, Batch Loss: 674623.1250\n",
            "Epoch 152, Batch 36/38, Batch Loss: 376922.0312\n",
            "Epoch 152, Batch 37/38, Batch Loss: 294091.9062\n",
            "Epoch 152, Batch 38/38, Batch Loss: 181673.0312\n",
            "Batch 1/10, Batch Loss: 379362.2812\n",
            "Batch 2/10, Batch Loss: 316509.0312\n",
            "Batch 3/10, Batch Loss: 247743.2031\n",
            "Batch 4/10, Batch Loss: 237716.0156\n",
            "Batch 5/10, Batch Loss: 346443.2500\n",
            "Batch 6/10, Batch Loss: 246053.7031\n",
            "Batch 7/10, Batch Loss: 206652.5156\n",
            "Batch 8/10, Batch Loss: 304211.9688\n",
            "Batch 9/10, Batch Loss: 419397.1250\n",
            "Batch 10/10, Batch Loss: 244094.6875\n",
            "Test Error: \n",
            " Avg loss: 294818.378125, Avg error: 199212.183333 \n",
            "\n",
            "Epoch 153, Batch 1/38, Batch Loss: 209448.2031\n",
            "Epoch 153, Batch 2/38, Batch Loss: 222416.4688\n",
            "Epoch 153, Batch 3/38, Batch Loss: 265407.0000\n",
            "Epoch 153, Batch 4/38, Batch Loss: 272720.8125\n",
            "Epoch 153, Batch 5/38, Batch Loss: 290933.4062\n",
            "Epoch 153, Batch 6/38, Batch Loss: 660223.5625\n",
            "Epoch 153, Batch 7/38, Batch Loss: 304980.2500\n",
            "Epoch 153, Batch 8/38, Batch Loss: 294805.6250\n",
            "Epoch 153, Batch 9/38, Batch Loss: 316649.5938\n",
            "Epoch 153, Batch 10/38, Batch Loss: 313110.9062\n",
            "Epoch 153, Batch 11/38, Batch Loss: 263677.1250\n",
            "Epoch 153, Batch 12/38, Batch Loss: 361328.2812\n",
            "Epoch 153, Batch 13/38, Batch Loss: 394677.8125\n",
            "Epoch 153, Batch 14/38, Batch Loss: 230458.2031\n",
            "Epoch 153, Batch 15/38, Batch Loss: 403802.6562\n",
            "Epoch 153, Batch 16/38, Batch Loss: 386367.9062\n",
            "Epoch 153, Batch 17/38, Batch Loss: 286774.0000\n",
            "Epoch 153, Batch 18/38, Batch Loss: 642203.2500\n",
            "Epoch 153, Batch 19/38, Batch Loss: 698261.1250\n",
            "Epoch 153, Batch 20/38, Batch Loss: 372533.7500\n",
            "Epoch 153, Batch 21/38, Batch Loss: 243475.7031\n",
            "Epoch 153, Batch 22/38, Batch Loss: 311537.5000\n",
            "Epoch 153, Batch 23/38, Batch Loss: 311380.1250\n",
            "Epoch 153, Batch 24/38, Batch Loss: 271469.8750\n",
            "Epoch 153, Batch 25/38, Batch Loss: 281532.8750\n",
            "Epoch 153, Batch 26/38, Batch Loss: 713208.5000\n",
            "Epoch 153, Batch 27/38, Batch Loss: 396793.0625\n",
            "Epoch 153, Batch 28/38, Batch Loss: 338423.6875\n",
            "Epoch 153, Batch 29/38, Batch Loss: 323300.9375\n",
            "Epoch 153, Batch 30/38, Batch Loss: 359872.5625\n",
            "Epoch 153, Batch 31/38, Batch Loss: 728667.7500\n",
            "Epoch 153, Batch 32/38, Batch Loss: 307391.2812\n",
            "Epoch 153, Batch 33/38, Batch Loss: 334054.4688\n",
            "Epoch 153, Batch 34/38, Batch Loss: 220655.9531\n",
            "Epoch 153, Batch 35/38, Batch Loss: 242826.3281\n",
            "Epoch 153, Batch 36/38, Batch Loss: 318024.9062\n",
            "Epoch 153, Batch 37/38, Batch Loss: 977304.6875\n",
            "Epoch 153, Batch 38/38, Batch Loss: 328910.4688\n",
            "Batch 1/10, Batch Loss: 283886.1250\n",
            "Batch 2/10, Batch Loss: 359556.3750\n",
            "Batch 3/10, Batch Loss: 346657.0625\n",
            "Batch 4/10, Batch Loss: 319085.8750\n",
            "Batch 5/10, Batch Loss: 235097.5625\n",
            "Batch 6/10, Batch Loss: 265641.8125\n",
            "Batch 7/10, Batch Loss: 324137.7812\n",
            "Batch 8/10, Batch Loss: 225541.4375\n",
            "Batch 9/10, Batch Loss: 393132.1562\n",
            "Batch 10/10, Batch Loss: 233289.1094\n",
            "Test Error: \n",
            " Avg loss: 298602.529687, Avg error: 202039.131667 \n",
            "\n",
            "Epoch 154, Batch 1/38, Batch Loss: 221716.0469\n",
            "Epoch 154, Batch 2/38, Batch Loss: 287507.1562\n",
            "Epoch 154, Batch 3/38, Batch Loss: 284323.3750\n",
            "Epoch 154, Batch 4/38, Batch Loss: 234479.5781\n",
            "Epoch 154, Batch 5/38, Batch Loss: 913590.0625\n",
            "Epoch 154, Batch 6/38, Batch Loss: 265207.6875\n",
            "Epoch 154, Batch 7/38, Batch Loss: 686445.1250\n",
            "Epoch 154, Batch 8/38, Batch Loss: 328950.0312\n",
            "Epoch 154, Batch 9/38, Batch Loss: 229501.4688\n",
            "Epoch 154, Batch 10/38, Batch Loss: 338433.7812\n",
            "Epoch 154, Batch 11/38, Batch Loss: 287390.2812\n",
            "Epoch 154, Batch 12/38, Batch Loss: 393637.9062\n",
            "Epoch 154, Batch 13/38, Batch Loss: 281112.4062\n",
            "Epoch 154, Batch 14/38, Batch Loss: 231789.8438\n",
            "Epoch 154, Batch 15/38, Batch Loss: 327512.3750\n",
            "Epoch 154, Batch 16/38, Batch Loss: 371499.5000\n",
            "Epoch 154, Batch 17/38, Batch Loss: 320303.5625\n",
            "Epoch 154, Batch 18/38, Batch Loss: 216735.1094\n",
            "Epoch 154, Batch 19/38, Batch Loss: 289848.8750\n",
            "Epoch 154, Batch 20/38, Batch Loss: 286956.9688\n",
            "Epoch 154, Batch 21/38, Batch Loss: 934261.0625\n",
            "Epoch 154, Batch 22/38, Batch Loss: 256690.1406\n",
            "Epoch 154, Batch 23/38, Batch Loss: 284342.1875\n",
            "Epoch 154, Batch 24/38, Batch Loss: 645791.8750\n",
            "Epoch 154, Batch 25/38, Batch Loss: 681544.8750\n",
            "Epoch 154, Batch 26/38, Batch Loss: 290636.7188\n",
            "Epoch 154, Batch 27/38, Batch Loss: 316315.3438\n",
            "Epoch 154, Batch 28/38, Batch Loss: 336547.6250\n",
            "Epoch 154, Batch 29/38, Batch Loss: 188846.9531\n",
            "Epoch 154, Batch 30/38, Batch Loss: 264123.1875\n",
            "Epoch 154, Batch 31/38, Batch Loss: 319071.2812\n",
            "Epoch 154, Batch 32/38, Batch Loss: 266463.1562\n",
            "Epoch 154, Batch 33/38, Batch Loss: 464772.4688\n",
            "Epoch 154, Batch 34/38, Batch Loss: 339146.8438\n",
            "Epoch 154, Batch 35/38, Batch Loss: 284093.2812\n",
            "Epoch 154, Batch 36/38, Batch Loss: 261418.9375\n",
            "Epoch 154, Batch 37/38, Batch Loss: 285568.5625\n",
            "Epoch 154, Batch 38/38, Batch Loss: 295401.9062\n",
            "Batch 1/10, Batch Loss: 395196.8125\n",
            "Batch 2/10, Batch Loss: 229520.2812\n",
            "Batch 3/10, Batch Loss: 334433.2812\n",
            "Batch 4/10, Batch Loss: 296112.7188\n",
            "Batch 5/10, Batch Loss: 353688.0312\n",
            "Batch 6/10, Batch Loss: 229179.6094\n",
            "Batch 7/10, Batch Loss: 317637.2500\n",
            "Batch 8/10, Batch Loss: 299506.7500\n",
            "Batch 9/10, Batch Loss: 354826.3750\n",
            "Batch 10/10, Batch Loss: 267195.4062\n",
            "Test Error: \n",
            " Avg loss: 307729.651562, Avg error: 200013.212500 \n",
            "\n",
            "Epoch 155, Batch 1/38, Batch Loss: 663905.6875\n",
            "Epoch 155, Batch 2/38, Batch Loss: 378807.3438\n",
            "Epoch 155, Batch 3/38, Batch Loss: 336838.7812\n",
            "Epoch 155, Batch 4/38, Batch Loss: 273134.3125\n",
            "Epoch 155, Batch 5/38, Batch Loss: 235665.3281\n",
            "Epoch 155, Batch 6/38, Batch Loss: 729056.6250\n",
            "Epoch 155, Batch 7/38, Batch Loss: 299359.2812\n",
            "Epoch 155, Batch 8/38, Batch Loss: 288994.3750\n",
            "Epoch 155, Batch 9/38, Batch Loss: 228284.3438\n",
            "Epoch 155, Batch 10/38, Batch Loss: 703995.5000\n",
            "Epoch 155, Batch 11/38, Batch Loss: 253082.9688\n",
            "Epoch 155, Batch 12/38, Batch Loss: 941676.4375\n",
            "Epoch 155, Batch 13/38, Batch Loss: 293644.9375\n",
            "Epoch 155, Batch 14/38, Batch Loss: 266240.2188\n",
            "Epoch 155, Batch 15/38, Batch Loss: 291874.2500\n",
            "Epoch 155, Batch 16/38, Batch Loss: 369679.3438\n",
            "Epoch 155, Batch 17/38, Batch Loss: 289565.8750\n",
            "Epoch 155, Batch 18/38, Batch Loss: 262222.2500\n",
            "Epoch 155, Batch 19/38, Batch Loss: 409323.1875\n",
            "Epoch 155, Batch 20/38, Batch Loss: 381933.8750\n",
            "Epoch 155, Batch 21/38, Batch Loss: 410843.0312\n",
            "Epoch 155, Batch 22/38, Batch Loss: 332517.6875\n",
            "Epoch 155, Batch 23/38, Batch Loss: 667859.3750\n",
            "Epoch 155, Batch 24/38, Batch Loss: 240390.7344\n",
            "Epoch 155, Batch 25/38, Batch Loss: 260368.2031\n",
            "Epoch 155, Batch 26/38, Batch Loss: 295301.5938\n",
            "Epoch 155, Batch 27/38, Batch Loss: 283495.1250\n",
            "Epoch 155, Batch 28/38, Batch Loss: 277440.0625\n",
            "Epoch 155, Batch 29/38, Batch Loss: 248670.1406\n",
            "Epoch 155, Batch 30/38, Batch Loss: 233898.3438\n",
            "Epoch 155, Batch 31/38, Batch Loss: 324137.8125\n",
            "Epoch 155, Batch 32/38, Batch Loss: 237301.9688\n",
            "Epoch 155, Batch 33/38, Batch Loss: 280128.2188\n",
            "Epoch 155, Batch 34/38, Batch Loss: 269817.6250\n",
            "Epoch 155, Batch 35/38, Batch Loss: 664626.1875\n",
            "Epoch 155, Batch 36/38, Batch Loss: 259347.1719\n",
            "Epoch 155, Batch 37/38, Batch Loss: 333468.2812\n",
            "Epoch 155, Batch 38/38, Batch Loss: 231150.6406\n",
            "Batch 1/10, Batch Loss: 247144.3281\n",
            "Batch 2/10, Batch Loss: 371795.0625\n",
            "Batch 3/10, Batch Loss: 321173.8438\n",
            "Batch 4/10, Batch Loss: 374296.1562\n",
            "Batch 5/10, Batch Loss: 310349.5938\n",
            "Batch 6/10, Batch Loss: 332521.8750\n",
            "Batch 7/10, Batch Loss: 327897.0000\n",
            "Batch 8/10, Batch Loss: 265616.6250\n",
            "Batch 9/10, Batch Loss: 296085.9688\n",
            "Batch 10/10, Batch Loss: 308321.2812\n",
            "Test Error: \n",
            " Avg loss: 315520.173438, Avg error: 200563.353333 \n",
            "\n",
            "Epoch 156, Batch 1/38, Batch Loss: 214188.5781\n",
            "Epoch 156, Batch 2/38, Batch Loss: 193478.7500\n",
            "Epoch 156, Batch 3/38, Batch Loss: 699439.8750\n",
            "Epoch 156, Batch 4/38, Batch Loss: 339446.2812\n",
            "Epoch 156, Batch 5/38, Batch Loss: 237152.1562\n",
            "Epoch 156, Batch 6/38, Batch Loss: 301928.9062\n",
            "Epoch 156, Batch 7/38, Batch Loss: 320565.5938\n",
            "Epoch 156, Batch 8/38, Batch Loss: 262468.9062\n",
            "Epoch 156, Batch 9/38, Batch Loss: 320209.7188\n",
            "Epoch 156, Batch 10/38, Batch Loss: 250328.2812\n",
            "Epoch 156, Batch 11/38, Batch Loss: 300740.0000\n",
            "Epoch 156, Batch 12/38, Batch Loss: 417357.6250\n",
            "Epoch 156, Batch 13/38, Batch Loss: 290563.4688\n",
            "Epoch 156, Batch 14/38, Batch Loss: 357771.5000\n",
            "Epoch 156, Batch 15/38, Batch Loss: 267558.4688\n",
            "Epoch 156, Batch 16/38, Batch Loss: 387688.8750\n",
            "Epoch 156, Batch 17/38, Batch Loss: 681606.2500\n",
            "Epoch 156, Batch 18/38, Batch Loss: 318675.2500\n",
            "Epoch 156, Batch 19/38, Batch Loss: 252606.2969\n",
            "Epoch 156, Batch 20/38, Batch Loss: 694395.8750\n",
            "Epoch 156, Batch 21/38, Batch Loss: 282635.0312\n",
            "Epoch 156, Batch 22/38, Batch Loss: 362492.8750\n",
            "Epoch 156, Batch 23/38, Batch Loss: 334535.8125\n",
            "Epoch 156, Batch 24/38, Batch Loss: 690026.9375\n",
            "Epoch 156, Batch 25/38, Batch Loss: 921755.6250\n",
            "Epoch 156, Batch 26/38, Batch Loss: 320127.3438\n",
            "Epoch 156, Batch 27/38, Batch Loss: 266096.3125\n",
            "Epoch 156, Batch 28/38, Batch Loss: 275407.4375\n",
            "Epoch 156, Batch 29/38, Batch Loss: 294032.7500\n",
            "Epoch 156, Batch 30/38, Batch Loss: 292159.5000\n",
            "Epoch 156, Batch 31/38, Batch Loss: 253538.1094\n",
            "Epoch 156, Batch 32/38, Batch Loss: 333911.9062\n",
            "Epoch 156, Batch 33/38, Batch Loss: 335529.3750\n",
            "Epoch 156, Batch 34/38, Batch Loss: 301297.2188\n",
            "Epoch 156, Batch 35/38, Batch Loss: 298449.9375\n",
            "Epoch 156, Batch 36/38, Batch Loss: 680685.4375\n",
            "Epoch 156, Batch 37/38, Batch Loss: 280973.0312\n",
            "Epoch 156, Batch 38/38, Batch Loss: 218772.4375\n",
            "Batch 1/10, Batch Loss: 327863.9688\n",
            "Batch 2/10, Batch Loss: 204822.5156\n",
            "Batch 3/10, Batch Loss: 302390.3438\n",
            "Batch 4/10, Batch Loss: 213846.7812\n",
            "Batch 5/10, Batch Loss: 293041.5000\n",
            "Batch 6/10, Batch Loss: 302129.7188\n",
            "Batch 7/10, Batch Loss: 281859.7500\n",
            "Batch 8/10, Batch Loss: 480425.7812\n",
            "Batch 9/10, Batch Loss: 235989.5156\n",
            "Batch 10/10, Batch Loss: 400836.3125\n",
            "Test Error: \n",
            " Avg loss: 304320.618750, Avg error: 198405.146667 \n",
            "\n",
            "Epoch 157, Batch 1/38, Batch Loss: 236587.1250\n",
            "Epoch 157, Batch 2/38, Batch Loss: 708099.3750\n",
            "Epoch 157, Batch 3/38, Batch Loss: 287692.3438\n",
            "Epoch 157, Batch 4/38, Batch Loss: 270665.7500\n",
            "Epoch 157, Batch 5/38, Batch Loss: 384321.2500\n",
            "Epoch 157, Batch 6/38, Batch Loss: 675573.6250\n",
            "Epoch 157, Batch 7/38, Batch Loss: 297377.8750\n",
            "Epoch 157, Batch 8/38, Batch Loss: 291821.2500\n",
            "Epoch 157, Batch 9/38, Batch Loss: 301176.8750\n",
            "Epoch 157, Batch 10/38, Batch Loss: 307230.0938\n",
            "Epoch 157, Batch 11/38, Batch Loss: 338714.8438\n",
            "Epoch 157, Batch 12/38, Batch Loss: 261702.0625\n",
            "Epoch 157, Batch 13/38, Batch Loss: 240748.5938\n",
            "Epoch 157, Batch 14/38, Batch Loss: 274547.4375\n",
            "Epoch 157, Batch 15/38, Batch Loss: 296572.0312\n",
            "Epoch 157, Batch 16/38, Batch Loss: 341341.1562\n",
            "Epoch 157, Batch 17/38, Batch Loss: 663084.8125\n",
            "Epoch 157, Batch 18/38, Batch Loss: 277424.6562\n",
            "Epoch 157, Batch 19/38, Batch Loss: 227179.0625\n",
            "Epoch 157, Batch 20/38, Batch Loss: 260034.5469\n",
            "Epoch 157, Batch 21/38, Batch Loss: 291671.0625\n",
            "Epoch 157, Batch 22/38, Batch Loss: 401075.8125\n",
            "Epoch 157, Batch 23/38, Batch Loss: 301994.4062\n",
            "Epoch 157, Batch 24/38, Batch Loss: 313135.6250\n",
            "Epoch 157, Batch 25/38, Batch Loss: 260434.6094\n",
            "Epoch 157, Batch 26/38, Batch Loss: 683655.8125\n",
            "Epoch 157, Batch 27/38, Batch Loss: 668932.5625\n",
            "Epoch 157, Batch 28/38, Batch Loss: 303468.3125\n",
            "Epoch 157, Batch 29/38, Batch Loss: 435364.7812\n",
            "Epoch 157, Batch 30/38, Batch Loss: 260229.8281\n",
            "Epoch 157, Batch 31/38, Batch Loss: 708883.1250\n",
            "Epoch 157, Batch 32/38, Batch Loss: 264569.0312\n",
            "Epoch 157, Batch 33/38, Batch Loss: 329487.8438\n",
            "Epoch 157, Batch 34/38, Batch Loss: 427089.9688\n",
            "Epoch 157, Batch 35/38, Batch Loss: 664690.5625\n",
            "Epoch 157, Batch 36/38, Batch Loss: 330303.9688\n",
            "Epoch 157, Batch 37/38, Batch Loss: 270193.2500\n",
            "Epoch 157, Batch 38/38, Batch Loss: 237209.4844\n",
            "Batch 1/10, Batch Loss: 387256.3125\n",
            "Batch 2/10, Batch Loss: 357487.9688\n",
            "Batch 3/10, Batch Loss: 339103.6562\n",
            "Batch 4/10, Batch Loss: 272740.7188\n",
            "Batch 5/10, Batch Loss: 307317.9375\n",
            "Batch 6/10, Batch Loss: 321612.1250\n",
            "Batch 7/10, Batch Loss: 293028.8125\n",
            "Batch 8/10, Batch Loss: 296692.8125\n",
            "Batch 9/10, Batch Loss: 306371.3438\n",
            "Batch 10/10, Batch Loss: 186026.5312\n",
            "Test Error: \n",
            " Avg loss: 306763.821875, Avg error: 200252.815000 \n",
            "\n",
            "Epoch 158, Batch 1/38, Batch Loss: 341267.7812\n",
            "Epoch 158, Batch 2/38, Batch Loss: 227197.6562\n",
            "Epoch 158, Batch 3/38, Batch Loss: 181945.2500\n",
            "Epoch 158, Batch 4/38, Batch Loss: 226142.8750\n",
            "Epoch 158, Batch 5/38, Batch Loss: 245763.8281\n",
            "Epoch 158, Batch 6/38, Batch Loss: 252657.4688\n",
            "Epoch 158, Batch 7/38, Batch Loss: 251927.5312\n",
            "Epoch 158, Batch 8/38, Batch Loss: 358526.1875\n",
            "Epoch 158, Batch 9/38, Batch Loss: 308038.6875\n",
            "Epoch 158, Batch 10/38, Batch Loss: 244689.1250\n",
            "Epoch 158, Batch 11/38, Batch Loss: 417392.4688\n",
            "Epoch 158, Batch 12/38, Batch Loss: 304111.7812\n",
            "Epoch 158, Batch 13/38, Batch Loss: 305386.4688\n",
            "Epoch 158, Batch 14/38, Batch Loss: 947663.1875\n",
            "Epoch 158, Batch 15/38, Batch Loss: 289877.5312\n",
            "Epoch 158, Batch 16/38, Batch Loss: 360460.9375\n",
            "Epoch 158, Batch 17/38, Batch Loss: 286536.6875\n",
            "Epoch 158, Batch 18/38, Batch Loss: 680937.3750\n",
            "Epoch 158, Batch 19/38, Batch Loss: 390886.7500\n",
            "Epoch 158, Batch 20/38, Batch Loss: 303647.0625\n",
            "Epoch 158, Batch 21/38, Batch Loss: 242992.5781\n",
            "Epoch 158, Batch 22/38, Batch Loss: 300584.9688\n",
            "Epoch 158, Batch 23/38, Batch Loss: 708498.3125\n",
            "Epoch 158, Batch 24/38, Batch Loss: 304637.8438\n",
            "Epoch 158, Batch 25/38, Batch Loss: 303780.4062\n",
            "Epoch 158, Batch 26/38, Batch Loss: 915819.4375\n",
            "Epoch 158, Batch 27/38, Batch Loss: 273649.7500\n",
            "Epoch 158, Batch 28/38, Batch Loss: 275912.3438\n",
            "Epoch 158, Batch 29/38, Batch Loss: 189517.0000\n",
            "Epoch 158, Batch 30/38, Batch Loss: 261724.3594\n",
            "Epoch 158, Batch 31/38, Batch Loss: 306757.8438\n",
            "Epoch 158, Batch 32/38, Batch Loss: 270621.9062\n",
            "Epoch 158, Batch 33/38, Batch Loss: 241545.4688\n",
            "Epoch 158, Batch 34/38, Batch Loss: 304307.7188\n",
            "Epoch 158, Batch 35/38, Batch Loss: 686892.6250\n",
            "Epoch 158, Batch 36/38, Batch Loss: 430390.0312\n",
            "Epoch 158, Batch 37/38, Batch Loss: 300345.4375\n",
            "Epoch 158, Batch 38/38, Batch Loss: 247705.0625\n",
            "Batch 1/10, Batch Loss: 248025.4688\n",
            "Batch 2/10, Batch Loss: 378415.8750\n",
            "Batch 3/10, Batch Loss: 289691.0312\n",
            "Batch 4/10, Batch Loss: 286850.5000\n",
            "Batch 5/10, Batch Loss: 316913.8750\n",
            "Batch 6/10, Batch Loss: 235843.9531\n",
            "Batch 7/10, Batch Loss: 339150.4062\n",
            "Batch 8/10, Batch Loss: 281310.5625\n",
            "Batch 9/10, Batch Loss: 380768.8125\n",
            "Batch 10/10, Batch Loss: 238581.3281\n",
            "Test Error: \n",
            " Avg loss: 299555.181250, Avg error: 196438.655000 \n",
            "\n",
            "Epoch 159, Batch 1/38, Batch Loss: 216778.5938\n",
            "Epoch 159, Batch 2/38, Batch Loss: 406433.0000\n",
            "Epoch 159, Batch 3/38, Batch Loss: 293243.3438\n",
            "Epoch 159, Batch 4/38, Batch Loss: 218432.4531\n",
            "Epoch 159, Batch 5/38, Batch Loss: 331576.6250\n",
            "Epoch 159, Batch 6/38, Batch Loss: 291835.0625\n",
            "Epoch 159, Batch 7/38, Batch Loss: 330675.5938\n",
            "Epoch 159, Batch 8/38, Batch Loss: 920409.1250\n",
            "Epoch 159, Batch 9/38, Batch Loss: 217857.0469\n",
            "Epoch 159, Batch 10/38, Batch Loss: 696685.9375\n",
            "Epoch 159, Batch 11/38, Batch Loss: 730555.3125\n",
            "Epoch 159, Batch 12/38, Batch Loss: 342733.2812\n",
            "Epoch 159, Batch 13/38, Batch Loss: 291069.0625\n",
            "Epoch 159, Batch 14/38, Batch Loss: 388768.0000\n",
            "Epoch 159, Batch 15/38, Batch Loss: 283020.5312\n",
            "Epoch 159, Batch 16/38, Batch Loss: 348761.1875\n",
            "Epoch 159, Batch 17/38, Batch Loss: 258024.2344\n",
            "Epoch 159, Batch 18/38, Batch Loss: 232862.7344\n",
            "Epoch 159, Batch 19/38, Batch Loss: 678843.8125\n",
            "Epoch 159, Batch 20/38, Batch Loss: 295147.8750\n",
            "Epoch 159, Batch 21/38, Batch Loss: 275576.0312\n",
            "Epoch 159, Batch 22/38, Batch Loss: 309267.9688\n",
            "Epoch 159, Batch 23/38, Batch Loss: 260780.7500\n",
            "Epoch 159, Batch 24/38, Batch Loss: 234938.6094\n",
            "Epoch 159, Batch 25/38, Batch Loss: 456959.0938\n",
            "Epoch 159, Batch 26/38, Batch Loss: 165416.4688\n",
            "Epoch 159, Batch 27/38, Batch Loss: 305528.0312\n",
            "Epoch 159, Batch 28/38, Batch Loss: 278829.1875\n",
            "Epoch 159, Batch 29/38, Batch Loss: 335366.4688\n",
            "Epoch 159, Batch 30/38, Batch Loss: 273041.0938\n",
            "Epoch 159, Batch 31/38, Batch Loss: 278762.9688\n",
            "Epoch 159, Batch 32/38, Batch Loss: 729504.8750\n",
            "Epoch 159, Batch 33/38, Batch Loss: 302120.0938\n",
            "Epoch 159, Batch 34/38, Batch Loss: 240291.2656\n",
            "Epoch 159, Batch 35/38, Batch Loss: 678922.6250\n",
            "Epoch 159, Batch 36/38, Batch Loss: 300204.1250\n",
            "Epoch 159, Batch 37/38, Batch Loss: 294801.7500\n",
            "Epoch 159, Batch 38/38, Batch Loss: 173159.8281\n",
            "Batch 1/10, Batch Loss: 368994.3750\n",
            "Batch 2/10, Batch Loss: 364140.6562\n",
            "Batch 3/10, Batch Loss: 279556.9375\n",
            "Batch 4/10, Batch Loss: 335359.9375\n",
            "Batch 5/10, Batch Loss: 278198.0938\n",
            "Batch 6/10, Batch Loss: 376054.9688\n",
            "Batch 7/10, Batch Loss: 264813.8750\n",
            "Batch 8/10, Batch Loss: 365180.5938\n",
            "Batch 9/10, Batch Loss: 300129.7500\n",
            "Batch 10/10, Batch Loss: 279368.5625\n",
            "Test Error: \n",
            " Avg loss: 321179.775000, Avg error: 206355.810000 \n",
            "\n",
            "Epoch 160, Batch 1/38, Batch Loss: 308197.0312\n",
            "Epoch 160, Batch 2/38, Batch Loss: 219830.7969\n",
            "Epoch 160, Batch 3/38, Batch Loss: 725750.4375\n",
            "Epoch 160, Batch 4/38, Batch Loss: 691178.0625\n",
            "Epoch 160, Batch 5/38, Batch Loss: 242806.8281\n",
            "Epoch 160, Batch 6/38, Batch Loss: 670512.5625\n",
            "Epoch 160, Batch 7/38, Batch Loss: 329732.3125\n",
            "Epoch 160, Batch 8/38, Batch Loss: 315559.9062\n",
            "Epoch 160, Batch 9/38, Batch Loss: 248135.2344\n",
            "Epoch 160, Batch 10/38, Batch Loss: 254406.6406\n",
            "Epoch 160, Batch 11/38, Batch Loss: 255007.0781\n",
            "Epoch 160, Batch 12/38, Batch Loss: 315709.8750\n",
            "Epoch 160, Batch 13/38, Batch Loss: 275741.1250\n",
            "Epoch 160, Batch 14/38, Batch Loss: 342267.5625\n",
            "Epoch 160, Batch 15/38, Batch Loss: 729679.6875\n",
            "Epoch 160, Batch 16/38, Batch Loss: 315296.9688\n",
            "Epoch 160, Batch 17/38, Batch Loss: 659798.5000\n",
            "Epoch 160, Batch 18/38, Batch Loss: 280057.3125\n",
            "Epoch 160, Batch 19/38, Batch Loss: 314265.0625\n",
            "Epoch 160, Batch 20/38, Batch Loss: 236082.6250\n",
            "Epoch 160, Batch 21/38, Batch Loss: 302496.2500\n",
            "Epoch 160, Batch 22/38, Batch Loss: 479468.0312\n",
            "Epoch 160, Batch 23/38, Batch Loss: 662266.5625\n",
            "Epoch 160, Batch 24/38, Batch Loss: 281018.3438\n",
            "Epoch 160, Batch 25/38, Batch Loss: 259734.3125\n",
            "Epoch 160, Batch 26/38, Batch Loss: 333604.5938\n",
            "Epoch 160, Batch 27/38, Batch Loss: 220120.0156\n",
            "Epoch 160, Batch 28/38, Batch Loss: 442265.8750\n",
            "Epoch 160, Batch 29/38, Batch Loss: 245378.4844\n",
            "Epoch 160, Batch 30/38, Batch Loss: 237178.7188\n",
            "Epoch 160, Batch 31/38, Batch Loss: 320291.8750\n",
            "Epoch 160, Batch 32/38, Batch Loss: 314534.7188\n",
            "Epoch 160, Batch 33/38, Batch Loss: 266547.5000\n",
            "Epoch 160, Batch 34/38, Batch Loss: 209110.2656\n",
            "Epoch 160, Batch 35/38, Batch Loss: 191457.0312\n",
            "Epoch 160, Batch 36/38, Batch Loss: 708448.4375\n",
            "Epoch 160, Batch 37/38, Batch Loss: 309494.2812\n",
            "Epoch 160, Batch 38/38, Batch Loss: 324790.9688\n",
            "Batch 1/10, Batch Loss: 380151.0000\n",
            "Batch 2/10, Batch Loss: 239600.1094\n",
            "Batch 3/10, Batch Loss: 388208.5625\n",
            "Batch 4/10, Batch Loss: 216459.2656\n",
            "Batch 5/10, Batch Loss: 337213.3750\n",
            "Batch 6/10, Batch Loss: 412588.6250\n",
            "Batch 7/10, Batch Loss: 296093.8125\n",
            "Batch 8/10, Batch Loss: 352306.7812\n",
            "Batch 9/10, Batch Loss: 436126.2500\n",
            "Batch 10/10, Batch Loss: 260460.6406\n",
            "Test Error: \n",
            " Avg loss: 331920.842187, Avg error: 215417.965833 \n",
            "\n",
            "2024-09-14 20:42:20.082317 Epoch 160, Average Training loss 364163.7335526316\n",
            "Epoch 161, Batch 1/38, Batch Loss: 373736.4375\n",
            "Epoch 161, Batch 2/38, Batch Loss: 333691.0625\n",
            "Epoch 161, Batch 3/38, Batch Loss: 327293.5938\n",
            "Epoch 161, Batch 4/38, Batch Loss: 384124.9688\n",
            "Epoch 161, Batch 5/38, Batch Loss: 258777.6562\n",
            "Epoch 161, Batch 6/38, Batch Loss: 250679.6094\n",
            "Epoch 161, Batch 7/38, Batch Loss: 308912.2188\n",
            "Epoch 161, Batch 8/38, Batch Loss: 401441.2188\n",
            "Epoch 161, Batch 9/38, Batch Loss: 357261.0625\n",
            "Epoch 161, Batch 10/38, Batch Loss: 253859.6875\n",
            "Epoch 161, Batch 11/38, Batch Loss: 690625.0625\n",
            "Epoch 161, Batch 12/38, Batch Loss: 276738.1562\n",
            "Epoch 161, Batch 13/38, Batch Loss: 331561.7500\n",
            "Epoch 161, Batch 14/38, Batch Loss: 256623.5938\n",
            "Epoch 161, Batch 15/38, Batch Loss: 254193.3594\n",
            "Epoch 161, Batch 16/38, Batch Loss: 673251.1875\n",
            "Epoch 161, Batch 17/38, Batch Loss: 267968.7188\n",
            "Epoch 161, Batch 18/38, Batch Loss: 704553.0625\n",
            "Epoch 161, Batch 19/38, Batch Loss: 330377.5312\n",
            "Epoch 161, Batch 20/38, Batch Loss: 276708.6250\n",
            "Epoch 161, Batch 21/38, Batch Loss: 307797.4375\n",
            "Epoch 161, Batch 22/38, Batch Loss: 696661.1875\n",
            "Epoch 161, Batch 23/38, Batch Loss: 291845.5000\n",
            "Epoch 161, Batch 24/38, Batch Loss: 219667.1562\n",
            "Epoch 161, Batch 25/38, Batch Loss: 265790.7500\n",
            "Epoch 161, Batch 26/38, Batch Loss: 292241.5312\n",
            "Epoch 161, Batch 27/38, Batch Loss: 265441.0625\n",
            "Epoch 161, Batch 28/38, Batch Loss: 742371.0625\n",
            "Epoch 161, Batch 29/38, Batch Loss: 222223.6094\n",
            "Epoch 161, Batch 30/38, Batch Loss: 250288.4062\n",
            "Epoch 161, Batch 31/38, Batch Loss: 360915.8750\n",
            "Epoch 161, Batch 32/38, Batch Loss: 249385.3125\n",
            "Epoch 161, Batch 33/38, Batch Loss: 263565.6875\n",
            "Epoch 161, Batch 34/38, Batch Loss: 310168.5312\n",
            "Epoch 161, Batch 35/38, Batch Loss: 702447.5625\n",
            "Epoch 161, Batch 36/38, Batch Loss: 727632.6250\n",
            "Epoch 161, Batch 37/38, Batch Loss: 268790.0938\n",
            "Epoch 161, Batch 38/38, Batch Loss: 155558.4688\n",
            "Batch 1/10, Batch Loss: 350747.3125\n",
            "Batch 2/10, Batch Loss: 224188.9375\n",
            "Batch 3/10, Batch Loss: 361677.6562\n",
            "Batch 4/10, Batch Loss: 344098.8438\n",
            "Batch 5/10, Batch Loss: 359332.4688\n",
            "Batch 6/10, Batch Loss: 304577.8750\n",
            "Batch 7/10, Batch Loss: 312404.4062\n",
            "Batch 8/10, Batch Loss: 208360.2188\n",
            "Batch 9/10, Batch Loss: 274219.6250\n",
            "Batch 10/10, Batch Loss: 255818.7812\n",
            "Test Error: \n",
            " Avg loss: 299542.612500, Avg error: 194980.065833 \n",
            "\n",
            "Epoch 162, Batch 1/38, Batch Loss: 244889.7344\n",
            "Epoch 162, Batch 2/38, Batch Loss: 406977.4688\n",
            "Epoch 162, Batch 3/38, Batch Loss: 297002.8750\n",
            "Epoch 162, Batch 4/38, Batch Loss: 251163.4844\n",
            "Epoch 162, Batch 5/38, Batch Loss: 242255.0781\n",
            "Epoch 162, Batch 6/38, Batch Loss: 684078.0625\n",
            "Epoch 162, Batch 7/38, Batch Loss: 222484.6719\n",
            "Epoch 162, Batch 8/38, Batch Loss: 362914.0938\n",
            "Epoch 162, Batch 9/38, Batch Loss: 344874.8125\n",
            "Epoch 162, Batch 10/38, Batch Loss: 353771.5625\n",
            "Epoch 162, Batch 11/38, Batch Loss: 691338.4375\n",
            "Epoch 162, Batch 12/38, Batch Loss: 719373.7500\n",
            "Epoch 162, Batch 13/38, Batch Loss: 249053.0312\n",
            "Epoch 162, Batch 14/38, Batch Loss: 268871.7188\n",
            "Epoch 162, Batch 15/38, Batch Loss: 262080.3750\n",
            "Epoch 162, Batch 16/38, Batch Loss: 261336.9062\n",
            "Epoch 162, Batch 17/38, Batch Loss: 246900.9219\n",
            "Epoch 162, Batch 18/38, Batch Loss: 309090.1875\n",
            "Epoch 162, Batch 19/38, Batch Loss: 675794.3750\n",
            "Epoch 162, Batch 20/38, Batch Loss: 270053.4062\n",
            "Epoch 162, Batch 21/38, Batch Loss: 245612.3750\n",
            "Epoch 162, Batch 22/38, Batch Loss: 250286.0312\n",
            "Epoch 162, Batch 23/38, Batch Loss: 241876.6719\n",
            "Epoch 162, Batch 24/38, Batch Loss: 473470.5938\n",
            "Epoch 162, Batch 25/38, Batch Loss: 296937.0000\n",
            "Epoch 162, Batch 26/38, Batch Loss: 325057.1250\n",
            "Epoch 162, Batch 27/38, Batch Loss: 234001.0469\n",
            "Epoch 162, Batch 28/38, Batch Loss: 721955.5000\n",
            "Epoch 162, Batch 29/38, Batch Loss: 313077.8438\n",
            "Epoch 162, Batch 30/38, Batch Loss: 706025.0625\n",
            "Epoch 162, Batch 31/38, Batch Loss: 281642.0312\n",
            "Epoch 162, Batch 32/38, Batch Loss: 242420.7188\n",
            "Epoch 162, Batch 33/38, Batch Loss: 349150.5625\n",
            "Epoch 162, Batch 34/38, Batch Loss: 304701.6250\n",
            "Epoch 162, Batch 35/38, Batch Loss: 326887.8125\n",
            "Epoch 162, Batch 36/38, Batch Loss: 282314.4062\n",
            "Epoch 162, Batch 37/38, Batch Loss: 654378.7500\n",
            "Epoch 162, Batch 38/38, Batch Loss: 353503.0000\n",
            "Batch 1/10, Batch Loss: 258211.2188\n",
            "Batch 2/10, Batch Loss: 270594.4375\n",
            "Batch 3/10, Batch Loss: 253655.2188\n",
            "Batch 4/10, Batch Loss: 329740.1562\n",
            "Batch 5/10, Batch Loss: 427943.0938\n",
            "Batch 6/10, Batch Loss: 295827.9375\n",
            "Batch 7/10, Batch Loss: 442103.8438\n",
            "Batch 8/10, Batch Loss: 236191.9531\n",
            "Batch 9/10, Batch Loss: 218907.5781\n",
            "Batch 10/10, Batch Loss: 198437.5000\n",
            "Test Error: \n",
            " Avg loss: 293161.293750, Avg error: 198008.280417 \n",
            "\n",
            "Epoch 163, Batch 1/38, Batch Loss: 225797.9375\n",
            "Epoch 163, Batch 2/38, Batch Loss: 699842.3125\n",
            "Epoch 163, Batch 3/38, Batch Loss: 282496.0938\n",
            "Epoch 163, Batch 4/38, Batch Loss: 269690.9688\n",
            "Epoch 163, Batch 5/38, Batch Loss: 699123.2500\n",
            "Epoch 163, Batch 6/38, Batch Loss: 287103.6250\n",
            "Epoch 163, Batch 7/38, Batch Loss: 300473.3125\n",
            "Epoch 163, Batch 8/38, Batch Loss: 700610.6250\n",
            "Epoch 163, Batch 9/38, Batch Loss: 260501.2500\n",
            "Epoch 163, Batch 10/38, Batch Loss: 382800.9062\n",
            "Epoch 163, Batch 11/38, Batch Loss: 253261.1719\n",
            "Epoch 163, Batch 12/38, Batch Loss: 248443.9688\n",
            "Epoch 163, Batch 13/38, Batch Loss: 202562.7969\n",
            "Epoch 163, Batch 14/38, Batch Loss: 271148.4375\n",
            "Epoch 163, Batch 15/38, Batch Loss: 675959.8125\n",
            "Epoch 163, Batch 16/38, Batch Loss: 436988.1562\n",
            "Epoch 163, Batch 17/38, Batch Loss: 745537.3125\n",
            "Epoch 163, Batch 18/38, Batch Loss: 243523.4531\n",
            "Epoch 163, Batch 19/38, Batch Loss: 292424.5312\n",
            "Epoch 163, Batch 20/38, Batch Loss: 671243.9375\n",
            "Epoch 163, Batch 21/38, Batch Loss: 285318.6562\n",
            "Epoch 163, Batch 22/38, Batch Loss: 314356.2812\n",
            "Epoch 163, Batch 23/38, Batch Loss: 266678.4375\n",
            "Epoch 163, Batch 24/38, Batch Loss: 304376.5938\n",
            "Epoch 163, Batch 25/38, Batch Loss: 254300.7031\n",
            "Epoch 163, Batch 26/38, Batch Loss: 195252.7656\n",
            "Epoch 163, Batch 27/38, Batch Loss: 369025.6250\n",
            "Epoch 163, Batch 28/38, Batch Loss: 342520.5625\n",
            "Epoch 163, Batch 29/38, Batch Loss: 467731.7812\n",
            "Epoch 163, Batch 30/38, Batch Loss: 753289.0000\n",
            "Epoch 163, Batch 31/38, Batch Loss: 405733.5625\n",
            "Epoch 163, Batch 32/38, Batch Loss: 240173.7969\n",
            "Epoch 163, Batch 33/38, Batch Loss: 257011.9062\n",
            "Epoch 163, Batch 34/38, Batch Loss: 286293.4688\n",
            "Epoch 163, Batch 35/38, Batch Loss: 316751.1250\n",
            "Epoch 163, Batch 36/38, Batch Loss: 232648.9219\n",
            "Epoch 163, Batch 37/38, Batch Loss: 254429.9062\n",
            "Epoch 163, Batch 38/38, Batch Loss: 325417.0625\n",
            "Batch 1/10, Batch Loss: 283403.8438\n",
            "Batch 2/10, Batch Loss: 348193.7188\n",
            "Batch 3/10, Batch Loss: 305194.0000\n",
            "Batch 4/10, Batch Loss: 446703.5625\n",
            "Batch 5/10, Batch Loss: 213353.0625\n",
            "Batch 6/10, Batch Loss: 282407.3750\n",
            "Batch 7/10, Batch Loss: 277064.1562\n",
            "Batch 8/10, Batch Loss: 280072.4688\n",
            "Batch 9/10, Batch Loss: 264352.7188\n",
            "Batch 10/10, Batch Loss: 245928.1875\n",
            "Test Error: \n",
            " Avg loss: 294667.309375, Avg error: 197985.334167 \n",
            "\n",
            "Epoch 164, Batch 1/38, Batch Loss: 676490.9375\n",
            "Epoch 164, Batch 2/38, Batch Loss: 291139.9375\n",
            "Epoch 164, Batch 3/38, Batch Loss: 317114.2812\n",
            "Epoch 164, Batch 4/38, Batch Loss: 663478.4375\n",
            "Epoch 164, Batch 5/38, Batch Loss: 249966.1875\n",
            "Epoch 164, Batch 6/38, Batch Loss: 441405.3125\n",
            "Epoch 164, Batch 7/38, Batch Loss: 342838.3438\n",
            "Epoch 164, Batch 8/38, Batch Loss: 237120.2969\n",
            "Epoch 164, Batch 9/38, Batch Loss: 193419.0781\n",
            "Epoch 164, Batch 10/38, Batch Loss: 288710.0938\n",
            "Epoch 164, Batch 11/38, Batch Loss: 284230.4375\n",
            "Epoch 164, Batch 12/38, Batch Loss: 262717.5312\n",
            "Epoch 164, Batch 13/38, Batch Loss: 436571.5312\n",
            "Epoch 164, Batch 14/38, Batch Loss: 680558.1875\n",
            "Epoch 164, Batch 15/38, Batch Loss: 317595.3125\n",
            "Epoch 164, Batch 16/38, Batch Loss: 267227.7812\n",
            "Epoch 164, Batch 17/38, Batch Loss: 236349.6094\n",
            "Epoch 164, Batch 18/38, Batch Loss: 241541.9219\n",
            "Epoch 164, Batch 19/38, Batch Loss: 679216.0000\n",
            "Epoch 164, Batch 20/38, Batch Loss: 277260.3125\n",
            "Epoch 164, Batch 21/38, Batch Loss: 335507.7188\n",
            "Epoch 164, Batch 22/38, Batch Loss: 332612.8750\n",
            "Epoch 164, Batch 23/38, Batch Loss: 665635.0000\n",
            "Epoch 164, Batch 24/38, Batch Loss: 710359.3750\n",
            "Epoch 164, Batch 25/38, Batch Loss: 284775.5938\n",
            "Epoch 164, Batch 26/38, Batch Loss: 261723.1875\n",
            "Epoch 164, Batch 27/38, Batch Loss: 232061.8438\n",
            "Epoch 164, Batch 28/38, Batch Loss: 681227.0625\n",
            "Epoch 164, Batch 29/38, Batch Loss: 267795.6562\n",
            "Epoch 164, Batch 30/38, Batch Loss: 300703.5000\n",
            "Epoch 164, Batch 31/38, Batch Loss: 208497.3281\n",
            "Epoch 164, Batch 32/38, Batch Loss: 258854.1094\n",
            "Epoch 164, Batch 33/38, Batch Loss: 300488.2812\n",
            "Epoch 164, Batch 34/38, Batch Loss: 346089.1250\n",
            "Epoch 164, Batch 35/38, Batch Loss: 410141.4062\n",
            "Epoch 164, Batch 36/38, Batch Loss: 263281.3438\n",
            "Epoch 164, Batch 37/38, Batch Loss: 279656.5625\n",
            "Epoch 164, Batch 38/38, Batch Loss: 251986.8906\n",
            "Batch 1/10, Batch Loss: 299215.5000\n",
            "Batch 2/10, Batch Loss: 272669.8750\n",
            "Batch 3/10, Batch Loss: 345407.3750\n",
            "Batch 4/10, Batch Loss: 263976.0312\n",
            "Batch 5/10, Batch Loss: 294520.5312\n",
            "Batch 6/10, Batch Loss: 262518.3750\n",
            "Batch 7/10, Batch Loss: 380525.4688\n",
            "Batch 8/10, Batch Loss: 179298.7656\n",
            "Batch 9/10, Batch Loss: 378939.5312\n",
            "Batch 10/10, Batch Loss: 460908.0625\n",
            "Test Error: \n",
            " Avg loss: 313797.951562, Avg error: 197233.247500 \n",
            "\n",
            "Epoch 165, Batch 1/38, Batch Loss: 677763.5000\n",
            "Epoch 165, Batch 2/38, Batch Loss: 315622.4688\n",
            "Epoch 165, Batch 3/38, Batch Loss: 292117.9688\n",
            "Epoch 165, Batch 4/38, Batch Loss: 245872.2812\n",
            "Epoch 165, Batch 5/38, Batch Loss: 248556.8125\n",
            "Epoch 165, Batch 6/38, Batch Loss: 959587.6250\n",
            "Epoch 165, Batch 7/38, Batch Loss: 529849.7500\n",
            "Epoch 165, Batch 8/38, Batch Loss: 689947.5000\n",
            "Epoch 165, Batch 9/38, Batch Loss: 234488.1250\n",
            "Epoch 165, Batch 10/38, Batch Loss: 667430.7500\n",
            "Epoch 165, Batch 11/38, Batch Loss: 271004.8438\n",
            "Epoch 165, Batch 12/38, Batch Loss: 289313.9062\n",
            "Epoch 165, Batch 13/38, Batch Loss: 250158.2656\n",
            "Epoch 165, Batch 14/38, Batch Loss: 326772.2500\n",
            "Epoch 165, Batch 15/38, Batch Loss: 294645.7500\n",
            "Epoch 165, Batch 16/38, Batch Loss: 267420.7500\n",
            "Epoch 165, Batch 17/38, Batch Loss: 735242.5625\n",
            "Epoch 165, Batch 18/38, Batch Loss: 230389.0000\n",
            "Epoch 165, Batch 19/38, Batch Loss: 362983.8750\n",
            "Epoch 165, Batch 20/38, Batch Loss: 230316.9531\n",
            "Epoch 165, Batch 21/38, Batch Loss: 304535.5938\n",
            "Epoch 165, Batch 22/38, Batch Loss: 355456.0000\n",
            "Epoch 165, Batch 23/38, Batch Loss: 228508.8281\n",
            "Epoch 165, Batch 24/38, Batch Loss: 259494.9062\n",
            "Epoch 165, Batch 25/38, Batch Loss: 279507.8750\n",
            "Epoch 165, Batch 26/38, Batch Loss: 291118.7188\n",
            "Epoch 165, Batch 27/38, Batch Loss: 277586.2188\n",
            "Epoch 165, Batch 28/38, Batch Loss: 285169.0312\n",
            "Epoch 165, Batch 29/38, Batch Loss: 368702.2188\n",
            "Epoch 165, Batch 30/38, Batch Loss: 323910.7188\n",
            "Epoch 165, Batch 31/38, Batch Loss: 339578.0312\n",
            "Epoch 165, Batch 32/38, Batch Loss: 263621.2188\n",
            "Epoch 165, Batch 33/38, Batch Loss: 246348.5938\n",
            "Epoch 165, Batch 34/38, Batch Loss: 259673.0781\n",
            "Epoch 165, Batch 35/38, Batch Loss: 306598.1875\n",
            "Epoch 165, Batch 36/38, Batch Loss: 303079.1875\n",
            "Epoch 165, Batch 37/38, Batch Loss: 693868.1875\n",
            "Epoch 165, Batch 38/38, Batch Loss: 195642.6250\n",
            "Batch 1/10, Batch Loss: 336126.1875\n",
            "Batch 2/10, Batch Loss: 292113.5938\n",
            "Batch 3/10, Batch Loss: 317278.6562\n",
            "Batch 4/10, Batch Loss: 409707.1562\n",
            "Batch 5/10, Batch Loss: 245016.7656\n",
            "Batch 6/10, Batch Loss: 226514.2500\n",
            "Batch 7/10, Batch Loss: 255601.0469\n",
            "Batch 8/10, Batch Loss: 270497.5312\n",
            "Batch 9/10, Batch Loss: 406535.5938\n",
            "Batch 10/10, Batch Loss: 320918.7812\n",
            "Test Error: \n",
            " Avg loss: 308030.956250, Avg error: 197321.733333 \n",
            "\n",
            "Epoch 166, Batch 1/38, Batch Loss: 348733.5000\n",
            "Epoch 166, Batch 2/38, Batch Loss: 250337.9375\n",
            "Epoch 166, Batch 3/38, Batch Loss: 273729.3438\n",
            "Epoch 166, Batch 4/38, Batch Loss: 260627.3125\n",
            "Epoch 166, Batch 5/38, Batch Loss: 324841.3125\n",
            "Epoch 166, Batch 6/38, Batch Loss: 938861.1250\n",
            "Epoch 166, Batch 7/38, Batch Loss: 263666.4688\n",
            "Epoch 166, Batch 8/38, Batch Loss: 326802.5938\n",
            "Epoch 166, Batch 9/38, Batch Loss: 176242.5469\n",
            "Epoch 166, Batch 10/38, Batch Loss: 678790.1250\n",
            "Epoch 166, Batch 11/38, Batch Loss: 192793.8281\n",
            "Epoch 166, Batch 12/38, Batch Loss: 309073.7188\n",
            "Epoch 166, Batch 13/38, Batch Loss: 444789.1250\n",
            "Epoch 166, Batch 14/38, Batch Loss: 330427.1562\n",
            "Epoch 166, Batch 15/38, Batch Loss: 679545.5000\n",
            "Epoch 166, Batch 16/38, Batch Loss: 312384.7500\n",
            "Epoch 166, Batch 17/38, Batch Loss: 316564.2500\n",
            "Epoch 166, Batch 18/38, Batch Loss: 315211.4688\n",
            "Epoch 166, Batch 19/38, Batch Loss: 251016.5000\n",
            "Epoch 166, Batch 20/38, Batch Loss: 304183.5312\n",
            "Epoch 166, Batch 21/38, Batch Loss: 306138.1250\n",
            "Epoch 166, Batch 22/38, Batch Loss: 333960.1875\n",
            "Epoch 166, Batch 23/38, Batch Loss: 731024.7500\n",
            "Epoch 166, Batch 24/38, Batch Loss: 250868.3438\n",
            "Epoch 166, Batch 25/38, Batch Loss: 275431.2500\n",
            "Epoch 166, Batch 26/38, Batch Loss: 296595.1250\n",
            "Epoch 166, Batch 27/38, Batch Loss: 236005.5625\n",
            "Epoch 166, Batch 28/38, Batch Loss: 691588.5000\n",
            "Epoch 166, Batch 29/38, Batch Loss: 718857.7500\n",
            "Epoch 166, Batch 30/38, Batch Loss: 309577.1562\n",
            "Epoch 166, Batch 31/38, Batch Loss: 286699.7188\n",
            "Epoch 166, Batch 32/38, Batch Loss: 406702.2500\n",
            "Epoch 166, Batch 33/38, Batch Loss: 295961.5625\n",
            "Epoch 166, Batch 34/38, Batch Loss: 294815.2500\n",
            "Epoch 166, Batch 35/38, Batch Loss: 275786.4375\n",
            "Epoch 166, Batch 36/38, Batch Loss: 294056.4688\n",
            "Epoch 166, Batch 37/38, Batch Loss: 237688.3438\n",
            "Epoch 166, Batch 38/38, Batch Loss: 305632.2812\n",
            "Batch 1/10, Batch Loss: 247718.7969\n",
            "Batch 2/10, Batch Loss: 367320.1875\n",
            "Batch 3/10, Batch Loss: 234223.5625\n",
            "Batch 4/10, Batch Loss: 351277.2812\n",
            "Batch 5/10, Batch Loss: 291302.9062\n",
            "Batch 6/10, Batch Loss: 357884.5938\n",
            "Batch 7/10, Batch Loss: 254254.6406\n",
            "Batch 8/10, Batch Loss: 219644.1719\n",
            "Batch 9/10, Batch Loss: 357137.5938\n",
            "Batch 10/10, Batch Loss: 513962.1562\n",
            "Test Error: \n",
            " Avg loss: 319472.589062, Avg error: 196243.720833 \n",
            "\n",
            "Epoch 167, Batch 1/38, Batch Loss: 724961.5000\n",
            "Epoch 167, Batch 2/38, Batch Loss: 351842.5938\n",
            "Epoch 167, Batch 3/38, Batch Loss: 293714.4062\n",
            "Epoch 167, Batch 4/38, Batch Loss: 214369.9688\n",
            "Epoch 167, Batch 5/38, Batch Loss: 298370.2188\n",
            "Epoch 167, Batch 6/38, Batch Loss: 263824.0625\n",
            "Epoch 167, Batch 7/38, Batch Loss: 862127.5000\n",
            "Epoch 167, Batch 8/38, Batch Loss: 219255.0156\n",
            "Epoch 167, Batch 9/38, Batch Loss: 315000.8438\n",
            "Epoch 167, Batch 10/38, Batch Loss: 319997.1250\n",
            "Epoch 167, Batch 11/38, Batch Loss: 301099.2500\n",
            "Epoch 167, Batch 12/38, Batch Loss: 303093.1250\n",
            "Epoch 167, Batch 13/38, Batch Loss: 209628.7188\n",
            "Epoch 167, Batch 14/38, Batch Loss: 383909.4062\n",
            "Epoch 167, Batch 15/38, Batch Loss: 277114.6875\n",
            "Epoch 167, Batch 16/38, Batch Loss: 288867.2500\n",
            "Epoch 167, Batch 17/38, Batch Loss: 279000.5000\n",
            "Epoch 167, Batch 18/38, Batch Loss: 309213.3438\n",
            "Epoch 167, Batch 19/38, Batch Loss: 276004.4062\n",
            "Epoch 167, Batch 20/38, Batch Loss: 457403.2812\n",
            "Epoch 167, Batch 21/38, Batch Loss: 281728.3125\n",
            "Epoch 167, Batch 22/38, Batch Loss: 236353.9688\n",
            "Epoch 167, Batch 23/38, Batch Loss: 263581.8750\n",
            "Epoch 167, Batch 24/38, Batch Loss: 753613.5625\n",
            "Epoch 167, Batch 25/38, Batch Loss: 291667.8125\n",
            "Epoch 167, Batch 26/38, Batch Loss: 210825.7812\n",
            "Epoch 167, Batch 27/38, Batch Loss: 271696.8125\n",
            "Epoch 167, Batch 28/38, Batch Loss: 256829.0000\n",
            "Epoch 167, Batch 29/38, Batch Loss: 933149.8750\n",
            "Epoch 167, Batch 30/38, Batch Loss: 227039.2500\n",
            "Epoch 167, Batch 31/38, Batch Loss: 241182.3438\n",
            "Epoch 167, Batch 32/38, Batch Loss: 261121.5156\n",
            "Epoch 167, Batch 33/38, Batch Loss: 281154.6562\n",
            "Epoch 167, Batch 34/38, Batch Loss: 276635.2188\n",
            "Epoch 167, Batch 35/38, Batch Loss: 686154.7500\n",
            "Epoch 167, Batch 36/38, Batch Loss: 312285.2188\n",
            "Epoch 167, Batch 37/38, Batch Loss: 470734.5625\n",
            "Epoch 167, Batch 38/38, Batch Loss: 317535.1875\n",
            "Batch 1/10, Batch Loss: 256144.2344\n",
            "Batch 2/10, Batch Loss: 356562.1875\n",
            "Batch 3/10, Batch Loss: 439492.1562\n",
            "Batch 4/10, Batch Loss: 264879.2812\n",
            "Batch 5/10, Batch Loss: 303316.2812\n",
            "Batch 6/10, Batch Loss: 258455.1875\n",
            "Batch 7/10, Batch Loss: 412836.8750\n",
            "Batch 8/10, Batch Loss: 249113.7500\n",
            "Batch 9/10, Batch Loss: 288386.0000\n",
            "Batch 10/10, Batch Loss: 378677.2500\n",
            "Test Error: \n",
            " Avg loss: 320786.320312, Avg error: 200710.195833 \n",
            "\n",
            "Epoch 168, Batch 1/38, Batch Loss: 321007.8438\n",
            "Epoch 168, Batch 2/38, Batch Loss: 227877.2500\n",
            "Epoch 168, Batch 3/38, Batch Loss: 330047.6562\n",
            "Epoch 168, Batch 4/38, Batch Loss: 349795.4062\n",
            "Epoch 168, Batch 5/38, Batch Loss: 289508.5312\n",
            "Epoch 168, Batch 6/38, Batch Loss: 216657.1719\n",
            "Epoch 168, Batch 7/38, Batch Loss: 300989.7812\n",
            "Epoch 168, Batch 8/38, Batch Loss: 713160.7500\n",
            "Epoch 168, Batch 9/38, Batch Loss: 617512.4375\n",
            "Epoch 168, Batch 10/38, Batch Loss: 255888.2188\n",
            "Epoch 168, Batch 11/38, Batch Loss: 249928.6094\n",
            "Epoch 168, Batch 12/38, Batch Loss: 318911.4062\n",
            "Epoch 168, Batch 13/38, Batch Loss: 245192.1562\n",
            "Epoch 168, Batch 14/38, Batch Loss: 680726.2500\n",
            "Epoch 168, Batch 15/38, Batch Loss: 721134.8125\n",
            "Epoch 168, Batch 16/38, Batch Loss: 376053.5938\n",
            "Epoch 168, Batch 17/38, Batch Loss: 257585.0312\n",
            "Epoch 168, Batch 18/38, Batch Loss: 272319.3438\n",
            "Epoch 168, Batch 19/38, Batch Loss: 406327.4375\n",
            "Epoch 168, Batch 20/38, Batch Loss: 304720.0938\n",
            "Epoch 168, Batch 21/38, Batch Loss: 692254.7500\n",
            "Epoch 168, Batch 22/38, Batch Loss: 359885.3750\n",
            "Epoch 168, Batch 23/38, Batch Loss: 424397.0938\n",
            "Epoch 168, Batch 24/38, Batch Loss: 249494.2188\n",
            "Epoch 168, Batch 25/38, Batch Loss: 263211.4062\n",
            "Epoch 168, Batch 26/38, Batch Loss: 218333.3594\n",
            "Epoch 168, Batch 27/38, Batch Loss: 636146.3125\n",
            "Epoch 168, Batch 28/38, Batch Loss: 288983.2188\n",
            "Epoch 168, Batch 29/38, Batch Loss: 275988.2188\n",
            "Epoch 168, Batch 30/38, Batch Loss: 259538.2344\n",
            "Epoch 168, Batch 31/38, Batch Loss: 264517.2812\n",
            "Epoch 168, Batch 32/38, Batch Loss: 299122.0938\n",
            "Epoch 168, Batch 33/38, Batch Loss: 196570.6719\n",
            "Epoch 168, Batch 34/38, Batch Loss: 719852.6875\n",
            "Epoch 168, Batch 35/38, Batch Loss: 366922.9062\n",
            "Epoch 168, Batch 36/38, Batch Loss: 229891.8594\n",
            "Epoch 168, Batch 37/38, Batch Loss: 687138.6250\n",
            "Epoch 168, Batch 38/38, Batch Loss: 292669.7500\n",
            "Batch 1/10, Batch Loss: 207675.0938\n",
            "Batch 2/10, Batch Loss: 266291.8750\n",
            "Batch 3/10, Batch Loss: 394691.0312\n",
            "Batch 4/10, Batch Loss: 315645.3438\n",
            "Batch 5/10, Batch Loss: 398590.5000\n",
            "Batch 6/10, Batch Loss: 215200.1875\n",
            "Batch 7/10, Batch Loss: 294926.8125\n",
            "Batch 8/10, Batch Loss: 334525.8438\n",
            "Batch 9/10, Batch Loss: 359592.2188\n",
            "Batch 10/10, Batch Loss: 182228.3594\n",
            "Test Error: \n",
            " Avg loss: 296936.726562, Avg error: 195701.954167 \n",
            "\n",
            "Epoch 169, Batch 1/38, Batch Loss: 282357.6875\n",
            "Epoch 169, Batch 2/38, Batch Loss: 355171.8125\n",
            "Epoch 169, Batch 3/38, Batch Loss: 203713.3281\n",
            "Epoch 169, Batch 4/38, Batch Loss: 269513.1875\n",
            "Epoch 169, Batch 5/38, Batch Loss: 397301.1250\n",
            "Epoch 169, Batch 6/38, Batch Loss: 270824.5000\n",
            "Epoch 169, Batch 7/38, Batch Loss: 198394.6406\n",
            "Epoch 169, Batch 8/38, Batch Loss: 338501.6250\n",
            "Epoch 169, Batch 9/38, Batch Loss: 759947.5625\n",
            "Epoch 169, Batch 10/38, Batch Loss: 267671.6875\n",
            "Epoch 169, Batch 11/38, Batch Loss: 200106.6875\n",
            "Epoch 169, Batch 12/38, Batch Loss: 269336.3750\n",
            "Epoch 169, Batch 13/38, Batch Loss: 728813.0000\n",
            "Epoch 169, Batch 14/38, Batch Loss: 262873.9062\n",
            "Epoch 169, Batch 15/38, Batch Loss: 694940.2500\n",
            "Epoch 169, Batch 16/38, Batch Loss: 270787.2812\n",
            "Epoch 169, Batch 17/38, Batch Loss: 338363.0938\n",
            "Epoch 169, Batch 18/38, Batch Loss: 302594.8750\n",
            "Epoch 169, Batch 19/38, Batch Loss: 701380.3750\n",
            "Epoch 169, Batch 20/38, Batch Loss: 251521.0469\n",
            "Epoch 169, Batch 21/38, Batch Loss: 280582.6562\n",
            "Epoch 169, Batch 22/38, Batch Loss: 221715.8281\n",
            "Epoch 169, Batch 23/38, Batch Loss: 775518.5000\n",
            "Epoch 169, Batch 24/38, Batch Loss: 203377.0469\n",
            "Epoch 169, Batch 25/38, Batch Loss: 234069.0469\n",
            "Epoch 169, Batch 26/38, Batch Loss: 224785.5469\n",
            "Epoch 169, Batch 27/38, Batch Loss: 331410.8438\n",
            "Epoch 169, Batch 28/38, Batch Loss: 270043.6875\n",
            "Epoch 169, Batch 29/38, Batch Loss: 260743.7188\n",
            "Epoch 169, Batch 30/38, Batch Loss: 241398.7344\n",
            "Epoch 169, Batch 31/38, Batch Loss: 244696.8281\n",
            "Epoch 169, Batch 32/38, Batch Loss: 392012.0312\n",
            "Epoch 169, Batch 33/38, Batch Loss: 248382.5469\n",
            "Epoch 169, Batch 34/38, Batch Loss: 691857.7500\n",
            "Epoch 169, Batch 35/38, Batch Loss: 242080.2812\n",
            "Epoch 169, Batch 36/38, Batch Loss: 261817.0000\n",
            "Epoch 169, Batch 37/38, Batch Loss: 724767.6250\n",
            "Epoch 169, Batch 38/38, Batch Loss: 332305.9062\n",
            "Batch 1/10, Batch Loss: 352582.5000\n",
            "Batch 2/10, Batch Loss: 341751.5312\n",
            "Batch 3/10, Batch Loss: 299453.4062\n",
            "Batch 4/10, Batch Loss: 287039.0938\n",
            "Batch 5/10, Batch Loss: 284777.2500\n",
            "Batch 6/10, Batch Loss: 307071.3438\n",
            "Batch 7/10, Batch Loss: 288601.2188\n",
            "Batch 8/10, Batch Loss: 296681.2500\n",
            "Batch 9/10, Batch Loss: 352014.4375\n",
            "Batch 10/10, Batch Loss: 142081.4219\n",
            "Test Error: \n",
            " Avg loss: 295205.345313, Avg error: 196311.342500 \n",
            "\n",
            "Epoch 170, Batch 1/38, Batch Loss: 259927.8594\n",
            "Epoch 170, Batch 2/38, Batch Loss: 680729.4375\n",
            "Epoch 170, Batch 3/38, Batch Loss: 259592.1562\n",
            "Epoch 170, Batch 4/38, Batch Loss: 298977.7188\n",
            "Epoch 170, Batch 5/38, Batch Loss: 676396.8125\n",
            "Epoch 170, Batch 6/38, Batch Loss: 413421.2188\n",
            "Epoch 170, Batch 7/38, Batch Loss: 289904.8750\n",
            "Epoch 170, Batch 8/38, Batch Loss: 350142.0312\n",
            "Epoch 170, Batch 9/38, Batch Loss: 238760.7344\n",
            "Epoch 170, Batch 10/38, Batch Loss: 324419.7812\n",
            "Epoch 170, Batch 11/38, Batch Loss: 219151.1094\n",
            "Epoch 170, Batch 12/38, Batch Loss: 258614.1562\n",
            "Epoch 170, Batch 13/38, Batch Loss: 271879.2500\n",
            "Epoch 170, Batch 14/38, Batch Loss: 337840.4062\n",
            "Epoch 170, Batch 15/38, Batch Loss: 226635.2969\n",
            "Epoch 170, Batch 16/38, Batch Loss: 270786.5938\n",
            "Epoch 170, Batch 17/38, Batch Loss: 286239.1562\n",
            "Epoch 170, Batch 18/38, Batch Loss: 265907.9375\n",
            "Epoch 170, Batch 19/38, Batch Loss: 212063.4219\n",
            "Epoch 170, Batch 20/38, Batch Loss: 293990.1875\n",
            "Epoch 170, Batch 21/38, Batch Loss: 316048.0312\n",
            "Epoch 170, Batch 22/38, Batch Loss: 252851.8281\n",
            "Epoch 170, Batch 23/38, Batch Loss: 424042.5312\n",
            "Epoch 170, Batch 24/38, Batch Loss: 239230.4219\n",
            "Epoch 170, Batch 25/38, Batch Loss: 289144.4375\n",
            "Epoch 170, Batch 26/38, Batch Loss: 239303.0156\n",
            "Epoch 170, Batch 27/38, Batch Loss: 747937.3125\n",
            "Epoch 170, Batch 28/38, Batch Loss: 250527.6250\n",
            "Epoch 170, Batch 29/38, Batch Loss: 330391.1875\n",
            "Epoch 170, Batch 30/38, Batch Loss: 245670.6719\n",
            "Epoch 170, Batch 31/38, Batch Loss: 690244.6875\n",
            "Epoch 170, Batch 32/38, Batch Loss: 289102.5625\n",
            "Epoch 170, Batch 33/38, Batch Loss: 364154.0625\n",
            "Epoch 170, Batch 34/38, Batch Loss: 695364.5000\n",
            "Epoch 170, Batch 35/38, Batch Loss: 334020.5312\n",
            "Epoch 170, Batch 36/38, Batch Loss: 345661.3750\n",
            "Epoch 170, Batch 37/38, Batch Loss: 664013.7500\n",
            "Epoch 170, Batch 38/38, Batch Loss: 992097.8125\n",
            "Batch 1/10, Batch Loss: 281420.4375\n",
            "Batch 2/10, Batch Loss: 287464.4375\n",
            "Batch 3/10, Batch Loss: 305486.7500\n",
            "Batch 4/10, Batch Loss: 267000.3438\n",
            "Batch 5/10, Batch Loss: 356966.7188\n",
            "Batch 6/10, Batch Loss: 284998.4062\n",
            "Batch 7/10, Batch Loss: 341452.0625\n",
            "Batch 8/10, Batch Loss: 251181.3125\n",
            "Batch 9/10, Batch Loss: 376164.2500\n",
            "Batch 10/10, Batch Loss: 182068.3281\n",
            "Test Error: \n",
            " Avg loss: 293420.304688, Avg error: 195837.295000 \n",
            "\n",
            "2024-09-14 20:49:02.431016 Epoch 170, Average Training loss 372241.7495888158\n",
            "Epoch 171, Batch 1/38, Batch Loss: 243707.0625\n",
            "Epoch 171, Batch 2/38, Batch Loss: 263052.9375\n",
            "Epoch 171, Batch 3/38, Batch Loss: 225888.5938\n",
            "Epoch 171, Batch 4/38, Batch Loss: 324272.8438\n",
            "Epoch 171, Batch 5/38, Batch Loss: 217064.0938\n",
            "Epoch 171, Batch 6/38, Batch Loss: 358236.6562\n",
            "Epoch 171, Batch 7/38, Batch Loss: 281460.4375\n",
            "Epoch 171, Batch 8/38, Batch Loss: 312497.3438\n",
            "Epoch 171, Batch 9/38, Batch Loss: 272041.3125\n",
            "Epoch 171, Batch 10/38, Batch Loss: 360561.6250\n",
            "Epoch 171, Batch 11/38, Batch Loss: 658338.1875\n",
            "Epoch 171, Batch 12/38, Batch Loss: 322380.8438\n",
            "Epoch 171, Batch 13/38, Batch Loss: 906715.5000\n",
            "Epoch 171, Batch 14/38, Batch Loss: 322469.2500\n",
            "Epoch 171, Batch 15/38, Batch Loss: 413544.0625\n",
            "Epoch 171, Batch 16/38, Batch Loss: 234877.6562\n",
            "Epoch 171, Batch 17/38, Batch Loss: 272771.8438\n",
            "Epoch 171, Batch 18/38, Batch Loss: 282512.2188\n",
            "Epoch 171, Batch 19/38, Batch Loss: 386456.0312\n",
            "Epoch 171, Batch 20/38, Batch Loss: 404462.3438\n",
            "Epoch 171, Batch 21/38, Batch Loss: 247734.1094\n",
            "Epoch 171, Batch 22/38, Batch Loss: 318420.5000\n",
            "Epoch 171, Batch 23/38, Batch Loss: 227905.2344\n",
            "Epoch 171, Batch 24/38, Batch Loss: 287153.7188\n",
            "Epoch 171, Batch 25/38, Batch Loss: 279017.4062\n",
            "Epoch 171, Batch 26/38, Batch Loss: 720611.5000\n",
            "Epoch 171, Batch 27/38, Batch Loss: 249163.5781\n",
            "Epoch 171, Batch 28/38, Batch Loss: 702614.0625\n",
            "Epoch 171, Batch 29/38, Batch Loss: 195222.8438\n",
            "Epoch 171, Batch 30/38, Batch Loss: 718419.6250\n",
            "Epoch 171, Batch 31/38, Batch Loss: 688867.1250\n",
            "Epoch 171, Batch 32/38, Batch Loss: 199662.3750\n",
            "Epoch 171, Batch 33/38, Batch Loss: 314362.0625\n",
            "Epoch 171, Batch 34/38, Batch Loss: 339416.5938\n",
            "Epoch 171, Batch 35/38, Batch Loss: 329982.1875\n",
            "Epoch 171, Batch 36/38, Batch Loss: 249186.3125\n",
            "Epoch 171, Batch 37/38, Batch Loss: 247672.1719\n",
            "Epoch 171, Batch 38/38, Batch Loss: 422231.0312\n",
            "Batch 1/10, Batch Loss: 221582.8750\n",
            "Batch 2/10, Batch Loss: 455877.5000\n",
            "Batch 3/10, Batch Loss: 273398.3438\n",
            "Batch 4/10, Batch Loss: 250408.9531\n",
            "Batch 5/10, Batch Loss: 278153.8125\n",
            "Batch 6/10, Batch Loss: 294447.8125\n",
            "Batch 7/10, Batch Loss: 266662.4062\n",
            "Batch 8/10, Batch Loss: 313447.2500\n",
            "Batch 9/10, Batch Loss: 271571.6875\n",
            "Batch 10/10, Batch Loss: 396221.2812\n",
            "Test Error: \n",
            " Avg loss: 302177.192188, Avg error: 194999.686667 \n",
            "\n",
            "Epoch 172, Batch 1/38, Batch Loss: 345680.6562\n",
            "Epoch 172, Batch 2/38, Batch Loss: 665294.2500\n",
            "Epoch 172, Batch 3/38, Batch Loss: 310335.7812\n",
            "Epoch 172, Batch 4/38, Batch Loss: 298292.1875\n",
            "Epoch 172, Batch 5/38, Batch Loss: 311546.9375\n",
            "Epoch 172, Batch 6/38, Batch Loss: 297162.6250\n",
            "Epoch 172, Batch 7/38, Batch Loss: 265179.1562\n",
            "Epoch 172, Batch 8/38, Batch Loss: 295387.9688\n",
            "Epoch 172, Batch 9/38, Batch Loss: 320908.5000\n",
            "Epoch 172, Batch 10/38, Batch Loss: 195051.5938\n",
            "Epoch 172, Batch 11/38, Batch Loss: 717714.0625\n",
            "Epoch 172, Batch 12/38, Batch Loss: 282438.5625\n",
            "Epoch 172, Batch 13/38, Batch Loss: 263717.3438\n",
            "Epoch 172, Batch 14/38, Batch Loss: 216995.6406\n",
            "Epoch 172, Batch 15/38, Batch Loss: 256083.1875\n",
            "Epoch 172, Batch 16/38, Batch Loss: 316208.8750\n",
            "Epoch 172, Batch 17/38, Batch Loss: 370100.2812\n",
            "Epoch 172, Batch 18/38, Batch Loss: 222734.5625\n",
            "Epoch 172, Batch 19/38, Batch Loss: 335064.7812\n",
            "Epoch 172, Batch 20/38, Batch Loss: 294189.3125\n",
            "Epoch 172, Batch 21/38, Batch Loss: 271448.9062\n",
            "Epoch 172, Batch 22/38, Batch Loss: 687193.9375\n",
            "Epoch 172, Batch 23/38, Batch Loss: 216441.2812\n",
            "Epoch 172, Batch 24/38, Batch Loss: 243285.0312\n",
            "Epoch 172, Batch 25/38, Batch Loss: 659660.4375\n",
            "Epoch 172, Batch 26/38, Batch Loss: 244368.2344\n",
            "Epoch 172, Batch 27/38, Batch Loss: 944789.5625\n",
            "Epoch 172, Batch 28/38, Batch Loss: 207373.2656\n",
            "Epoch 172, Batch 29/38, Batch Loss: 204452.5781\n",
            "Epoch 172, Batch 30/38, Batch Loss: 653258.7500\n",
            "Epoch 172, Batch 31/38, Batch Loss: 244813.4062\n",
            "Epoch 172, Batch 32/38, Batch Loss: 282240.9062\n",
            "Epoch 172, Batch 33/38, Batch Loss: 369231.6250\n",
            "Epoch 172, Batch 34/38, Batch Loss: 295268.0000\n",
            "Epoch 172, Batch 35/38, Batch Loss: 235924.9531\n",
            "Epoch 172, Batch 36/38, Batch Loss: 438873.2188\n",
            "Epoch 172, Batch 37/38, Batch Loss: 258919.8594\n",
            "Epoch 172, Batch 38/38, Batch Loss: 279155.7500\n",
            "Batch 1/10, Batch Loss: 301021.6250\n",
            "Batch 2/10, Batch Loss: 258283.5312\n",
            "Batch 3/10, Batch Loss: 328446.3125\n",
            "Batch 4/10, Batch Loss: 339006.8750\n",
            "Batch 5/10, Batch Loss: 333002.7812\n",
            "Batch 6/10, Batch Loss: 276875.5000\n",
            "Batch 7/10, Batch Loss: 276230.9688\n",
            "Batch 8/10, Batch Loss: 283430.2500\n",
            "Batch 9/10, Batch Loss: 207766.8906\n",
            "Batch 10/10, Batch Loss: 419136.7500\n",
            "Test Error: \n",
            " Avg loss: 302320.148438, Avg error: 192842.131667 \n",
            "\n",
            "Epoch 173, Batch 1/38, Batch Loss: 260675.0469\n",
            "Epoch 173, Batch 2/38, Batch Loss: 226440.4688\n",
            "Epoch 173, Batch 3/38, Batch Loss: 662213.1875\n",
            "Epoch 173, Batch 4/38, Batch Loss: 649402.5000\n",
            "Epoch 173, Batch 5/38, Batch Loss: 309175.9062\n",
            "Epoch 173, Batch 6/38, Batch Loss: 278881.0938\n",
            "Epoch 173, Batch 7/38, Batch Loss: 223202.7031\n",
            "Epoch 173, Batch 8/38, Batch Loss: 955720.3125\n",
            "Epoch 173, Batch 9/38, Batch Loss: 425480.9688\n",
            "Epoch 173, Batch 10/38, Batch Loss: 318561.3438\n",
            "Epoch 173, Batch 11/38, Batch Loss: 375090.6562\n",
            "Epoch 173, Batch 12/38, Batch Loss: 680363.9375\n",
            "Epoch 173, Batch 13/38, Batch Loss: 231868.9531\n",
            "Epoch 173, Batch 14/38, Batch Loss: 348111.7500\n",
            "Epoch 173, Batch 15/38, Batch Loss: 283868.6875\n",
            "Epoch 173, Batch 16/38, Batch Loss: 270421.4688\n",
            "Epoch 173, Batch 17/38, Batch Loss: 274511.1562\n",
            "Epoch 173, Batch 18/38, Batch Loss: 297587.0625\n",
            "Epoch 173, Batch 19/38, Batch Loss: 247989.7969\n",
            "Epoch 173, Batch 20/38, Batch Loss: 685422.6875\n",
            "Epoch 173, Batch 21/38, Batch Loss: 292374.3125\n",
            "Epoch 173, Batch 22/38, Batch Loss: 244246.9844\n",
            "Epoch 173, Batch 23/38, Batch Loss: 255685.5469\n",
            "Epoch 173, Batch 24/38, Batch Loss: 306170.6875\n",
            "Epoch 173, Batch 25/38, Batch Loss: 277520.4375\n",
            "Epoch 173, Batch 26/38, Batch Loss: 252989.9375\n",
            "Epoch 173, Batch 27/38, Batch Loss: 275243.9688\n",
            "Epoch 173, Batch 28/38, Batch Loss: 313629.7188\n",
            "Epoch 173, Batch 29/38, Batch Loss: 243441.4688\n",
            "Epoch 173, Batch 30/38, Batch Loss: 265943.8125\n",
            "Epoch 173, Batch 31/38, Batch Loss: 436878.0938\n",
            "Epoch 173, Batch 32/38, Batch Loss: 453835.0938\n",
            "Epoch 173, Batch 33/38, Batch Loss: 250547.6094\n",
            "Epoch 173, Batch 34/38, Batch Loss: 666921.8125\n",
            "Epoch 173, Batch 35/38, Batch Loss: 213242.2656\n",
            "Epoch 173, Batch 36/38, Batch Loss: 309341.0312\n",
            "Epoch 173, Batch 37/38, Batch Loss: 215833.5469\n",
            "Epoch 173, Batch 38/38, Batch Loss: 263519.3438\n",
            "Batch 1/10, Batch Loss: 297031.4688\n",
            "Batch 2/10, Batch Loss: 460258.6562\n",
            "Batch 3/10, Batch Loss: 259523.3281\n",
            "Batch 4/10, Batch Loss: 292150.0000\n",
            "Batch 5/10, Batch Loss: 258218.9062\n",
            "Batch 6/10, Batch Loss: 254076.4844\n",
            "Batch 7/10, Batch Loss: 241992.7812\n",
            "Batch 8/10, Batch Loss: 260817.1094\n",
            "Batch 9/10, Batch Loss: 364344.5625\n",
            "Batch 10/10, Batch Loss: 198257.9219\n",
            "Test Error: \n",
            " Avg loss: 288667.121875, Avg error: 193824.748333 \n",
            "\n",
            "Epoch 174, Batch 1/38, Batch Loss: 324155.5312\n",
            "Epoch 174, Batch 2/38, Batch Loss: 263478.0625\n",
            "Epoch 174, Batch 3/38, Batch Loss: 272913.5312\n",
            "Epoch 174, Batch 4/38, Batch Loss: 663338.1875\n",
            "Epoch 174, Batch 5/38, Batch Loss: 227149.7344\n",
            "Epoch 174, Batch 6/38, Batch Loss: 245148.2500\n",
            "Epoch 174, Batch 7/38, Batch Loss: 237354.4688\n",
            "Epoch 174, Batch 8/38, Batch Loss: 286762.5000\n",
            "Epoch 174, Batch 9/38, Batch Loss: 655055.3750\n",
            "Epoch 174, Batch 10/38, Batch Loss: 248598.5156\n",
            "Epoch 174, Batch 11/38, Batch Loss: 299347.5625\n",
            "Epoch 174, Batch 12/38, Batch Loss: 920611.5625\n",
            "Epoch 174, Batch 13/38, Batch Loss: 241989.9375\n",
            "Epoch 174, Batch 14/38, Batch Loss: 281538.7812\n",
            "Epoch 174, Batch 15/38, Batch Loss: 363021.3438\n",
            "Epoch 174, Batch 16/38, Batch Loss: 250728.1406\n",
            "Epoch 174, Batch 17/38, Batch Loss: 291201.8125\n",
            "Epoch 174, Batch 18/38, Batch Loss: 684048.3750\n",
            "Epoch 174, Batch 19/38, Batch Loss: 291817.7188\n",
            "Epoch 174, Batch 20/38, Batch Loss: 248167.5469\n",
            "Epoch 174, Batch 21/38, Batch Loss: 416817.2812\n",
            "Epoch 174, Batch 22/38, Batch Loss: 509083.8125\n",
            "Epoch 174, Batch 23/38, Batch Loss: 256941.5469\n",
            "Epoch 174, Batch 24/38, Batch Loss: 389523.7500\n",
            "Epoch 174, Batch 25/38, Batch Loss: 294685.7812\n",
            "Epoch 174, Batch 26/38, Batch Loss: 252999.7656\n",
            "Epoch 174, Batch 27/38, Batch Loss: 249689.6875\n",
            "Epoch 174, Batch 28/38, Batch Loss: 321914.8750\n",
            "Epoch 174, Batch 29/38, Batch Loss: 318025.9062\n",
            "Epoch 174, Batch 30/38, Batch Loss: 238192.4531\n",
            "Epoch 174, Batch 31/38, Batch Loss: 237136.7500\n",
            "Epoch 174, Batch 32/38, Batch Loss: 206111.7812\n",
            "Epoch 174, Batch 33/38, Batch Loss: 677048.5000\n",
            "Epoch 174, Batch 34/38, Batch Loss: 656460.0625\n",
            "Epoch 174, Batch 35/38, Batch Loss: 303813.0312\n",
            "Epoch 174, Batch 36/38, Batch Loss: 337267.5000\n",
            "Epoch 174, Batch 37/38, Batch Loss: 243650.2812\n",
            "Epoch 174, Batch 38/38, Batch Loss: 290186.4688\n",
            "Batch 1/10, Batch Loss: 271388.8750\n",
            "Batch 2/10, Batch Loss: 291170.6562\n",
            "Batch 3/10, Batch Loss: 322303.2500\n",
            "Batch 4/10, Batch Loss: 199951.2969\n",
            "Batch 5/10, Batch Loss: 273159.5625\n",
            "Batch 6/10, Batch Loss: 331758.7812\n",
            "Batch 7/10, Batch Loss: 405221.7500\n",
            "Batch 8/10, Batch Loss: 247290.1719\n",
            "Batch 9/10, Batch Loss: 239256.4688\n",
            "Batch 10/10, Batch Loss: 437577.0938\n",
            "Test Error: \n",
            " Avg loss: 301907.790625, Avg error: 192181.414167 \n",
            "\n",
            "Epoch 175, Batch 1/38, Batch Loss: 690631.0625\n",
            "Epoch 175, Batch 2/38, Batch Loss: 302787.6562\n",
            "Epoch 175, Batch 3/38, Batch Loss: 314781.4375\n",
            "Epoch 175, Batch 4/38, Batch Loss: 268893.1875\n",
            "Epoch 175, Batch 5/38, Batch Loss: 275749.7500\n",
            "Epoch 175, Batch 6/38, Batch Loss: 654663.6250\n",
            "Epoch 175, Batch 7/38, Batch Loss: 226990.2812\n",
            "Epoch 175, Batch 8/38, Batch Loss: 686433.3125\n",
            "Epoch 175, Batch 9/38, Batch Loss: 669658.5000\n",
            "Epoch 175, Batch 10/38, Batch Loss: 664042.0625\n",
            "Epoch 175, Batch 11/38, Batch Loss: 279721.5000\n",
            "Epoch 175, Batch 12/38, Batch Loss: 257613.9219\n",
            "Epoch 175, Batch 13/38, Batch Loss: 276010.2500\n",
            "Epoch 175, Batch 14/38, Batch Loss: 261878.5312\n",
            "Epoch 175, Batch 15/38, Batch Loss: 656956.6250\n",
            "Epoch 175, Batch 16/38, Batch Loss: 720787.8750\n",
            "Epoch 175, Batch 17/38, Batch Loss: 402143.5000\n",
            "Epoch 175, Batch 18/38, Batch Loss: 403160.8438\n",
            "Epoch 175, Batch 19/38, Batch Loss: 289813.6875\n",
            "Epoch 175, Batch 20/38, Batch Loss: 255267.0312\n",
            "Epoch 175, Batch 21/38, Batch Loss: 367905.2188\n",
            "Epoch 175, Batch 22/38, Batch Loss: 311857.3750\n",
            "Epoch 175, Batch 23/38, Batch Loss: 357060.4688\n",
            "Epoch 175, Batch 24/38, Batch Loss: 328183.1250\n",
            "Epoch 175, Batch 25/38, Batch Loss: 210085.1562\n",
            "Epoch 175, Batch 26/38, Batch Loss: 257423.2969\n",
            "Epoch 175, Batch 27/38, Batch Loss: 342596.9688\n",
            "Epoch 175, Batch 28/38, Batch Loss: 339880.3125\n",
            "Epoch 175, Batch 29/38, Batch Loss: 238431.9219\n",
            "Epoch 175, Batch 30/38, Batch Loss: 304511.2500\n",
            "Epoch 175, Batch 31/38, Batch Loss: 300511.0000\n",
            "Epoch 175, Batch 32/38, Batch Loss: 244548.0625\n",
            "Epoch 175, Batch 33/38, Batch Loss: 348149.1250\n",
            "Epoch 175, Batch 34/38, Batch Loss: 245720.6406\n",
            "Epoch 175, Batch 35/38, Batch Loss: 281312.0625\n",
            "Epoch 175, Batch 36/38, Batch Loss: 299411.0000\n",
            "Epoch 175, Batch 37/38, Batch Loss: 286818.6250\n",
            "Epoch 175, Batch 38/38, Batch Loss: 202979.1094\n",
            "Batch 1/10, Batch Loss: 248138.9531\n",
            "Batch 2/10, Batch Loss: 285693.4062\n",
            "Batch 3/10, Batch Loss: 185804.7031\n",
            "Batch 4/10, Batch Loss: 389421.3125\n",
            "Batch 5/10, Batch Loss: 339948.1875\n",
            "Batch 6/10, Batch Loss: 267376.7188\n",
            "Batch 7/10, Batch Loss: 236526.5469\n",
            "Batch 8/10, Batch Loss: 418184.4062\n",
            "Batch 9/10, Batch Loss: 308857.7188\n",
            "Batch 10/10, Batch Loss: 274706.4062\n",
            "Test Error: \n",
            " Avg loss: 295465.835938, Avg error: 197400.307500 \n",
            "\n",
            "Epoch 176, Batch 1/38, Batch Loss: 294946.5938\n",
            "Epoch 176, Batch 2/38, Batch Loss: 244017.3438\n",
            "Epoch 176, Batch 3/38, Batch Loss: 256709.3594\n",
            "Epoch 176, Batch 4/38, Batch Loss: 431528.1562\n",
            "Epoch 176, Batch 5/38, Batch Loss: 232914.0000\n",
            "Epoch 176, Batch 6/38, Batch Loss: 281289.1250\n",
            "Epoch 176, Batch 7/38, Batch Loss: 306285.7500\n",
            "Epoch 176, Batch 8/38, Batch Loss: 254441.9375\n",
            "Epoch 176, Batch 9/38, Batch Loss: 661230.8125\n",
            "Epoch 176, Batch 10/38, Batch Loss: 221952.5312\n",
            "Epoch 176, Batch 11/38, Batch Loss: 314130.2188\n",
            "Epoch 176, Batch 12/38, Batch Loss: 252614.9531\n",
            "Epoch 176, Batch 13/38, Batch Loss: 683686.8750\n",
            "Epoch 176, Batch 14/38, Batch Loss: 342218.0938\n",
            "Epoch 176, Batch 15/38, Batch Loss: 273210.5938\n",
            "Epoch 176, Batch 16/38, Batch Loss: 301940.4375\n",
            "Epoch 176, Batch 17/38, Batch Loss: 658502.5625\n",
            "Epoch 176, Batch 18/38, Batch Loss: 292815.4062\n",
            "Epoch 176, Batch 19/38, Batch Loss: 279378.9375\n",
            "Epoch 176, Batch 20/38, Batch Loss: 221480.8906\n",
            "Epoch 176, Batch 21/38, Batch Loss: 347096.7500\n",
            "Epoch 176, Batch 22/38, Batch Loss: 254424.5000\n",
            "Epoch 176, Batch 23/38, Batch Loss: 339235.3750\n",
            "Epoch 176, Batch 24/38, Batch Loss: 329285.1562\n",
            "Epoch 176, Batch 25/38, Batch Loss: 308112.1250\n",
            "Epoch 176, Batch 26/38, Batch Loss: 286085.8438\n",
            "Epoch 176, Batch 27/38, Batch Loss: 926718.4375\n",
            "Epoch 176, Batch 28/38, Batch Loss: 294227.1562\n",
            "Epoch 176, Batch 29/38, Batch Loss: 301517.8750\n",
            "Epoch 176, Batch 30/38, Batch Loss: 283235.5000\n",
            "Epoch 176, Batch 31/38, Batch Loss: 261636.4062\n",
            "Epoch 176, Batch 32/38, Batch Loss: 302917.1562\n",
            "Epoch 176, Batch 33/38, Batch Loss: 263823.2188\n",
            "Epoch 176, Batch 34/38, Batch Loss: 225096.0469\n",
            "Epoch 176, Batch 35/38, Batch Loss: 664590.3750\n",
            "Epoch 176, Batch 36/38, Batch Loss: 385870.1562\n",
            "Epoch 176, Batch 37/38, Batch Loss: 708633.1875\n",
            "Epoch 176, Batch 38/38, Batch Loss: 248029.1719\n",
            "Batch 1/10, Batch Loss: 239546.8438\n",
            "Batch 2/10, Batch Loss: 336844.3750\n",
            "Batch 3/10, Batch Loss: 426619.5938\n",
            "Batch 4/10, Batch Loss: 346089.0625\n",
            "Batch 5/10, Batch Loss: 226033.0938\n",
            "Batch 6/10, Batch Loss: 288899.5938\n",
            "Batch 7/10, Batch Loss: 211550.9062\n",
            "Batch 8/10, Batch Loss: 320433.0625\n",
            "Batch 9/10, Batch Loss: 328231.4688\n",
            "Batch 10/10, Batch Loss: 342226.1250\n",
            "Test Error: \n",
            " Avg loss: 306647.412500, Avg error: 196714.941667 \n",
            "\n",
            "Epoch 177, Batch 1/38, Batch Loss: 316554.6562\n",
            "Epoch 177, Batch 2/38, Batch Loss: 245329.4219\n",
            "Epoch 177, Batch 3/38, Batch Loss: 262075.2031\n",
            "Epoch 177, Batch 4/38, Batch Loss: 282526.5000\n",
            "Epoch 177, Batch 5/38, Batch Loss: 693240.0625\n",
            "Epoch 177, Batch 6/38, Batch Loss: 257319.0312\n",
            "Epoch 177, Batch 7/38, Batch Loss: 319346.8125\n",
            "Epoch 177, Batch 8/38, Batch Loss: 231194.7500\n",
            "Epoch 177, Batch 9/38, Batch Loss: 239620.0938\n",
            "Epoch 177, Batch 10/38, Batch Loss: 703306.6250\n",
            "Epoch 177, Batch 11/38, Batch Loss: 430042.1875\n",
            "Epoch 177, Batch 12/38, Batch Loss: 255376.5156\n",
            "Epoch 177, Batch 13/38, Batch Loss: 686341.0625\n",
            "Epoch 177, Batch 14/38, Batch Loss: 332457.5625\n",
            "Epoch 177, Batch 15/38, Batch Loss: 692927.8750\n",
            "Epoch 177, Batch 16/38, Batch Loss: 244087.2344\n",
            "Epoch 177, Batch 17/38, Batch Loss: 246433.4688\n",
            "Epoch 177, Batch 18/38, Batch Loss: 215957.7969\n",
            "Epoch 177, Batch 19/38, Batch Loss: 214011.0000\n",
            "Epoch 177, Batch 20/38, Batch Loss: 294349.0625\n",
            "Epoch 177, Batch 21/38, Batch Loss: 663761.3125\n",
            "Epoch 177, Batch 22/38, Batch Loss: 707754.6875\n",
            "Epoch 177, Batch 23/38, Batch Loss: 317988.1562\n",
            "Epoch 177, Batch 24/38, Batch Loss: 357931.1875\n",
            "Epoch 177, Batch 25/38, Batch Loss: 356366.8750\n",
            "Epoch 177, Batch 26/38, Batch Loss: 355505.3750\n",
            "Epoch 177, Batch 27/38, Batch Loss: 260023.5469\n",
            "Epoch 177, Batch 28/38, Batch Loss: 694475.2500\n",
            "Epoch 177, Batch 29/38, Batch Loss: 290398.1875\n",
            "Epoch 177, Batch 30/38, Batch Loss: 277206.9062\n",
            "Epoch 177, Batch 31/38, Batch Loss: 302433.7500\n",
            "Epoch 177, Batch 32/38, Batch Loss: 291396.8438\n",
            "Epoch 177, Batch 33/38, Batch Loss: 358958.0625\n",
            "Epoch 177, Batch 34/38, Batch Loss: 242191.1250\n",
            "Epoch 177, Batch 35/38, Batch Loss: 389328.0000\n",
            "Epoch 177, Batch 36/38, Batch Loss: 279844.3438\n",
            "Epoch 177, Batch 37/38, Batch Loss: 244642.2344\n",
            "Epoch 177, Batch 38/38, Batch Loss: 250264.6562\n",
            "Batch 1/10, Batch Loss: 282160.2500\n",
            "Batch 2/10, Batch Loss: 228672.4531\n",
            "Batch 3/10, Batch Loss: 449414.5312\n",
            "Batch 4/10, Batch Loss: 262077.5781\n",
            "Batch 5/10, Batch Loss: 188587.0469\n",
            "Batch 6/10, Batch Loss: 318802.3438\n",
            "Batch 7/10, Batch Loss: 291153.0938\n",
            "Batch 8/10, Batch Loss: 260148.5938\n",
            "Batch 9/10, Batch Loss: 304574.8438\n",
            "Batch 10/10, Batch Loss: 400843.0625\n",
            "Test Error: \n",
            " Avg loss: 298643.379688, Avg error: 191562.447500 \n",
            "\n",
            "Epoch 178, Batch 1/38, Batch Loss: 309686.9062\n",
            "Epoch 178, Batch 2/38, Batch Loss: 647518.3750\n",
            "Epoch 178, Batch 3/38, Batch Loss: 308591.0000\n",
            "Epoch 178, Batch 4/38, Batch Loss: 663590.5000\n",
            "Epoch 178, Batch 5/38, Batch Loss: 283461.2812\n",
            "Epoch 178, Batch 6/38, Batch Loss: 248586.2656\n",
            "Epoch 178, Batch 7/38, Batch Loss: 226544.1094\n",
            "Epoch 178, Batch 8/38, Batch Loss: 266404.2812\n",
            "Epoch 178, Batch 9/38, Batch Loss: 426228.8750\n",
            "Epoch 178, Batch 10/38, Batch Loss: 249360.5156\n",
            "Epoch 178, Batch 11/38, Batch Loss: 289698.3750\n",
            "Epoch 178, Batch 12/38, Batch Loss: 276036.6250\n",
            "Epoch 178, Batch 13/38, Batch Loss: 299927.3750\n",
            "Epoch 178, Batch 14/38, Batch Loss: 286441.1250\n",
            "Epoch 178, Batch 15/38, Batch Loss: 271922.4688\n",
            "Epoch 178, Batch 16/38, Batch Loss: 361511.7812\n",
            "Epoch 178, Batch 17/38, Batch Loss: 277972.1250\n",
            "Epoch 178, Batch 18/38, Batch Loss: 203719.2031\n",
            "Epoch 178, Batch 19/38, Batch Loss: 271566.4062\n",
            "Epoch 178, Batch 20/38, Batch Loss: 696778.0625\n",
            "Epoch 178, Batch 21/38, Batch Loss: 270085.9375\n",
            "Epoch 178, Batch 22/38, Batch Loss: 687843.5625\n",
            "Epoch 178, Batch 23/38, Batch Loss: 281047.7812\n",
            "Epoch 178, Batch 24/38, Batch Loss: 361426.1562\n",
            "Epoch 178, Batch 25/38, Batch Loss: 257957.2969\n",
            "Epoch 178, Batch 26/38, Batch Loss: 664300.8750\n",
            "Epoch 178, Batch 27/38, Batch Loss: 255752.0938\n",
            "Epoch 178, Batch 28/38, Batch Loss: 312290.0000\n",
            "Epoch 178, Batch 29/38, Batch Loss: 750070.5000\n",
            "Epoch 178, Batch 30/38, Batch Loss: 321034.0312\n",
            "Epoch 178, Batch 31/38, Batch Loss: 270376.5000\n",
            "Epoch 178, Batch 32/38, Batch Loss: 272094.7500\n",
            "Epoch 178, Batch 33/38, Batch Loss: 653098.6250\n",
            "Epoch 178, Batch 34/38, Batch Loss: 219836.3594\n",
            "Epoch 178, Batch 35/38, Batch Loss: 326260.3125\n",
            "Epoch 178, Batch 36/38, Batch Loss: 297172.0938\n",
            "Epoch 178, Batch 37/38, Batch Loss: 267433.2500\n",
            "Epoch 178, Batch 38/38, Batch Loss: 562613.8750\n",
            "Batch 1/10, Batch Loss: 300092.8438\n",
            "Batch 2/10, Batch Loss: 373989.3125\n",
            "Batch 3/10, Batch Loss: 235420.2344\n",
            "Batch 4/10, Batch Loss: 428268.8125\n",
            "Batch 5/10, Batch Loss: 262588.5938\n",
            "Batch 6/10, Batch Loss: 279027.0625\n",
            "Batch 7/10, Batch Loss: 283501.2500\n",
            "Batch 8/10, Batch Loss: 276651.2500\n",
            "Batch 9/10, Batch Loss: 407142.2188\n",
            "Batch 10/10, Batch Loss: 317912.5312\n",
            "Test Error: \n",
            " Avg loss: 316459.410938, Avg error: 202271.856667 \n",
            "\n",
            "Epoch 179, Batch 1/38, Batch Loss: 258563.6719\n",
            "Epoch 179, Batch 2/38, Batch Loss: 307239.6250\n",
            "Epoch 179, Batch 3/38, Batch Loss: 186995.7656\n",
            "Epoch 179, Batch 4/38, Batch Loss: 689738.3125\n",
            "Epoch 179, Batch 5/38, Batch Loss: 271510.7188\n",
            "Epoch 179, Batch 6/38, Batch Loss: 465827.4062\n",
            "Epoch 179, Batch 7/38, Batch Loss: 211081.2969\n",
            "Epoch 179, Batch 8/38, Batch Loss: 257576.8594\n",
            "Epoch 179, Batch 9/38, Batch Loss: 243101.6562\n",
            "Epoch 179, Batch 10/38, Batch Loss: 254547.0312\n",
            "Epoch 179, Batch 11/38, Batch Loss: 243919.2031\n",
            "Epoch 179, Batch 12/38, Batch Loss: 319034.0312\n",
            "Epoch 179, Batch 13/38, Batch Loss: 237956.1250\n",
            "Epoch 179, Batch 14/38, Batch Loss: 670485.1250\n",
            "Epoch 179, Batch 15/38, Batch Loss: 263185.2188\n",
            "Epoch 179, Batch 16/38, Batch Loss: 310079.7188\n",
            "Epoch 179, Batch 17/38, Batch Loss: 248473.8594\n",
            "Epoch 179, Batch 18/38, Batch Loss: 242942.4688\n",
            "Epoch 179, Batch 19/38, Batch Loss: 291303.8125\n",
            "Epoch 179, Batch 20/38, Batch Loss: 337362.4375\n",
            "Epoch 179, Batch 21/38, Batch Loss: 347181.8750\n",
            "Epoch 179, Batch 22/38, Batch Loss: 336498.1875\n",
            "Epoch 179, Batch 23/38, Batch Loss: 259683.7031\n",
            "Epoch 179, Batch 24/38, Batch Loss: 316611.4688\n",
            "Epoch 179, Batch 25/38, Batch Loss: 208779.4219\n",
            "Epoch 179, Batch 26/38, Batch Loss: 299495.8125\n",
            "Epoch 179, Batch 27/38, Batch Loss: 188547.6719\n",
            "Epoch 179, Batch 28/38, Batch Loss: 666564.7500\n",
            "Epoch 179, Batch 29/38, Batch Loss: 696455.3125\n",
            "Epoch 179, Batch 30/38, Batch Loss: 347779.5000\n",
            "Epoch 179, Batch 31/38, Batch Loss: 793156.5000\n",
            "Epoch 179, Batch 32/38, Batch Loss: 675982.0000\n",
            "Epoch 179, Batch 33/38, Batch Loss: 316281.2188\n",
            "Epoch 179, Batch 34/38, Batch Loss: 694226.9375\n",
            "Epoch 179, Batch 35/38, Batch Loss: 390320.3750\n",
            "Epoch 179, Batch 36/38, Batch Loss: 253569.3125\n",
            "Epoch 179, Batch 37/38, Batch Loss: 354388.7500\n",
            "Epoch 179, Batch 38/38, Batch Loss: 235500.9844\n",
            "Batch 1/10, Batch Loss: 269166.7188\n",
            "Batch 2/10, Batch Loss: 384344.0312\n",
            "Batch 3/10, Batch Loss: 261888.2656\n",
            "Batch 4/10, Batch Loss: 350398.6250\n",
            "Batch 5/10, Batch Loss: 306153.1562\n",
            "Batch 6/10, Batch Loss: 242556.6250\n",
            "Batch 7/10, Batch Loss: 297455.4062\n",
            "Batch 8/10, Batch Loss: 370861.9375\n",
            "Batch 9/10, Batch Loss: 226360.8750\n",
            "Batch 10/10, Batch Loss: 239129.1875\n",
            "Test Error: \n",
            " Avg loss: 294831.482812, Avg error: 192902.715833 \n",
            "\n",
            "Epoch 180, Batch 1/38, Batch Loss: 247203.8906\n",
            "Epoch 180, Batch 2/38, Batch Loss: 258036.1406\n",
            "Epoch 180, Batch 3/38, Batch Loss: 285352.3750\n",
            "Epoch 180, Batch 4/38, Batch Loss: 281202.3125\n",
            "Epoch 180, Batch 5/38, Batch Loss: 240183.7344\n",
            "Epoch 180, Batch 6/38, Batch Loss: 294169.5938\n",
            "Epoch 180, Batch 7/38, Batch Loss: 648162.8125\n",
            "Epoch 180, Batch 8/38, Batch Loss: 275631.2500\n",
            "Epoch 180, Batch 9/38, Batch Loss: 254646.1250\n",
            "Epoch 180, Batch 10/38, Batch Loss: 272139.6562\n",
            "Epoch 180, Batch 11/38, Batch Loss: 318267.3750\n",
            "Epoch 180, Batch 12/38, Batch Loss: 255329.3750\n",
            "Epoch 180, Batch 13/38, Batch Loss: 241207.7969\n",
            "Epoch 180, Batch 14/38, Batch Loss: 221447.2812\n",
            "Epoch 180, Batch 15/38, Batch Loss: 331319.1875\n",
            "Epoch 180, Batch 16/38, Batch Loss: 705977.0625\n",
            "Epoch 180, Batch 17/38, Batch Loss: 301192.6562\n",
            "Epoch 180, Batch 18/38, Batch Loss: 238346.1250\n",
            "Epoch 180, Batch 19/38, Batch Loss: 449005.5000\n",
            "Epoch 180, Batch 20/38, Batch Loss: 296307.6250\n",
            "Epoch 180, Batch 21/38, Batch Loss: 379630.5938\n",
            "Epoch 180, Batch 22/38, Batch Loss: 252480.2188\n",
            "Epoch 180, Batch 23/38, Batch Loss: 305240.8125\n",
            "Epoch 180, Batch 24/38, Batch Loss: 350822.6875\n",
            "Epoch 180, Batch 25/38, Batch Loss: 921420.7500\n",
            "Epoch 180, Batch 26/38, Batch Loss: 712529.2500\n",
            "Epoch 180, Batch 27/38, Batch Loss: 647228.3125\n",
            "Epoch 180, Batch 28/38, Batch Loss: 303106.2500\n",
            "Epoch 180, Batch 29/38, Batch Loss: 645911.0625\n",
            "Epoch 180, Batch 30/38, Batch Loss: 224225.6719\n",
            "Epoch 180, Batch 31/38, Batch Loss: 321068.0000\n",
            "Epoch 180, Batch 32/38, Batch Loss: 234563.2656\n",
            "Epoch 180, Batch 33/38, Batch Loss: 265644.2500\n",
            "Epoch 180, Batch 34/38, Batch Loss: 269862.6562\n",
            "Epoch 180, Batch 35/38, Batch Loss: 210336.1562\n",
            "Epoch 180, Batch 36/38, Batch Loss: 316275.2188\n",
            "Epoch 180, Batch 37/38, Batch Loss: 352709.7500\n",
            "Epoch 180, Batch 38/38, Batch Loss: 275727.8750\n",
            "Batch 1/10, Batch Loss: 338197.1250\n",
            "Batch 2/10, Batch Loss: 308574.3438\n",
            "Batch 3/10, Batch Loss: 255610.0156\n",
            "Batch 4/10, Batch Loss: 231553.2969\n",
            "Batch 5/10, Batch Loss: 203092.8438\n",
            "Batch 6/10, Batch Loss: 249303.6094\n",
            "Batch 7/10, Batch Loss: 461594.6875\n",
            "Batch 8/10, Batch Loss: 343121.3125\n",
            "Batch 9/10, Batch Loss: 215937.0000\n",
            "Batch 10/10, Batch Loss: 257042.5469\n",
            "Test Error: \n",
            " Avg loss: 286402.678125, Avg error: 194538.673333 \n",
            "\n",
            "2024-09-14 20:55:45.547385 Epoch 180, Average Training loss 352734.49095394736\n",
            "Epoch 181, Batch 1/38, Batch Loss: 297644.2188\n",
            "Epoch 181, Batch 2/38, Batch Loss: 248995.0312\n",
            "Epoch 181, Batch 3/38, Batch Loss: 258610.8281\n",
            "Epoch 181, Batch 4/38, Batch Loss: 276801.2812\n",
            "Epoch 181, Batch 5/38, Batch Loss: 281743.8438\n",
            "Epoch 181, Batch 6/38, Batch Loss: 687854.3750\n",
            "Epoch 181, Batch 7/38, Batch Loss: 366668.3438\n",
            "Epoch 181, Batch 8/38, Batch Loss: 349113.7188\n",
            "Epoch 181, Batch 9/38, Batch Loss: 627519.9375\n",
            "Epoch 181, Batch 10/38, Batch Loss: 284890.5938\n",
            "Epoch 181, Batch 11/38, Batch Loss: 289396.6562\n",
            "Epoch 181, Batch 12/38, Batch Loss: 253090.5000\n",
            "Epoch 181, Batch 13/38, Batch Loss: 260007.3750\n",
            "Epoch 181, Batch 14/38, Batch Loss: 325467.1875\n",
            "Epoch 181, Batch 15/38, Batch Loss: 359430.2500\n",
            "Epoch 181, Batch 16/38, Batch Loss: 241202.1719\n",
            "Epoch 181, Batch 17/38, Batch Loss: 183224.7344\n",
            "Epoch 181, Batch 18/38, Batch Loss: 341666.1875\n",
            "Epoch 181, Batch 19/38, Batch Loss: 233460.6406\n",
            "Epoch 181, Batch 20/38, Batch Loss: 229886.5156\n",
            "Epoch 181, Batch 21/38, Batch Loss: 670448.3750\n",
            "Epoch 181, Batch 22/38, Batch Loss: 253698.7188\n",
            "Epoch 181, Batch 23/38, Batch Loss: 369339.1562\n",
            "Epoch 181, Batch 24/38, Batch Loss: 361954.7812\n",
            "Epoch 181, Batch 25/38, Batch Loss: 270064.3438\n",
            "Epoch 181, Batch 26/38, Batch Loss: 313134.4375\n",
            "Epoch 181, Batch 27/38, Batch Loss: 314210.8750\n",
            "Epoch 181, Batch 28/38, Batch Loss: 297362.4062\n",
            "Epoch 181, Batch 29/38, Batch Loss: 420172.0625\n",
            "Epoch 181, Batch 30/38, Batch Loss: 702188.4375\n",
            "Epoch 181, Batch 31/38, Batch Loss: 702565.3750\n",
            "Epoch 181, Batch 32/38, Batch Loss: 652271.5625\n",
            "Epoch 181, Batch 33/38, Batch Loss: 347814.9688\n",
            "Epoch 181, Batch 34/38, Batch Loss: 258804.9688\n",
            "Epoch 181, Batch 35/38, Batch Loss: 627636.6250\n",
            "Epoch 181, Batch 36/38, Batch Loss: 239793.9531\n",
            "Epoch 181, Batch 37/38, Batch Loss: 280632.1250\n",
            "Epoch 181, Batch 38/38, Batch Loss: 211777.1875\n",
            "Batch 1/10, Batch Loss: 344579.4062\n",
            "Batch 2/10, Batch Loss: 291322.5000\n",
            "Batch 3/10, Batch Loss: 311484.8750\n",
            "Batch 4/10, Batch Loss: 328106.1562\n",
            "Batch 5/10, Batch Loss: 306254.2812\n",
            "Batch 6/10, Batch Loss: 207146.2656\n",
            "Batch 7/10, Batch Loss: 272153.0312\n",
            "Batch 8/10, Batch Loss: 305476.5625\n",
            "Batch 9/10, Batch Loss: 383092.5625\n",
            "Batch 10/10, Batch Loss: 179679.5156\n",
            "Test Error: \n",
            " Avg loss: 292929.515625, Avg error: 193193.816667 \n",
            "\n",
            "Epoch 182, Batch 1/38, Batch Loss: 297370.0938\n",
            "Epoch 182, Batch 2/38, Batch Loss: 655123.3750\n",
            "Epoch 182, Batch 3/38, Batch Loss: 246122.5312\n",
            "Epoch 182, Batch 4/38, Batch Loss: 247653.6094\n",
            "Epoch 182, Batch 5/38, Batch Loss: 258012.0156\n",
            "Epoch 182, Batch 6/38, Batch Loss: 357441.3125\n",
            "Epoch 182, Batch 7/38, Batch Loss: 644936.1250\n",
            "Epoch 182, Batch 8/38, Batch Loss: 288331.7500\n",
            "Epoch 182, Batch 9/38, Batch Loss: 261533.2500\n",
            "Epoch 182, Batch 10/38, Batch Loss: 422041.3438\n",
            "Epoch 182, Batch 11/38, Batch Loss: 315722.6875\n",
            "Epoch 182, Batch 12/38, Batch Loss: 327584.5938\n",
            "Epoch 182, Batch 13/38, Batch Loss: 263339.2812\n",
            "Epoch 182, Batch 14/38, Batch Loss: 220698.6562\n",
            "Epoch 182, Batch 15/38, Batch Loss: 656651.6875\n",
            "Epoch 182, Batch 16/38, Batch Loss: 292642.8125\n",
            "Epoch 182, Batch 17/38, Batch Loss: 367738.1562\n",
            "Epoch 182, Batch 18/38, Batch Loss: 628296.0000\n",
            "Epoch 182, Batch 19/38, Batch Loss: 297960.2188\n",
            "Epoch 182, Batch 20/38, Batch Loss: 270630.9688\n",
            "Epoch 182, Batch 21/38, Batch Loss: 679366.9375\n",
            "Epoch 182, Batch 22/38, Batch Loss: 364870.8438\n",
            "Epoch 182, Batch 23/38, Batch Loss: 297271.6562\n",
            "Epoch 182, Batch 24/38, Batch Loss: 265669.7500\n",
            "Epoch 182, Batch 25/38, Batch Loss: 261020.4062\n",
            "Epoch 182, Batch 26/38, Batch Loss: 242293.4688\n",
            "Epoch 182, Batch 27/38, Batch Loss: 680884.1250\n",
            "Epoch 182, Batch 28/38, Batch Loss: 264717.5625\n",
            "Epoch 182, Batch 29/38, Batch Loss: 303876.8750\n",
            "Epoch 182, Batch 30/38, Batch Loss: 331278.5625\n",
            "Epoch 182, Batch 31/38, Batch Loss: 249620.0469\n",
            "Epoch 182, Batch 32/38, Batch Loss: 712626.8750\n",
            "Epoch 182, Batch 33/38, Batch Loss: 352391.5938\n",
            "Epoch 182, Batch 34/38, Batch Loss: 285123.3750\n",
            "Epoch 182, Batch 35/38, Batch Loss: 350043.5000\n",
            "Epoch 182, Batch 36/38, Batch Loss: 216692.7812\n",
            "Epoch 182, Batch 37/38, Batch Loss: 381567.9375\n",
            "Epoch 182, Batch 38/38, Batch Loss: 299005.5312\n",
            "Batch 1/10, Batch Loss: 197812.5156\n",
            "Batch 2/10, Batch Loss: 379638.9062\n",
            "Batch 3/10, Batch Loss: 310119.9062\n",
            "Batch 4/10, Batch Loss: 286505.2812\n",
            "Batch 5/10, Batch Loss: 264778.5625\n",
            "Batch 6/10, Batch Loss: 347925.6875\n",
            "Batch 7/10, Batch Loss: 285574.8125\n",
            "Batch 8/10, Batch Loss: 357898.7812\n",
            "Batch 9/10, Batch Loss: 225008.8438\n",
            "Batch 10/10, Batch Loss: 216860.0000\n",
            "Test Error: \n",
            " Avg loss: 287212.329688, Avg error: 194745.744167 \n",
            "\n",
            "Epoch 183, Batch 1/38, Batch Loss: 323629.9688\n",
            "Epoch 183, Batch 2/38, Batch Loss: 214627.4062\n",
            "Epoch 183, Batch 3/38, Batch Loss: 235397.3750\n",
            "Epoch 183, Batch 4/38, Batch Loss: 236365.4531\n",
            "Epoch 183, Batch 5/38, Batch Loss: 672042.6875\n",
            "Epoch 183, Batch 6/38, Batch Loss: 342298.4375\n",
            "Epoch 183, Batch 7/38, Batch Loss: 265441.6562\n",
            "Epoch 183, Batch 8/38, Batch Loss: 213318.5938\n",
            "Epoch 183, Batch 9/38, Batch Loss: 213096.3750\n",
            "Epoch 183, Batch 10/38, Batch Loss: 280435.0625\n",
            "Epoch 183, Batch 11/38, Batch Loss: 363900.3438\n",
            "Epoch 183, Batch 12/38, Batch Loss: 209245.3438\n",
            "Epoch 183, Batch 13/38, Batch Loss: 261166.9062\n",
            "Epoch 183, Batch 14/38, Batch Loss: 247110.6719\n",
            "Epoch 183, Batch 15/38, Batch Loss: 679215.4375\n",
            "Epoch 183, Batch 16/38, Batch Loss: 237852.9844\n",
            "Epoch 183, Batch 17/38, Batch Loss: 395594.4688\n",
            "Epoch 183, Batch 18/38, Batch Loss: 269783.3750\n",
            "Epoch 183, Batch 19/38, Batch Loss: 652343.8125\n",
            "Epoch 183, Batch 20/38, Batch Loss: 357616.4375\n",
            "Epoch 183, Batch 21/38, Batch Loss: 343506.0312\n",
            "Epoch 183, Batch 22/38, Batch Loss: 280888.2812\n",
            "Epoch 183, Batch 23/38, Batch Loss: 297792.3750\n",
            "Epoch 183, Batch 24/38, Batch Loss: 304361.6250\n",
            "Epoch 183, Batch 25/38, Batch Loss: 276926.5312\n",
            "Epoch 183, Batch 26/38, Batch Loss: 293605.8438\n",
            "Epoch 183, Batch 27/38, Batch Loss: 248147.2031\n",
            "Epoch 183, Batch 28/38, Batch Loss: 257546.8125\n",
            "Epoch 183, Batch 29/38, Batch Loss: 288752.0625\n",
            "Epoch 183, Batch 30/38, Batch Loss: 676249.3750\n",
            "Epoch 183, Batch 31/38, Batch Loss: 762659.0000\n",
            "Epoch 183, Batch 32/38, Batch Loss: 264978.0312\n",
            "Epoch 183, Batch 33/38, Batch Loss: 239139.5469\n",
            "Epoch 183, Batch 34/38, Batch Loss: 258617.0781\n",
            "Epoch 183, Batch 35/38, Batch Loss: 300599.8438\n",
            "Epoch 183, Batch 36/38, Batch Loss: 355596.9375\n",
            "Epoch 183, Batch 37/38, Batch Loss: 692697.2500\n",
            "Epoch 183, Batch 38/38, Batch Loss: 968052.2500\n",
            "Batch 1/10, Batch Loss: 267792.5625\n",
            "Batch 2/10, Batch Loss: 269287.3750\n",
            "Batch 3/10, Batch Loss: 213329.0156\n",
            "Batch 4/10, Batch Loss: 249487.6875\n",
            "Batch 5/10, Batch Loss: 356708.9062\n",
            "Batch 6/10, Batch Loss: 264654.7500\n",
            "Batch 7/10, Batch Loss: 348239.0312\n",
            "Batch 8/10, Batch Loss: 283433.5000\n",
            "Batch 9/10, Batch Loss: 326581.7188\n",
            "Batch 10/10, Batch Loss: 415927.2188\n",
            "Test Error: \n",
            " Avg loss: 299544.176563, Avg error: 194171.775000 \n",
            "\n",
            "Epoch 184, Batch 1/38, Batch Loss: 277216.0000\n",
            "Epoch 184, Batch 2/38, Batch Loss: 291455.7812\n",
            "Epoch 184, Batch 3/38, Batch Loss: 303349.4062\n",
            "Epoch 184, Batch 4/38, Batch Loss: 312578.6250\n",
            "Epoch 184, Batch 5/38, Batch Loss: 638220.8750\n",
            "Epoch 184, Batch 6/38, Batch Loss: 649910.2500\n",
            "Epoch 184, Batch 7/38, Batch Loss: 335346.7188\n",
            "Epoch 184, Batch 8/38, Batch Loss: 250424.5938\n",
            "Epoch 184, Batch 9/38, Batch Loss: 280185.5000\n",
            "Epoch 184, Batch 10/38, Batch Loss: 310420.5312\n",
            "Epoch 184, Batch 11/38, Batch Loss: 222043.0625\n",
            "Epoch 184, Batch 12/38, Batch Loss: 310995.7188\n",
            "Epoch 184, Batch 13/38, Batch Loss: 675208.4375\n",
            "Epoch 184, Batch 14/38, Batch Loss: 391155.0938\n",
            "Epoch 184, Batch 15/38, Batch Loss: 196432.4375\n",
            "Epoch 184, Batch 16/38, Batch Loss: 234463.9531\n",
            "Epoch 184, Batch 17/38, Batch Loss: 311921.3125\n",
            "Epoch 184, Batch 18/38, Batch Loss: 711961.9375\n",
            "Epoch 184, Batch 19/38, Batch Loss: 307223.9062\n",
            "Epoch 184, Batch 20/38, Batch Loss: 934022.2500\n",
            "Epoch 184, Batch 21/38, Batch Loss: 728322.8750\n",
            "Epoch 184, Batch 22/38, Batch Loss: 284314.8125\n",
            "Epoch 184, Batch 23/38, Batch Loss: 313487.3750\n",
            "Epoch 184, Batch 24/38, Batch Loss: 237752.2344\n",
            "Epoch 184, Batch 25/38, Batch Loss: 328856.4375\n",
            "Epoch 184, Batch 26/38, Batch Loss: 395284.9375\n",
            "Epoch 184, Batch 27/38, Batch Loss: 366558.8750\n",
            "Epoch 184, Batch 28/38, Batch Loss: 262535.3125\n",
            "Epoch 184, Batch 29/38, Batch Loss: 240259.9844\n",
            "Epoch 184, Batch 30/38, Batch Loss: 367408.7812\n",
            "Epoch 184, Batch 31/38, Batch Loss: 308727.0312\n",
            "Epoch 184, Batch 32/38, Batch Loss: 425905.3125\n",
            "Epoch 184, Batch 33/38, Batch Loss: 219448.7969\n",
            "Epoch 184, Batch 34/38, Batch Loss: 200410.3281\n",
            "Epoch 184, Batch 35/38, Batch Loss: 329980.6250\n",
            "Epoch 184, Batch 36/38, Batch Loss: 233696.0781\n",
            "Epoch 184, Batch 37/38, Batch Loss: 269075.0312\n",
            "Epoch 184, Batch 38/38, Batch Loss: 515630.3125\n",
            "Batch 1/10, Batch Loss: 282837.8438\n",
            "Batch 2/10, Batch Loss: 465553.4062\n",
            "Batch 3/10, Batch Loss: 366175.6875\n",
            "Batch 4/10, Batch Loss: 284547.8750\n",
            "Batch 5/10, Batch Loss: 238572.3125\n",
            "Batch 6/10, Batch Loss: 332141.8125\n",
            "Batch 7/10, Batch Loss: 289902.2188\n",
            "Batch 8/10, Batch Loss: 282589.5000\n",
            "Batch 9/10, Batch Loss: 230420.6250\n",
            "Batch 10/10, Batch Loss: 500951.5312\n",
            "Test Error: \n",
            " Avg loss: 327369.281250, Avg error: 204396.536667 \n",
            "\n",
            "Epoch 185, Batch 1/38, Batch Loss: 257483.2969\n",
            "Epoch 185, Batch 2/38, Batch Loss: 285277.3438\n",
            "Epoch 185, Batch 3/38, Batch Loss: 685476.9375\n",
            "Epoch 185, Batch 4/38, Batch Loss: 217224.8906\n",
            "Epoch 185, Batch 5/38, Batch Loss: 666433.8750\n",
            "Epoch 185, Batch 6/38, Batch Loss: 294684.9688\n",
            "Epoch 185, Batch 7/38, Batch Loss: 330458.8125\n",
            "Epoch 185, Batch 8/38, Batch Loss: 695819.5625\n",
            "Epoch 185, Batch 9/38, Batch Loss: 276442.6875\n",
            "Epoch 185, Batch 10/38, Batch Loss: 278975.5000\n",
            "Epoch 185, Batch 11/38, Batch Loss: 249280.8594\n",
            "Epoch 185, Batch 12/38, Batch Loss: 293755.2812\n",
            "Epoch 185, Batch 13/38, Batch Loss: 317928.5938\n",
            "Epoch 185, Batch 14/38, Batch Loss: 303426.3750\n",
            "Epoch 185, Batch 15/38, Batch Loss: 274391.2500\n",
            "Epoch 185, Batch 16/38, Batch Loss: 269753.0938\n",
            "Epoch 185, Batch 17/38, Batch Loss: 284616.3438\n",
            "Epoch 185, Batch 18/38, Batch Loss: 273524.2500\n",
            "Epoch 185, Batch 19/38, Batch Loss: 323938.5000\n",
            "Epoch 185, Batch 20/38, Batch Loss: 373255.1875\n",
            "Epoch 185, Batch 21/38, Batch Loss: 267413.0625\n",
            "Epoch 185, Batch 22/38, Batch Loss: 241307.7500\n",
            "Epoch 185, Batch 23/38, Batch Loss: 310283.7812\n",
            "Epoch 185, Batch 24/38, Batch Loss: 666821.1250\n",
            "Epoch 185, Batch 25/38, Batch Loss: 271357.5625\n",
            "Epoch 185, Batch 26/38, Batch Loss: 295267.0938\n",
            "Epoch 185, Batch 27/38, Batch Loss: 309038.6250\n",
            "Epoch 185, Batch 28/38, Batch Loss: 933336.2500\n",
            "Epoch 185, Batch 29/38, Batch Loss: 384256.7188\n",
            "Epoch 185, Batch 30/38, Batch Loss: 242154.0781\n",
            "Epoch 185, Batch 31/38, Batch Loss: 271064.0000\n",
            "Epoch 185, Batch 32/38, Batch Loss: 257685.8594\n",
            "Epoch 185, Batch 33/38, Batch Loss: 679222.8125\n",
            "Epoch 185, Batch 34/38, Batch Loss: 337041.7188\n",
            "Epoch 185, Batch 35/38, Batch Loss: 258488.7969\n",
            "Epoch 185, Batch 36/38, Batch Loss: 255898.5156\n",
            "Epoch 185, Batch 37/38, Batch Loss: 194848.1406\n",
            "Epoch 185, Batch 38/38, Batch Loss: 200118.0469\n",
            "Batch 1/10, Batch Loss: 336962.6250\n",
            "Batch 2/10, Batch Loss: 392513.8438\n",
            "Batch 3/10, Batch Loss: 259485.1094\n",
            "Batch 4/10, Batch Loss: 348893.9688\n",
            "Batch 5/10, Batch Loss: 260105.5781\n",
            "Batch 6/10, Batch Loss: 253303.3438\n",
            "Batch 7/10, Batch Loss: 324986.3438\n",
            "Batch 8/10, Batch Loss: 295362.5938\n",
            "Batch 9/10, Batch Loss: 302732.1875\n",
            "Batch 10/10, Batch Loss: 246756.3750\n",
            "Test Error: \n",
            " Avg loss: 302110.196875, Avg error: 195172.200000 \n",
            "\n",
            "Epoch 186, Batch 1/38, Batch Loss: 272748.8750\n",
            "Epoch 186, Batch 2/38, Batch Loss: 287105.4062\n",
            "Epoch 186, Batch 3/38, Batch Loss: 667268.2500\n",
            "Epoch 186, Batch 4/38, Batch Loss: 227849.2812\n",
            "Epoch 186, Batch 5/38, Batch Loss: 348269.9062\n",
            "Epoch 186, Batch 6/38, Batch Loss: 321008.3438\n",
            "Epoch 186, Batch 7/38, Batch Loss: 267956.3438\n",
            "Epoch 186, Batch 8/38, Batch Loss: 258135.2812\n",
            "Epoch 186, Batch 9/38, Batch Loss: 261322.7969\n",
            "Epoch 186, Batch 10/38, Batch Loss: 199484.9688\n",
            "Epoch 186, Batch 11/38, Batch Loss: 257758.5156\n",
            "Epoch 186, Batch 12/38, Batch Loss: 277834.6562\n",
            "Epoch 186, Batch 13/38, Batch Loss: 266969.4375\n",
            "Epoch 186, Batch 14/38, Batch Loss: 261208.8281\n",
            "Epoch 186, Batch 15/38, Batch Loss: 341636.1562\n",
            "Epoch 186, Batch 16/38, Batch Loss: 307982.4375\n",
            "Epoch 186, Batch 17/38, Batch Loss: 690353.9375\n",
            "Epoch 186, Batch 18/38, Batch Loss: 652949.6250\n",
            "Epoch 186, Batch 19/38, Batch Loss: 256967.0781\n",
            "Epoch 186, Batch 20/38, Batch Loss: 292176.2188\n",
            "Epoch 186, Batch 21/38, Batch Loss: 336706.0938\n",
            "Epoch 186, Batch 22/38, Batch Loss: 252324.2500\n",
            "Epoch 186, Batch 23/38, Batch Loss: 265986.9375\n",
            "Epoch 186, Batch 24/38, Batch Loss: 243849.5625\n",
            "Epoch 186, Batch 25/38, Batch Loss: 230430.7031\n",
            "Epoch 186, Batch 26/38, Batch Loss: 964851.1250\n",
            "Epoch 186, Batch 27/38, Batch Loss: 348401.3438\n",
            "Epoch 186, Batch 28/38, Batch Loss: 653316.9375\n",
            "Epoch 186, Batch 29/38, Batch Loss: 195502.3281\n",
            "Epoch 186, Batch 30/38, Batch Loss: 296973.7500\n",
            "Epoch 186, Batch 31/38, Batch Loss: 383332.8438\n",
            "Epoch 186, Batch 32/38, Batch Loss: 282342.1562\n",
            "Epoch 186, Batch 33/38, Batch Loss: 315418.7188\n",
            "Epoch 186, Batch 34/38, Batch Loss: 262969.0625\n",
            "Epoch 186, Batch 35/38, Batch Loss: 729758.5000\n",
            "Epoch 186, Batch 36/38, Batch Loss: 239816.4219\n",
            "Epoch 186, Batch 37/38, Batch Loss: 312740.8750\n",
            "Epoch 186, Batch 38/38, Batch Loss: 207274.2031\n",
            "Batch 1/10, Batch Loss: 209187.1562\n",
            "Batch 2/10, Batch Loss: 370922.3125\n",
            "Batch 3/10, Batch Loss: 227451.6406\n",
            "Batch 4/10, Batch Loss: 323498.9062\n",
            "Batch 5/10, Batch Loss: 286492.9062\n",
            "Batch 6/10, Batch Loss: 222761.6875\n",
            "Batch 7/10, Batch Loss: 215727.9844\n",
            "Batch 8/10, Batch Loss: 536437.8750\n",
            "Batch 9/10, Batch Loss: 259794.2344\n",
            "Batch 10/10, Batch Loss: 227825.8125\n",
            "Test Error: \n",
            " Avg loss: 288010.051563, Avg error: 192234.616667 \n",
            "\n",
            "Epoch 187, Batch 1/38, Batch Loss: 347211.3125\n",
            "Epoch 187, Batch 2/38, Batch Loss: 675631.5000\n",
            "Epoch 187, Batch 3/38, Batch Loss: 298911.3750\n",
            "Epoch 187, Batch 4/38, Batch Loss: 733258.0625\n",
            "Epoch 187, Batch 5/38, Batch Loss: 221703.9219\n",
            "Epoch 187, Batch 6/38, Batch Loss: 252280.2344\n",
            "Epoch 187, Batch 7/38, Batch Loss: 222440.3906\n",
            "Epoch 187, Batch 8/38, Batch Loss: 237432.5781\n",
            "Epoch 187, Batch 9/38, Batch Loss: 194516.4844\n",
            "Epoch 187, Batch 10/38, Batch Loss: 311543.2500\n",
            "Epoch 187, Batch 11/38, Batch Loss: 361385.1250\n",
            "Epoch 187, Batch 12/38, Batch Loss: 244697.7344\n",
            "Epoch 187, Batch 13/38, Batch Loss: 258171.8906\n",
            "Epoch 187, Batch 14/38, Batch Loss: 386316.5938\n",
            "Epoch 187, Batch 15/38, Batch Loss: 247198.3125\n",
            "Epoch 187, Batch 16/38, Batch Loss: 341605.0625\n",
            "Epoch 187, Batch 17/38, Batch Loss: 416234.6250\n",
            "Epoch 187, Batch 18/38, Batch Loss: 257519.2656\n",
            "Epoch 187, Batch 19/38, Batch Loss: 238344.2500\n",
            "Epoch 187, Batch 20/38, Batch Loss: 377985.9375\n",
            "Epoch 187, Batch 21/38, Batch Loss: 644136.5000\n",
            "Epoch 187, Batch 22/38, Batch Loss: 308334.2500\n",
            "Epoch 187, Batch 23/38, Batch Loss: 340037.0625\n",
            "Epoch 187, Batch 24/38, Batch Loss: 270909.0000\n",
            "Epoch 187, Batch 25/38, Batch Loss: 653738.6250\n",
            "Epoch 187, Batch 26/38, Batch Loss: 343384.2188\n",
            "Epoch 187, Batch 27/38, Batch Loss: 722304.0000\n",
            "Epoch 187, Batch 28/38, Batch Loss: 320651.7188\n",
            "Epoch 187, Batch 29/38, Batch Loss: 248725.8906\n",
            "Epoch 187, Batch 30/38, Batch Loss: 236717.3906\n",
            "Epoch 187, Batch 31/38, Batch Loss: 669740.8750\n",
            "Epoch 187, Batch 32/38, Batch Loss: 315631.1875\n",
            "Epoch 187, Batch 33/38, Batch Loss: 269256.4688\n",
            "Epoch 187, Batch 34/38, Batch Loss: 218114.5625\n",
            "Epoch 187, Batch 35/38, Batch Loss: 297845.2188\n",
            "Epoch 187, Batch 36/38, Batch Loss: 311633.6250\n",
            "Epoch 187, Batch 37/38, Batch Loss: 672011.3125\n",
            "Epoch 187, Batch 38/38, Batch Loss: 282164.8438\n",
            "Batch 1/10, Batch Loss: 246367.0000\n",
            "Batch 2/10, Batch Loss: 264249.9375\n",
            "Batch 3/10, Batch Loss: 315566.5312\n",
            "Batch 4/10, Batch Loss: 378018.3125\n",
            "Batch 5/10, Batch Loss: 297596.9375\n",
            "Batch 6/10, Batch Loss: 324530.7188\n",
            "Batch 7/10, Batch Loss: 238900.3750\n",
            "Batch 8/10, Batch Loss: 284634.4062\n",
            "Batch 9/10, Batch Loss: 369919.3750\n",
            "Batch 10/10, Batch Loss: 329583.6562\n",
            "Test Error: \n",
            " Avg loss: 304936.725000, Avg error: 192493.951667 \n",
            "\n",
            "Epoch 188, Batch 1/38, Batch Loss: 291052.6875\n",
            "Epoch 188, Batch 2/38, Batch Loss: 673972.3750\n",
            "Epoch 188, Batch 3/38, Batch Loss: 290091.6875\n",
            "Epoch 188, Batch 4/38, Batch Loss: 259945.1094\n",
            "Epoch 188, Batch 5/38, Batch Loss: 204733.7500\n",
            "Epoch 188, Batch 6/38, Batch Loss: 724306.5000\n",
            "Epoch 188, Batch 7/38, Batch Loss: 678088.0000\n",
            "Epoch 188, Batch 8/38, Batch Loss: 700055.3125\n",
            "Epoch 188, Batch 9/38, Batch Loss: 237020.9219\n",
            "Epoch 188, Batch 10/38, Batch Loss: 419119.3750\n",
            "Epoch 188, Batch 11/38, Batch Loss: 277484.4688\n",
            "Epoch 188, Batch 12/38, Batch Loss: 184094.1406\n",
            "Epoch 188, Batch 13/38, Batch Loss: 246557.5469\n",
            "Epoch 188, Batch 14/38, Batch Loss: 290672.4688\n",
            "Epoch 188, Batch 15/38, Batch Loss: 254344.8750\n",
            "Epoch 188, Batch 16/38, Batch Loss: 300488.9375\n",
            "Epoch 188, Batch 17/38, Batch Loss: 308293.1875\n",
            "Epoch 188, Batch 18/38, Batch Loss: 288392.5000\n",
            "Epoch 188, Batch 19/38, Batch Loss: 229263.5625\n",
            "Epoch 188, Batch 20/38, Batch Loss: 287372.5625\n",
            "Epoch 188, Batch 21/38, Batch Loss: 345291.7500\n",
            "Epoch 188, Batch 22/38, Batch Loss: 264210.3125\n",
            "Epoch 188, Batch 23/38, Batch Loss: 264315.6562\n",
            "Epoch 188, Batch 24/38, Batch Loss: 267182.7812\n",
            "Epoch 188, Batch 25/38, Batch Loss: 236803.0938\n",
            "Epoch 188, Batch 26/38, Batch Loss: 261631.4531\n",
            "Epoch 188, Batch 27/38, Batch Loss: 221764.5312\n",
            "Epoch 188, Batch 28/38, Batch Loss: 342387.2188\n",
            "Epoch 188, Batch 29/38, Batch Loss: 261252.3438\n",
            "Epoch 188, Batch 30/38, Batch Loss: 303747.0312\n",
            "Epoch 188, Batch 31/38, Batch Loss: 660876.8125\n",
            "Epoch 188, Batch 32/38, Batch Loss: 212844.9375\n",
            "Epoch 188, Batch 33/38, Batch Loss: 270046.3438\n",
            "Epoch 188, Batch 34/38, Batch Loss: 345222.4688\n",
            "Epoch 188, Batch 35/38, Batch Loss: 689614.5000\n",
            "Epoch 188, Batch 36/38, Batch Loss: 719816.2500\n",
            "Epoch 188, Batch 37/38, Batch Loss: 358242.0938\n",
            "Epoch 188, Batch 38/38, Batch Loss: 310111.5000\n",
            "Batch 1/10, Batch Loss: 317618.3438\n",
            "Batch 2/10, Batch Loss: 250853.9688\n",
            "Batch 3/10, Batch Loss: 354888.8750\n",
            "Batch 4/10, Batch Loss: 281717.0000\n",
            "Batch 5/10, Batch Loss: 215804.4688\n",
            "Batch 6/10, Batch Loss: 304052.8125\n",
            "Batch 7/10, Batch Loss: 330564.5938\n",
            "Batch 8/10, Batch Loss: 318128.7500\n",
            "Batch 9/10, Batch Loss: 303134.0938\n",
            "Batch 10/10, Batch Loss: 166640.2188\n",
            "Test Error: \n",
            " Avg loss: 284340.312500, Avg error: 189354.992083 \n",
            "\n",
            "Epoch 189, Batch 1/38, Batch Loss: 281003.7188\n",
            "Epoch 189, Batch 2/38, Batch Loss: 313073.0625\n",
            "Epoch 189, Batch 3/38, Batch Loss: 249598.1094\n",
            "Epoch 189, Batch 4/38, Batch Loss: 239334.1250\n",
            "Epoch 189, Batch 5/38, Batch Loss: 375420.2812\n",
            "Epoch 189, Batch 6/38, Batch Loss: 288305.5312\n",
            "Epoch 189, Batch 7/38, Batch Loss: 193245.8750\n",
            "Epoch 189, Batch 8/38, Batch Loss: 353790.5625\n",
            "Epoch 189, Batch 9/38, Batch Loss: 261216.2344\n",
            "Epoch 189, Batch 10/38, Batch Loss: 721051.7500\n",
            "Epoch 189, Batch 11/38, Batch Loss: 681467.1875\n",
            "Epoch 189, Batch 12/38, Batch Loss: 235710.7188\n",
            "Epoch 189, Batch 13/38, Batch Loss: 282611.5625\n",
            "Epoch 189, Batch 14/38, Batch Loss: 235839.5469\n",
            "Epoch 189, Batch 15/38, Batch Loss: 326074.8125\n",
            "Epoch 189, Batch 16/38, Batch Loss: 643331.1875\n",
            "Epoch 189, Batch 17/38, Batch Loss: 674602.7500\n",
            "Epoch 189, Batch 18/38, Batch Loss: 248770.7969\n",
            "Epoch 189, Batch 19/38, Batch Loss: 293630.2500\n",
            "Epoch 189, Batch 20/38, Batch Loss: 684031.3125\n",
            "Epoch 189, Batch 21/38, Batch Loss: 354715.4062\n",
            "Epoch 189, Batch 22/38, Batch Loss: 701708.6875\n",
            "Epoch 189, Batch 23/38, Batch Loss: 678319.6875\n",
            "Epoch 189, Batch 24/38, Batch Loss: 234218.4688\n",
            "Epoch 189, Batch 25/38, Batch Loss: 281837.1562\n",
            "Epoch 189, Batch 26/38, Batch Loss: 315646.6250\n",
            "Epoch 189, Batch 27/38, Batch Loss: 287534.7188\n",
            "Epoch 189, Batch 28/38, Batch Loss: 271401.9375\n",
            "Epoch 189, Batch 29/38, Batch Loss: 281227.0312\n",
            "Epoch 189, Batch 30/38, Batch Loss: 341696.4062\n",
            "Epoch 189, Batch 31/38, Batch Loss: 342200.4062\n",
            "Epoch 189, Batch 32/38, Batch Loss: 248995.3281\n",
            "Epoch 189, Batch 33/38, Batch Loss: 200562.4375\n",
            "Epoch 189, Batch 34/38, Batch Loss: 262199.7188\n",
            "Epoch 189, Batch 35/38, Batch Loss: 277267.4688\n",
            "Epoch 189, Batch 36/38, Batch Loss: 386659.4062\n",
            "Epoch 189, Batch 37/38, Batch Loss: 226601.5938\n",
            "Epoch 189, Batch 38/38, Batch Loss: 333806.6562\n",
            "Batch 1/10, Batch Loss: 219304.2656\n",
            "Batch 2/10, Batch Loss: 250999.4219\n",
            "Batch 3/10, Batch Loss: 355207.7812\n",
            "Batch 4/10, Batch Loss: 330555.8438\n",
            "Batch 5/10, Batch Loss: 321063.9062\n",
            "Batch 6/10, Batch Loss: 287867.9375\n",
            "Batch 7/10, Batch Loss: 250891.5000\n",
            "Batch 8/10, Batch Loss: 458338.1250\n",
            "Batch 9/10, Batch Loss: 326655.5938\n",
            "Batch 10/10, Batch Loss: 142120.8906\n",
            "Test Error: \n",
            " Avg loss: 294300.526562, Avg error: 193118.530833 \n",
            "\n",
            "Epoch 190, Batch 1/38, Batch Loss: 637438.1875\n",
            "Epoch 190, Batch 2/38, Batch Loss: 267763.9375\n",
            "Epoch 190, Batch 3/38, Batch Loss: 725776.2500\n",
            "Epoch 190, Batch 4/38, Batch Loss: 277219.5000\n",
            "Epoch 190, Batch 5/38, Batch Loss: 330237.0938\n",
            "Epoch 190, Batch 6/38, Batch Loss: 302562.5625\n",
            "Epoch 190, Batch 7/38, Batch Loss: 221346.1094\n",
            "Epoch 190, Batch 8/38, Batch Loss: 381483.6562\n",
            "Epoch 190, Batch 9/38, Batch Loss: 307409.0312\n",
            "Epoch 190, Batch 10/38, Batch Loss: 299277.0312\n",
            "Epoch 190, Batch 11/38, Batch Loss: 370278.2500\n",
            "Epoch 190, Batch 12/38, Batch Loss: 208221.9062\n",
            "Epoch 190, Batch 13/38, Batch Loss: 238611.6406\n",
            "Epoch 190, Batch 14/38, Batch Loss: 216557.8281\n",
            "Epoch 190, Batch 15/38, Batch Loss: 331426.7188\n",
            "Epoch 190, Batch 16/38, Batch Loss: 312376.0625\n",
            "Epoch 190, Batch 17/38, Batch Loss: 320202.1875\n",
            "Epoch 190, Batch 18/38, Batch Loss: 198513.2188\n",
            "Epoch 190, Batch 19/38, Batch Loss: 292911.2812\n",
            "Epoch 190, Batch 20/38, Batch Loss: 331395.9688\n",
            "Epoch 190, Batch 21/38, Batch Loss: 233384.1250\n",
            "Epoch 190, Batch 22/38, Batch Loss: 230535.5469\n",
            "Epoch 190, Batch 23/38, Batch Loss: 316146.4375\n",
            "Epoch 190, Batch 24/38, Batch Loss: 903953.6250\n",
            "Epoch 190, Batch 25/38, Batch Loss: 681155.2500\n",
            "Epoch 190, Batch 26/38, Batch Loss: 311123.5625\n",
            "Epoch 190, Batch 27/38, Batch Loss: 242646.3750\n",
            "Epoch 190, Batch 28/38, Batch Loss: 685415.3750\n",
            "Epoch 190, Batch 29/38, Batch Loss: 279242.3750\n",
            "Epoch 190, Batch 30/38, Batch Loss: 291824.2500\n",
            "Epoch 190, Batch 31/38, Batch Loss: 254916.1562\n",
            "Epoch 190, Batch 32/38, Batch Loss: 652342.3125\n",
            "Epoch 190, Batch 33/38, Batch Loss: 262026.7969\n",
            "Epoch 190, Batch 34/38, Batch Loss: 223879.8125\n",
            "Epoch 190, Batch 35/38, Batch Loss: 281110.0938\n",
            "Epoch 190, Batch 36/38, Batch Loss: 238424.2656\n",
            "Epoch 190, Batch 37/38, Batch Loss: 346863.9062\n",
            "Epoch 190, Batch 38/38, Batch Loss: 268578.2812\n",
            "Batch 1/10, Batch Loss: 436187.5000\n",
            "Batch 2/10, Batch Loss: 321758.9688\n",
            "Batch 3/10, Batch Loss: 460058.5938\n",
            "Batch 4/10, Batch Loss: 328365.5312\n",
            "Batch 5/10, Batch Loss: 353332.6562\n",
            "Batch 6/10, Batch Loss: 304197.8438\n",
            "Batch 7/10, Batch Loss: 262965.2812\n",
            "Batch 8/10, Batch Loss: 283960.9062\n",
            "Batch 9/10, Batch Loss: 201658.4219\n",
            "Batch 10/10, Batch Loss: 251951.0781\n",
            "Test Error: \n",
            " Avg loss: 320443.678125, Avg error: 207662.226667 \n",
            "\n",
            "2024-09-14 21:02:29.153943 Epoch 190, Average Training loss 349330.9728618421\n",
            "Epoch 191, Batch 1/38, Batch Loss: 683920.9375\n",
            "Epoch 191, Batch 2/38, Batch Loss: 306565.3125\n",
            "Epoch 191, Batch 3/38, Batch Loss: 213318.9688\n",
            "Epoch 191, Batch 4/38, Batch Loss: 231813.0000\n",
            "Epoch 191, Batch 5/38, Batch Loss: 234654.6875\n",
            "Epoch 191, Batch 6/38, Batch Loss: 273421.7812\n",
            "Epoch 191, Batch 7/38, Batch Loss: 695401.1875\n",
            "Epoch 191, Batch 8/38, Batch Loss: 267556.3438\n",
            "Epoch 191, Batch 9/38, Batch Loss: 348234.4688\n",
            "Epoch 191, Batch 10/38, Batch Loss: 342896.7500\n",
            "Epoch 191, Batch 11/38, Batch Loss: 290454.6875\n",
            "Epoch 191, Batch 12/38, Batch Loss: 670176.5000\n",
            "Epoch 191, Batch 13/38, Batch Loss: 272501.1562\n",
            "Epoch 191, Batch 14/38, Batch Loss: 693296.8125\n",
            "Epoch 191, Batch 15/38, Batch Loss: 284943.8125\n",
            "Epoch 191, Batch 16/38, Batch Loss: 291799.7500\n",
            "Epoch 191, Batch 17/38, Batch Loss: 281330.0000\n",
            "Epoch 191, Batch 18/38, Batch Loss: 654887.6250\n",
            "Epoch 191, Batch 19/38, Batch Loss: 317891.0625\n",
            "Epoch 191, Batch 20/38, Batch Loss: 254616.5000\n",
            "Epoch 191, Batch 21/38, Batch Loss: 362982.6250\n",
            "Epoch 191, Batch 22/38, Batch Loss: 347400.7812\n",
            "Epoch 191, Batch 23/38, Batch Loss: 672870.7500\n",
            "Epoch 191, Batch 24/38, Batch Loss: 506073.6562\n",
            "Epoch 191, Batch 25/38, Batch Loss: 211697.4688\n",
            "Epoch 191, Batch 26/38, Batch Loss: 350518.0938\n",
            "Epoch 191, Batch 27/38, Batch Loss: 226724.6719\n",
            "Epoch 191, Batch 28/38, Batch Loss: 221699.8594\n",
            "Epoch 191, Batch 29/38, Batch Loss: 388331.7812\n",
            "Epoch 191, Batch 30/38, Batch Loss: 290015.7812\n",
            "Epoch 191, Batch 31/38, Batch Loss: 258558.3125\n",
            "Epoch 191, Batch 32/38, Batch Loss: 322475.4375\n",
            "Epoch 191, Batch 33/38, Batch Loss: 211355.1250\n",
            "Epoch 191, Batch 34/38, Batch Loss: 659858.6250\n",
            "Epoch 191, Batch 35/38, Batch Loss: 245631.8438\n",
            "Epoch 191, Batch 36/38, Batch Loss: 256900.1406\n",
            "Epoch 191, Batch 37/38, Batch Loss: 258555.9375\n",
            "Epoch 191, Batch 38/38, Batch Loss: 223552.4531\n",
            "Batch 1/10, Batch Loss: 245192.6562\n",
            "Batch 2/10, Batch Loss: 344449.6562\n",
            "Batch 3/10, Batch Loss: 196999.7500\n",
            "Batch 4/10, Batch Loss: 215683.3906\n",
            "Batch 5/10, Batch Loss: 376761.7188\n",
            "Batch 6/10, Batch Loss: 375673.9375\n",
            "Batch 7/10, Batch Loss: 358696.2812\n",
            "Batch 8/10, Batch Loss: 333536.1562\n",
            "Batch 9/10, Batch Loss: 259314.4375\n",
            "Batch 10/10, Batch Loss: 266926.1875\n",
            "Test Error: \n",
            " Avg loss: 297323.417187, Avg error: 192475.581250 \n",
            "\n",
            "Epoch 192, Batch 1/38, Batch Loss: 307118.2812\n",
            "Epoch 192, Batch 2/38, Batch Loss: 235600.6562\n",
            "Epoch 192, Batch 3/38, Batch Loss: 236838.4844\n",
            "Epoch 192, Batch 4/38, Batch Loss: 303620.2500\n",
            "Epoch 192, Batch 5/38, Batch Loss: 275920.7812\n",
            "Epoch 192, Batch 6/38, Batch Loss: 338854.9062\n",
            "Epoch 192, Batch 7/38, Batch Loss: 294354.9375\n",
            "Epoch 192, Batch 8/38, Batch Loss: 268510.5000\n",
            "Epoch 192, Batch 9/38, Batch Loss: 344662.9062\n",
            "Epoch 192, Batch 10/38, Batch Loss: 693266.2500\n",
            "Epoch 192, Batch 11/38, Batch Loss: 193666.4062\n",
            "Epoch 192, Batch 12/38, Batch Loss: 265685.6250\n",
            "Epoch 192, Batch 13/38, Batch Loss: 656412.6875\n",
            "Epoch 192, Batch 14/38, Batch Loss: 285498.0312\n",
            "Epoch 192, Batch 15/38, Batch Loss: 658912.7500\n",
            "Epoch 192, Batch 16/38, Batch Loss: 267651.0312\n",
            "Epoch 192, Batch 17/38, Batch Loss: 960858.4375\n",
            "Epoch 192, Batch 18/38, Batch Loss: 374353.1562\n",
            "Epoch 192, Batch 19/38, Batch Loss: 258241.2344\n",
            "Epoch 192, Batch 20/38, Batch Loss: 236244.8594\n",
            "Epoch 192, Batch 21/38, Batch Loss: 230418.4062\n",
            "Epoch 192, Batch 22/38, Batch Loss: 274134.2188\n",
            "Epoch 192, Batch 23/38, Batch Loss: 306495.3750\n",
            "Epoch 192, Batch 24/38, Batch Loss: 312175.1250\n",
            "Epoch 192, Batch 25/38, Batch Loss: 288152.8750\n",
            "Epoch 192, Batch 26/38, Batch Loss: 273936.0000\n",
            "Epoch 192, Batch 27/38, Batch Loss: 205971.9375\n",
            "Epoch 192, Batch 28/38, Batch Loss: 689709.0625\n",
            "Epoch 192, Batch 29/38, Batch Loss: 278154.2500\n",
            "Epoch 192, Batch 30/38, Batch Loss: 282514.5000\n",
            "Epoch 192, Batch 31/38, Batch Loss: 252460.5312\n",
            "Epoch 192, Batch 32/38, Batch Loss: 220954.3281\n",
            "Epoch 192, Batch 33/38, Batch Loss: 325790.1250\n",
            "Epoch 192, Batch 34/38, Batch Loss: 204923.8281\n",
            "Epoch 192, Batch 35/38, Batch Loss: 276058.1875\n",
            "Epoch 192, Batch 36/38, Batch Loss: 339671.0625\n",
            "Epoch 192, Batch 37/38, Batch Loss: 707308.8125\n",
            "Epoch 192, Batch 38/38, Batch Loss: 245773.0156\n",
            "Batch 1/10, Batch Loss: 354477.3125\n",
            "Batch 2/10, Batch Loss: 385046.7500\n",
            "Batch 3/10, Batch Loss: 246342.0625\n",
            "Batch 4/10, Batch Loss: 294216.8750\n",
            "Batch 5/10, Batch Loss: 233939.8281\n",
            "Batch 6/10, Batch Loss: 374841.7812\n",
            "Batch 7/10, Batch Loss: 227202.0312\n",
            "Batch 8/10, Batch Loss: 205290.4531\n",
            "Batch 9/10, Batch Loss: 272885.6562\n",
            "Batch 10/10, Batch Loss: 436539.0938\n",
            "Test Error: \n",
            " Avg loss: 303078.184375, Avg error: 190457.759167 \n",
            "\n",
            "Epoch 193, Batch 1/38, Batch Loss: 266556.2812\n",
            "Epoch 193, Batch 2/38, Batch Loss: 921889.9375\n",
            "Epoch 193, Batch 3/38, Batch Loss: 295180.5312\n",
            "Epoch 193, Batch 4/38, Batch Loss: 213557.8125\n",
            "Epoch 193, Batch 5/38, Batch Loss: 228833.7031\n",
            "Epoch 193, Batch 6/38, Batch Loss: 650896.4375\n",
            "Epoch 193, Batch 7/38, Batch Loss: 869499.8750\n",
            "Epoch 193, Batch 8/38, Batch Loss: 336847.0312\n",
            "Epoch 193, Batch 9/38, Batch Loss: 322466.0000\n",
            "Epoch 193, Batch 10/38, Batch Loss: 426335.3750\n",
            "Epoch 193, Batch 11/38, Batch Loss: 224718.3125\n",
            "Epoch 193, Batch 12/38, Batch Loss: 247533.6719\n",
            "Epoch 193, Batch 13/38, Batch Loss: 293164.7812\n",
            "Epoch 193, Batch 14/38, Batch Loss: 458598.5938\n",
            "Epoch 193, Batch 15/38, Batch Loss: 325006.8438\n",
            "Epoch 193, Batch 16/38, Batch Loss: 217846.0156\n",
            "Epoch 193, Batch 17/38, Batch Loss: 288678.4062\n",
            "Epoch 193, Batch 18/38, Batch Loss: 329777.1875\n",
            "Epoch 193, Batch 19/38, Batch Loss: 703058.5000\n",
            "Epoch 193, Batch 20/38, Batch Loss: 355141.4062\n",
            "Epoch 193, Batch 21/38, Batch Loss: 218571.7812\n",
            "Epoch 193, Batch 22/38, Batch Loss: 250611.7344\n",
            "Epoch 193, Batch 23/38, Batch Loss: 302081.5000\n",
            "Epoch 193, Batch 24/38, Batch Loss: 256457.4531\n",
            "Epoch 193, Batch 25/38, Batch Loss: 252452.2188\n",
            "Epoch 193, Batch 26/38, Batch Loss: 259379.2188\n",
            "Epoch 193, Batch 27/38, Batch Loss: 325733.8438\n",
            "Epoch 193, Batch 28/38, Batch Loss: 683688.4375\n",
            "Epoch 193, Batch 29/38, Batch Loss: 226545.4688\n",
            "Epoch 193, Batch 30/38, Batch Loss: 375726.1250\n",
            "Epoch 193, Batch 31/38, Batch Loss: 258709.0938\n",
            "Epoch 193, Batch 32/38, Batch Loss: 257301.4375\n",
            "Epoch 193, Batch 33/38, Batch Loss: 304512.8750\n",
            "Epoch 193, Batch 34/38, Batch Loss: 266954.0938\n",
            "Epoch 193, Batch 35/38, Batch Loss: 238119.2188\n",
            "Epoch 193, Batch 36/38, Batch Loss: 277091.0625\n",
            "Epoch 193, Batch 37/38, Batch Loss: 306859.8750\n",
            "Epoch 193, Batch 38/38, Batch Loss: 232636.0625\n",
            "Batch 1/10, Batch Loss: 267246.2812\n",
            "Batch 2/10, Batch Loss: 206669.2188\n",
            "Batch 3/10, Batch Loss: 378902.4062\n",
            "Batch 4/10, Batch Loss: 265959.3750\n",
            "Batch 5/10, Batch Loss: 364031.7812\n",
            "Batch 6/10, Batch Loss: 349466.4062\n",
            "Batch 7/10, Batch Loss: 311376.5938\n",
            "Batch 8/10, Batch Loss: 279472.5312\n",
            "Batch 9/10, Batch Loss: 287393.1875\n",
            "Batch 10/10, Batch Loss: 315983.8750\n",
            "Test Error: \n",
            " Avg loss: 302650.165625, Avg error: 191840.950833 \n",
            "\n",
            "Epoch 194, Batch 1/38, Batch Loss: 231087.8594\n",
            "Epoch 194, Batch 2/38, Batch Loss: 260888.8906\n",
            "Epoch 194, Batch 3/38, Batch Loss: 237184.4688\n",
            "Epoch 194, Batch 4/38, Batch Loss: 367256.0938\n",
            "Epoch 194, Batch 5/38, Batch Loss: 305658.1250\n",
            "Epoch 194, Batch 6/38, Batch Loss: 658590.8750\n",
            "Epoch 194, Batch 7/38, Batch Loss: 696159.6875\n",
            "Epoch 194, Batch 8/38, Batch Loss: 302727.5938\n",
            "Epoch 194, Batch 9/38, Batch Loss: 679675.9375\n",
            "Epoch 194, Batch 10/38, Batch Loss: 284466.6875\n",
            "Epoch 194, Batch 11/38, Batch Loss: 283051.4688\n",
            "Epoch 194, Batch 12/38, Batch Loss: 246727.4219\n",
            "Epoch 194, Batch 13/38, Batch Loss: 306526.0625\n",
            "Epoch 194, Batch 14/38, Batch Loss: 380065.1875\n",
            "Epoch 194, Batch 15/38, Batch Loss: 412144.2500\n",
            "Epoch 194, Batch 16/38, Batch Loss: 697147.0625\n",
            "Epoch 194, Batch 17/38, Batch Loss: 257064.1250\n",
            "Epoch 194, Batch 18/38, Batch Loss: 286016.9062\n",
            "Epoch 194, Batch 19/38, Batch Loss: 225305.6250\n",
            "Epoch 194, Batch 20/38, Batch Loss: 706814.8125\n",
            "Epoch 194, Batch 21/38, Batch Loss: 375135.1562\n",
            "Epoch 194, Batch 22/38, Batch Loss: 236390.3281\n",
            "Epoch 194, Batch 23/38, Batch Loss: 223091.4062\n",
            "Epoch 194, Batch 24/38, Batch Loss: 280538.5312\n",
            "Epoch 194, Batch 25/38, Batch Loss: 272769.8750\n",
            "Epoch 194, Batch 26/38, Batch Loss: 285432.0312\n",
            "Epoch 194, Batch 27/38, Batch Loss: 642528.1875\n",
            "Epoch 194, Batch 28/38, Batch Loss: 252413.5938\n",
            "Epoch 194, Batch 29/38, Batch Loss: 657793.9375\n",
            "Epoch 194, Batch 30/38, Batch Loss: 295722.0312\n",
            "Epoch 194, Batch 31/38, Batch Loss: 310555.5000\n",
            "Epoch 194, Batch 32/38, Batch Loss: 263822.9062\n",
            "Epoch 194, Batch 33/38, Batch Loss: 271468.8750\n",
            "Epoch 194, Batch 34/38, Batch Loss: 304702.8125\n",
            "Epoch 194, Batch 35/38, Batch Loss: 243028.3125\n",
            "Epoch 194, Batch 36/38, Batch Loss: 329327.8750\n",
            "Epoch 194, Batch 37/38, Batch Loss: 290981.9062\n",
            "Epoch 194, Batch 38/38, Batch Loss: 181057.5938\n",
            "Batch 1/10, Batch Loss: 275418.4688\n",
            "Batch 2/10, Batch Loss: 324145.8438\n",
            "Batch 3/10, Batch Loss: 441342.3750\n",
            "Batch 4/10, Batch Loss: 271857.9062\n",
            "Batch 5/10, Batch Loss: 258565.7188\n",
            "Batch 6/10, Batch Loss: 282605.0625\n",
            "Batch 7/10, Batch Loss: 248997.8438\n",
            "Batch 8/10, Batch Loss: 341433.7500\n",
            "Batch 9/10, Batch Loss: 405806.0000\n",
            "Batch 10/10, Batch Loss: 314262.5625\n",
            "Test Error: \n",
            " Avg loss: 316443.553125, Avg error: 200617.443333 \n",
            "\n",
            "Epoch 195, Batch 1/38, Batch Loss: 688611.3125\n",
            "Epoch 195, Batch 2/38, Batch Loss: 374683.2500\n",
            "Epoch 195, Batch 3/38, Batch Loss: 673803.9375\n",
            "Epoch 195, Batch 4/38, Batch Loss: 279350.1562\n",
            "Epoch 195, Batch 5/38, Batch Loss: 254171.4062\n",
            "Epoch 195, Batch 6/38, Batch Loss: 313352.8750\n",
            "Epoch 195, Batch 7/38, Batch Loss: 397191.4688\n",
            "Epoch 195, Batch 8/38, Batch Loss: 327076.4375\n",
            "Epoch 195, Batch 9/38, Batch Loss: 691337.6250\n",
            "Epoch 195, Batch 10/38, Batch Loss: 672341.0000\n",
            "Epoch 195, Batch 11/38, Batch Loss: 227750.0156\n",
            "Epoch 195, Batch 12/38, Batch Loss: 221367.4219\n",
            "Epoch 195, Batch 13/38, Batch Loss: 251629.4219\n",
            "Epoch 195, Batch 14/38, Batch Loss: 266395.3750\n",
            "Epoch 195, Batch 15/38, Batch Loss: 477202.4062\n",
            "Epoch 195, Batch 16/38, Batch Loss: 689840.5625\n",
            "Epoch 195, Batch 17/38, Batch Loss: 327562.9062\n",
            "Epoch 195, Batch 18/38, Batch Loss: 296761.2500\n",
            "Epoch 195, Batch 19/38, Batch Loss: 269899.7188\n",
            "Epoch 195, Batch 20/38, Batch Loss: 227468.3438\n",
            "Epoch 195, Batch 21/38, Batch Loss: 676404.9375\n",
            "Epoch 195, Batch 22/38, Batch Loss: 323894.1562\n",
            "Epoch 195, Batch 23/38, Batch Loss: 311529.6562\n",
            "Epoch 195, Batch 24/38, Batch Loss: 238343.6094\n",
            "Epoch 195, Batch 25/38, Batch Loss: 240324.6250\n",
            "Epoch 195, Batch 26/38, Batch Loss: 710354.7500\n",
            "Epoch 195, Batch 27/38, Batch Loss: 317799.5312\n",
            "Epoch 195, Batch 28/38, Batch Loss: 268481.3438\n",
            "Epoch 195, Batch 29/38, Batch Loss: 221437.1562\n",
            "Epoch 195, Batch 30/38, Batch Loss: 396976.3750\n",
            "Epoch 195, Batch 31/38, Batch Loss: 214907.7188\n",
            "Epoch 195, Batch 32/38, Batch Loss: 172175.3125\n",
            "Epoch 195, Batch 33/38, Batch Loss: 310046.0000\n",
            "Epoch 195, Batch 34/38, Batch Loss: 405288.0312\n",
            "Epoch 195, Batch 35/38, Batch Loss: 295971.8438\n",
            "Epoch 195, Batch 36/38, Batch Loss: 234861.7344\n",
            "Epoch 195, Batch 37/38, Batch Loss: 224923.0312\n",
            "Epoch 195, Batch 38/38, Batch Loss: 272133.2188\n",
            "Batch 1/10, Batch Loss: 285027.9375\n",
            "Batch 2/10, Batch Loss: 276650.8438\n",
            "Batch 3/10, Batch Loss: 383744.8438\n",
            "Batch 4/10, Batch Loss: 350344.1250\n",
            "Batch 5/10, Batch Loss: 288009.0312\n",
            "Batch 6/10, Batch Loss: 252303.2031\n",
            "Batch 7/10, Batch Loss: 246776.8750\n",
            "Batch 8/10, Batch Loss: 373478.7812\n",
            "Batch 9/10, Batch Loss: 305207.1875\n",
            "Batch 10/10, Batch Loss: 228921.0469\n",
            "Test Error: \n",
            " Avg loss: 299046.387500, Avg error: 194037.801667 \n",
            "\n",
            "Epoch 196, Batch 1/38, Batch Loss: 834875.1250\n",
            "Epoch 196, Batch 2/38, Batch Loss: 276334.2500\n",
            "Epoch 196, Batch 3/38, Batch Loss: 303669.9375\n",
            "Epoch 196, Batch 4/38, Batch Loss: 236378.4375\n",
            "Epoch 196, Batch 5/38, Batch Loss: 227785.7969\n",
            "Epoch 196, Batch 6/38, Batch Loss: 688900.8750\n",
            "Epoch 196, Batch 7/38, Batch Loss: 273456.9062\n",
            "Epoch 196, Batch 8/38, Batch Loss: 257440.9844\n",
            "Epoch 196, Batch 9/38, Batch Loss: 327166.0938\n",
            "Epoch 196, Batch 10/38, Batch Loss: 343715.2188\n",
            "Epoch 196, Batch 11/38, Batch Loss: 245456.6250\n",
            "Epoch 196, Batch 12/38, Batch Loss: 675215.5000\n",
            "Epoch 196, Batch 13/38, Batch Loss: 220583.3906\n",
            "Epoch 196, Batch 14/38, Batch Loss: 283194.8750\n",
            "Epoch 196, Batch 15/38, Batch Loss: 339688.5625\n",
            "Epoch 196, Batch 16/38, Batch Loss: 246616.3594\n",
            "Epoch 196, Batch 17/38, Batch Loss: 671724.6875\n",
            "Epoch 196, Batch 18/38, Batch Loss: 342769.5625\n",
            "Epoch 196, Batch 19/38, Batch Loss: 391214.2500\n",
            "Epoch 196, Batch 20/38, Batch Loss: 229923.6094\n",
            "Epoch 196, Batch 21/38, Batch Loss: 220061.9219\n",
            "Epoch 196, Batch 22/38, Batch Loss: 244720.6562\n",
            "Epoch 196, Batch 23/38, Batch Loss: 641211.0625\n",
            "Epoch 196, Batch 24/38, Batch Loss: 367087.8438\n",
            "Epoch 196, Batch 25/38, Batch Loss: 691725.8125\n",
            "Epoch 196, Batch 26/38, Batch Loss: 268627.0938\n",
            "Epoch 196, Batch 27/38, Batch Loss: 252756.6094\n",
            "Epoch 196, Batch 28/38, Batch Loss: 256201.9219\n",
            "Epoch 196, Batch 29/38, Batch Loss: 263363.9062\n",
            "Epoch 196, Batch 30/38, Batch Loss: 215799.2188\n",
            "Epoch 196, Batch 31/38, Batch Loss: 283319.3438\n",
            "Epoch 196, Batch 32/38, Batch Loss: 363516.4688\n",
            "Epoch 196, Batch 33/38, Batch Loss: 260333.8125\n",
            "Epoch 196, Batch 34/38, Batch Loss: 336858.8438\n",
            "Epoch 196, Batch 35/38, Batch Loss: 463160.8438\n",
            "Epoch 196, Batch 36/38, Batch Loss: 277078.0625\n",
            "Epoch 196, Batch 37/38, Batch Loss: 299951.5000\n",
            "Epoch 196, Batch 38/38, Batch Loss: 219611.5469\n",
            "Batch 1/10, Batch Loss: 334088.4688\n",
            "Batch 2/10, Batch Loss: 398037.1875\n",
            "Batch 3/10, Batch Loss: 303488.8438\n",
            "Batch 4/10, Batch Loss: 204808.1719\n",
            "Batch 5/10, Batch Loss: 318562.3750\n",
            "Batch 6/10, Batch Loss: 299024.0312\n",
            "Batch 7/10, Batch Loss: 224716.0000\n",
            "Batch 8/10, Batch Loss: 330278.3750\n",
            "Batch 9/10, Batch Loss: 306907.3438\n",
            "Batch 10/10, Batch Loss: 159295.2656\n",
            "Test Error: \n",
            " Avg loss: 287920.606250, Avg error: 189780.137500 \n",
            "\n",
            "Epoch 197, Batch 1/38, Batch Loss: 276965.1562\n",
            "Epoch 197, Batch 2/38, Batch Loss: 734501.7500\n",
            "Epoch 197, Batch 3/38, Batch Loss: 271027.3125\n",
            "Epoch 197, Batch 4/38, Batch Loss: 295468.8438\n",
            "Epoch 197, Batch 5/38, Batch Loss: 219668.8281\n",
            "Epoch 197, Batch 6/38, Batch Loss: 255130.1406\n",
            "Epoch 197, Batch 7/38, Batch Loss: 321394.0938\n",
            "Epoch 197, Batch 8/38, Batch Loss: 352153.2188\n",
            "Epoch 197, Batch 9/38, Batch Loss: 350954.7812\n",
            "Epoch 197, Batch 10/38, Batch Loss: 275491.9062\n",
            "Epoch 197, Batch 11/38, Batch Loss: 454690.4688\n",
            "Epoch 197, Batch 12/38, Batch Loss: 242319.0000\n",
            "Epoch 197, Batch 13/38, Batch Loss: 236503.8906\n",
            "Epoch 197, Batch 14/38, Batch Loss: 234192.9531\n",
            "Epoch 197, Batch 15/38, Batch Loss: 397054.1250\n",
            "Epoch 197, Batch 16/38, Batch Loss: 273808.8125\n",
            "Epoch 197, Batch 17/38, Batch Loss: 689022.7500\n",
            "Epoch 197, Batch 18/38, Batch Loss: 260508.4844\n",
            "Epoch 197, Batch 19/38, Batch Loss: 219914.4531\n",
            "Epoch 197, Batch 20/38, Batch Loss: 912497.1250\n",
            "Epoch 197, Batch 21/38, Batch Loss: 275847.0000\n",
            "Epoch 197, Batch 22/38, Batch Loss: 322979.4688\n",
            "Epoch 197, Batch 23/38, Batch Loss: 598995.6250\n",
            "Epoch 197, Batch 24/38, Batch Loss: 258364.5469\n",
            "Epoch 197, Batch 25/38, Batch Loss: 339638.4375\n",
            "Epoch 197, Batch 26/38, Batch Loss: 300607.3438\n",
            "Epoch 197, Batch 27/38, Batch Loss: 273122.9688\n",
            "Epoch 197, Batch 28/38, Batch Loss: 308093.8438\n",
            "Epoch 197, Batch 29/38, Batch Loss: 272239.9375\n",
            "Epoch 197, Batch 30/38, Batch Loss: 249596.6250\n",
            "Epoch 197, Batch 31/38, Batch Loss: 258108.3750\n",
            "Epoch 197, Batch 32/38, Batch Loss: 279744.5625\n",
            "Epoch 197, Batch 33/38, Batch Loss: 238986.1875\n",
            "Epoch 197, Batch 34/38, Batch Loss: 271900.0625\n",
            "Epoch 197, Batch 35/38, Batch Loss: 236451.0625\n",
            "Epoch 197, Batch 36/38, Batch Loss: 667086.0625\n",
            "Epoch 197, Batch 37/38, Batch Loss: 697444.0625\n",
            "Epoch 197, Batch 38/38, Batch Loss: 256432.0781\n",
            "Batch 1/10, Batch Loss: 251837.3438\n",
            "Batch 2/10, Batch Loss: 362924.0312\n",
            "Batch 3/10, Batch Loss: 213287.9062\n",
            "Batch 4/10, Batch Loss: 239197.4688\n",
            "Batch 5/10, Batch Loss: 330579.7500\n",
            "Batch 6/10, Batch Loss: 290344.2188\n",
            "Batch 7/10, Batch Loss: 392171.3125\n",
            "Batch 8/10, Batch Loss: 376816.8438\n",
            "Batch 9/10, Batch Loss: 261741.3906\n",
            "Batch 10/10, Batch Loss: 311413.7188\n",
            "Test Error: \n",
            " Avg loss: 303031.398438, Avg error: 193955.778333 \n",
            "\n",
            "Epoch 198, Batch 1/38, Batch Loss: 267310.4062\n",
            "Epoch 198, Batch 2/38, Batch Loss: 657312.1250\n",
            "Epoch 198, Batch 3/38, Batch Loss: 390251.9062\n",
            "Epoch 198, Batch 4/38, Batch Loss: 287865.0312\n",
            "Epoch 198, Batch 5/38, Batch Loss: 699217.6250\n",
            "Epoch 198, Batch 6/38, Batch Loss: 285854.8438\n",
            "Epoch 198, Batch 7/38, Batch Loss: 691220.8125\n",
            "Epoch 198, Batch 8/38, Batch Loss: 270755.2500\n",
            "Epoch 198, Batch 9/38, Batch Loss: 276629.8125\n",
            "Epoch 198, Batch 10/38, Batch Loss: 286118.0312\n",
            "Epoch 198, Batch 11/38, Batch Loss: 211981.5781\n",
            "Epoch 198, Batch 12/38, Batch Loss: 290646.2812\n",
            "Epoch 198, Batch 13/38, Batch Loss: 285113.2500\n",
            "Epoch 198, Batch 14/38, Batch Loss: 281795.3125\n",
            "Epoch 198, Batch 15/38, Batch Loss: 277389.9062\n",
            "Epoch 198, Batch 16/38, Batch Loss: 311362.1875\n",
            "Epoch 198, Batch 17/38, Batch Loss: 243094.2188\n",
            "Epoch 198, Batch 18/38, Batch Loss: 275619.0938\n",
            "Epoch 198, Batch 19/38, Batch Loss: 504773.0000\n",
            "Epoch 198, Batch 20/38, Batch Loss: 231571.2500\n",
            "Epoch 198, Batch 21/38, Batch Loss: 653815.0000\n",
            "Epoch 198, Batch 22/38, Batch Loss: 315423.7500\n",
            "Epoch 198, Batch 23/38, Batch Loss: 273335.6562\n",
            "Epoch 198, Batch 24/38, Batch Loss: 346041.1562\n",
            "Epoch 198, Batch 25/38, Batch Loss: 676921.7500\n",
            "Epoch 198, Batch 26/38, Batch Loss: 238013.0469\n",
            "Epoch 198, Batch 27/38, Batch Loss: 286165.1250\n",
            "Epoch 198, Batch 28/38, Batch Loss: 356425.7500\n",
            "Epoch 198, Batch 29/38, Batch Loss: 334601.1875\n",
            "Epoch 198, Batch 30/38, Batch Loss: 223533.4375\n",
            "Epoch 198, Batch 31/38, Batch Loss: 282451.0000\n",
            "Epoch 198, Batch 32/38, Batch Loss: 223225.6406\n",
            "Epoch 198, Batch 33/38, Batch Loss: 295017.0312\n",
            "Epoch 198, Batch 34/38, Batch Loss: 222960.3906\n",
            "Epoch 198, Batch 35/38, Batch Loss: 297078.6875\n",
            "Epoch 198, Batch 36/38, Batch Loss: 265386.5000\n",
            "Epoch 198, Batch 37/38, Batch Loss: 903728.0625\n",
            "Epoch 198, Batch 38/38, Batch Loss: 506988.2188\n",
            "Batch 1/10, Batch Loss: 302794.1250\n",
            "Batch 2/10, Batch Loss: 247119.7031\n",
            "Batch 3/10, Batch Loss: 278937.5938\n",
            "Batch 4/10, Batch Loss: 233307.0000\n",
            "Batch 5/10, Batch Loss: 341263.4375\n",
            "Batch 6/10, Batch Loss: 270741.6562\n",
            "Batch 7/10, Batch Loss: 406154.1562\n",
            "Batch 8/10, Batch Loss: 331528.5000\n",
            "Batch 9/10, Batch Loss: 339360.5938\n",
            "Batch 10/10, Batch Loss: 200726.6562\n",
            "Test Error: \n",
            " Avg loss: 295193.342187, Avg error: 189675.819583 \n",
            "\n",
            "Epoch 199, Batch 1/38, Batch Loss: 246354.3438\n",
            "Epoch 199, Batch 2/38, Batch Loss: 665801.3125\n",
            "Epoch 199, Batch 3/38, Batch Loss: 255975.2188\n",
            "Epoch 199, Batch 4/38, Batch Loss: 325591.8438\n",
            "Epoch 199, Batch 5/38, Batch Loss: 330309.0625\n",
            "Epoch 199, Batch 6/38, Batch Loss: 729178.5625\n",
            "Epoch 199, Batch 7/38, Batch Loss: 300620.4062\n",
            "Epoch 199, Batch 8/38, Batch Loss: 255345.9844\n",
            "Epoch 199, Batch 9/38, Batch Loss: 265762.9062\n",
            "Epoch 199, Batch 10/38, Batch Loss: 459692.9688\n",
            "Epoch 199, Batch 11/38, Batch Loss: 233848.8594\n",
            "Epoch 199, Batch 12/38, Batch Loss: 307809.1875\n",
            "Epoch 199, Batch 13/38, Batch Loss: 246367.3906\n",
            "Epoch 199, Batch 14/38, Batch Loss: 189372.1406\n",
            "Epoch 199, Batch 15/38, Batch Loss: 610897.2500\n",
            "Epoch 199, Batch 16/38, Batch Loss: 351540.9688\n",
            "Epoch 199, Batch 17/38, Batch Loss: 652086.8125\n",
            "Epoch 199, Batch 18/38, Batch Loss: 297416.6250\n",
            "Epoch 199, Batch 19/38, Batch Loss: 274969.4062\n",
            "Epoch 199, Batch 20/38, Batch Loss: 221115.2812\n",
            "Epoch 199, Batch 21/38, Batch Loss: 278145.6875\n",
            "Epoch 199, Batch 22/38, Batch Loss: 276563.1250\n",
            "Epoch 199, Batch 23/38, Batch Loss: 230050.7656\n",
            "Epoch 199, Batch 24/38, Batch Loss: 267116.9062\n",
            "Epoch 199, Batch 25/38, Batch Loss: 299773.0938\n",
            "Epoch 199, Batch 26/38, Batch Loss: 209555.9688\n",
            "Epoch 199, Batch 27/38, Batch Loss: 223272.2969\n",
            "Epoch 199, Batch 28/38, Batch Loss: 404566.6562\n",
            "Epoch 199, Batch 29/38, Batch Loss: 918528.8125\n",
            "Epoch 199, Batch 30/38, Batch Loss: 230902.5469\n",
            "Epoch 199, Batch 31/38, Batch Loss: 285215.1875\n",
            "Epoch 199, Batch 32/38, Batch Loss: 333060.3750\n",
            "Epoch 199, Batch 33/38, Batch Loss: 323604.4062\n",
            "Epoch 199, Batch 34/38, Batch Loss: 310197.1250\n",
            "Epoch 199, Batch 35/38, Batch Loss: 289902.2500\n",
            "Epoch 199, Batch 36/38, Batch Loss: 331833.1562\n",
            "Epoch 199, Batch 37/38, Batch Loss: 710473.3125\n",
            "Epoch 199, Batch 38/38, Batch Loss: 310119.3438\n",
            "Batch 1/10, Batch Loss: 265377.0938\n",
            "Batch 2/10, Batch Loss: 303657.4062\n",
            "Batch 3/10, Batch Loss: 507961.8125\n",
            "Batch 4/10, Batch Loss: 336412.0312\n",
            "Batch 5/10, Batch Loss: 290615.0938\n",
            "Batch 6/10, Batch Loss: 267405.8438\n",
            "Batch 7/10, Batch Loss: 247817.2031\n",
            "Batch 8/10, Batch Loss: 358180.1562\n",
            "Batch 9/10, Batch Loss: 257804.5938\n",
            "Batch 10/10, Batch Loss: 334802.6250\n",
            "Test Error: \n",
            " Avg loss: 317003.385937, Avg error: 200275.383333 \n",
            "\n",
            "Epoch 200, Batch 1/38, Batch Loss: 302130.1562\n",
            "Epoch 200, Batch 2/38, Batch Loss: 254888.2656\n",
            "Epoch 200, Batch 3/38, Batch Loss: 180092.5625\n",
            "Epoch 200, Batch 4/38, Batch Loss: 680663.8750\n",
            "Epoch 200, Batch 5/38, Batch Loss: 310032.8125\n",
            "Epoch 200, Batch 6/38, Batch Loss: 263719.8438\n",
            "Epoch 200, Batch 7/38, Batch Loss: 271544.1875\n",
            "Epoch 200, Batch 8/38, Batch Loss: 266570.7500\n",
            "Epoch 200, Batch 9/38, Batch Loss: 288241.4688\n",
            "Epoch 200, Batch 10/38, Batch Loss: 257317.0312\n",
            "Epoch 200, Batch 11/38, Batch Loss: 331496.1875\n",
            "Epoch 200, Batch 12/38, Batch Loss: 240669.4062\n",
            "Epoch 200, Batch 13/38, Batch Loss: 694211.5000\n",
            "Epoch 200, Batch 14/38, Batch Loss: 284337.3438\n",
            "Epoch 200, Batch 15/38, Batch Loss: 235703.5000\n",
            "Epoch 200, Batch 16/38, Batch Loss: 666478.8750\n",
            "Epoch 200, Batch 17/38, Batch Loss: 309377.2188\n",
            "Epoch 200, Batch 18/38, Batch Loss: 313510.9062\n",
            "Epoch 200, Batch 19/38, Batch Loss: 358119.6250\n",
            "Epoch 200, Batch 20/38, Batch Loss: 342392.8750\n",
            "Epoch 200, Batch 21/38, Batch Loss: 487855.5625\n",
            "Epoch 200, Batch 22/38, Batch Loss: 310733.6875\n",
            "Epoch 200, Batch 23/38, Batch Loss: 690850.3125\n",
            "Epoch 200, Batch 24/38, Batch Loss: 293154.8125\n",
            "Epoch 200, Batch 25/38, Batch Loss: 252762.5312\n",
            "Epoch 200, Batch 26/38, Batch Loss: 269369.8125\n",
            "Epoch 200, Batch 27/38, Batch Loss: 212201.6250\n",
            "Epoch 200, Batch 28/38, Batch Loss: 662235.6875\n",
            "Epoch 200, Batch 29/38, Batch Loss: 259123.6406\n",
            "Epoch 200, Batch 30/38, Batch Loss: 228954.0312\n",
            "Epoch 200, Batch 31/38, Batch Loss: 333252.9375\n",
            "Epoch 200, Batch 32/38, Batch Loss: 642230.6875\n",
            "Epoch 200, Batch 33/38, Batch Loss: 240998.8750\n",
            "Epoch 200, Batch 34/38, Batch Loss: 670776.1250\n",
            "Epoch 200, Batch 35/38, Batch Loss: 325680.8438\n",
            "Epoch 200, Batch 36/38, Batch Loss: 296640.2812\n",
            "Epoch 200, Batch 37/38, Batch Loss: 255015.8750\n",
            "Epoch 200, Batch 38/38, Batch Loss: 270409.8125\n",
            "Batch 1/10, Batch Loss: 333275.5312\n",
            "Batch 2/10, Batch Loss: 247187.3750\n",
            "Batch 3/10, Batch Loss: 234032.4688\n",
            "Batch 4/10, Batch Loss: 339141.6562\n",
            "Batch 5/10, Batch Loss: 365420.1250\n",
            "Batch 6/10, Batch Loss: 223942.2812\n",
            "Batch 7/10, Batch Loss: 332525.3438\n",
            "Batch 8/10, Batch Loss: 320374.5312\n",
            "Batch 9/10, Batch Loss: 229167.6875\n",
            "Batch 10/10, Batch Loss: 263523.3750\n",
            "Test Error: \n",
            " Avg loss: 288859.037500, Avg error: 189009.958333 \n",
            "\n",
            "2024-09-14 21:09:13.576047 Epoch 200, Average Training loss 356677.51398026315\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento\n",
        "n_epochs = 200\n",
        "train_losses, val_losses = training_loop(\n",
        "    n_epochs=n_epochs,\n",
        "    optimizer=optimizer,\n",
        "    model=model,\n",
        "    loss_fn=loss_fn,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "CVB9iAVh8Laa",
        "outputId": "84f037e9-e7bf-4666-abad-bba87b604fea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graficar las p√©rdidas\n",
        "plt.plot(range(1, n_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}